{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jqUrrzKINPeK",
        "outputId": "35dddd57-cb15-4845-efd7-d2b6671b3a94"
      },
      "outputs": [],
      "source": [
        "#Install:\n",
        "##1.HF's transformers library, 2.SP for t5,\n",
        "##3.models' evaluation metric, 4.pdf to text string extraction library.\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade sentencepiece\n",
        "!pip install bert_score\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "gm4da62zNbYT"
      },
      "outputs": [],
      "source": [
        "#Importing libraries.\n",
        "import PyPDF2\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from bert_score import score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUpo2gQMy0rr"
      },
      "source": [
        "# Notebook Aims\n",
        "\n",
        "\n",
        "1.   **Use bart-large-cnn and T5 models for text summarization.**\n",
        "Article 6 BloombergGPT_ A Large Language Model for Finance.\n",
        "---\n",
        "2.   **Create a very short but very informative and well-structured summaries.**\n",
        "---\n",
        "3.   **Test models' performance with pairwise cosine similarity function.**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AZJ5-kRQNnxd",
        "outputId": "8e9b57fb-8d42-4d47-b9df-a3f201b64c89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "#Instantiating models and tokenizers.\n",
        "bart_summarizer = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn', cache_dir=None)\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "t5_summarizer = T5ForConditionalGeneration.from_pretrained('t5-large', cache_dir=None)\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YPGLLLYIS4Gk"
      },
      "outputs": [],
      "source": [
        "#Move the models to GPU.\n",
        "bart_summarizer = bart_summarizer.to('cuda')\n",
        "t5_summarizer = t5_summarizer.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "reBJLo5DN8Ks"
      },
      "outputs": [],
      "source": [
        "#Defining necessary functions.\n",
        "#Part 1: Dealing with text inputs.\n",
        "\n",
        "#Pre-processing: Clean the text before encoding.\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Input: through the pdf_extractor function.\n",
        "    Ouptut: text without \\xa0 or similar unwanted characters.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[\\xa0]+', ' ', text)\n",
        "\n",
        "#PDF extraction to text string.\n",
        "def pdf_extractor(path):\n",
        "    \"\"\"\n",
        "    Input: path to file.\n",
        "    Output: text string.\n",
        "    Functionality: this function extracts the pdf content into a text string format.\n",
        "    \"\"\"\n",
        "    with open(path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        if pdf_reader.is_encrypted:\n",
        "            pdf_reader.decrypt(\"\")\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return clean_text(text)\n",
        "\n",
        "#Text string chunks divider.\n",
        "def text_chunker(text, model_tokenizer, max_tokens=350, overlapPercent=10):\n",
        "    \"\"\"\n",
        "    Input: text output of pdf_extractor function.\n",
        "    Output: chunks of tokens. I used a limit of 350 tokens as Bart-large-cnn limit\n",
        "            is 1024 for encoder, but t5-large model limitations is 512 tokens for encoder and decoder together.\n",
        "    Functionality: this function chunks the textual inputs.\n",
        "    \"\"\"\n",
        "    tokens = model_tokenizer.tokenize(text)\n",
        "    overlap_tokens = int(max_tokens * overlapPercent / 100)\n",
        "    chunks = [tokens[i:i + max_tokens]\n",
        "              for i in range(0, len(tokens),\n",
        "                             max_tokens - overlap_tokens)]\n",
        "    text_chunks = [model_tokenizer.decode(\n",
        "        model_tokenizer.convert_tokens_to_ids(chunk),\n",
        "        skip_special_tokens=True) for chunk in chunks]\n",
        "    return text_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "L9yZDn9rEfDa"
      },
      "outputs": [],
      "source": [
        "#Defining necessary functions.\n",
        "#Part 2:Encoding, summarization, and decoding the summaries.\n",
        "\n",
        "#bart-large-cnn.Encoding limitation:1024.\n",
        "def bart_summarize(text):\n",
        "    \"\"\"\n",
        "    Input: text chunks.\n",
        "    Output: summarized chunks inputs.\n",
        "    Functionality:\n",
        "                  -> sequential optimization (3 beams),\n",
        "                  -> length penalty (0.6; Want to go with a bit shorter sentences),\n",
        "                  -> summary output range (80-160 tokens per chunk).\n",
        "    \"\"\"\n",
        "    inputs = bart_tokenizer.encode(\"summarize: \" +\n",
        "                                   text,\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   max_length=350,\n",
        "                                   truncation=True).to('cuda')\n",
        "    summary_ids = bart_summarizer.generate(inputs,\n",
        "                                           max_length=160,\n",
        "                                           min_length=80,\n",
        "                                           num_beams=3,\n",
        "                                           length_penalty=0.6,\n",
        "                                           early_stopping=True)\n",
        "    summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "#t5-large.Endodin+decoding limitaiton:512.\n",
        "def t5_summarize(text):\n",
        "    \"\"\"\n",
        "    Input: text chunks.\n",
        "    Output: summarized chunks inputs.\n",
        "    Functionality: -> sequential optimization (3 beams),\n",
        "                   -> length penalty (0.6; Want to go with a bit shorter sentences),\n",
        "                   -> summary output range (80-160 tokens per chunk).\n",
        "    \"\"\"\n",
        "    inputs = t5_tokenizer.encode(\"summarize: \" +\n",
        "                                 text,\n",
        "                                 return_tensors=\"pt\",\n",
        "                                 max_length=350,\n",
        "                                 truncation=True).to('cuda')\n",
        "    summary_ids = t5_summarizer.generate(inputs,\n",
        "                                         max_length=160,\n",
        "                                         min_length=80,\n",
        "                                         num_beams=3,\n",
        "                                         length_penalty=0.6,\n",
        "                                         early_stopping=True)\n",
        "    summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rIuBj4SWOma4"
      },
      "outputs": [],
      "source": [
        "#Defining necessary functions.\n",
        "#Part 3: Recusion function over concatenated summary. The goal is to get iterate\n",
        "##over the concatenated summary and get a summary of 210 tokens max.\n",
        "\n",
        "#Recursive summarizer for bart-long-cnn.\n",
        "def b_recursive_summarize(text, recursion_l=0):\n",
        "    \"\"\"\n",
        "    Input: concatenated decoded summaries.\n",
        "    Output: re-iterated summary (for contextual benefits) of max 210 tokens.\n",
        "    Functionality: -> iterating over the summarized output,\n",
        "                   -> gaining contextual benefits,\n",
        "                   -> aiming at short and meaningful output of max 210 tokens.\n",
        "    \"\"\"\n",
        "    recursion_level = recursion_l + 1\n",
        "    print(f\"Bart long recurs. level: {recursion_level}\\n\")\n",
        "    tokens = bart_tokenizer.tokenize(text)\n",
        "    expectedCountOfChunks = len(tokens)/210\n",
        "    max_length=int(len(tokens)/expectedCountOfChunks)+2\n",
        "\n",
        "    chunks = text_chunker(text,\n",
        "                          bart_tokenizer,\n",
        "                          max_tokens=350)\n",
        "    print(f\"Number of chunks: {len(chunks)}\")\n",
        "\n",
        "    summaries = []\n",
        "    for i,chunk in enumerate(chunks,1):\n",
        "      print(f\"Chunk no.{i}:\")\n",
        "      print(chunk, \"\\n\")\n",
        "      summary = bart_summarize(chunk)\n",
        "      print(\"Summary:\", summary)\n",
        "      summaries.append(summary)\n",
        "      print(\"_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\")\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    concatenated_summary = ' '.join(summaries)\n",
        "\n",
        "    tokens = bart_tokenizer.tokenize(concatenated_summary)\n",
        "\n",
        "    if len(tokens) > 210:\n",
        "        print(\"Recursive\")\n",
        "        return b_recursive_summarize(concatenated_summary,\n",
        "                                     recursion_l=recursion_level)\n",
        "    else:\n",
        "        final_summary = concatenated_summary\n",
        "        if len(chunks) > 1:\n",
        "            final_summary = bart_summarizer(concatenated_summary)\n",
        "        return final_summary\n",
        "\n",
        "#Recursive summarizer for t5-long.\n",
        "def t5_recursive_summarize(text, recursion_l=0):\n",
        "    \"\"\"\n",
        "    Input: concatenated decoded summaries.\n",
        "    Output: re-iterated summary (for contextual benefits) of max 210 tokens.\n",
        "    Functionality: -> iterating over the summarized output,\n",
        "                   -> gaining contextual benefits,\n",
        "                   -> aiming at short and meaningful output of max 210 tokens.\n",
        "    \"\"\"\n",
        "    recursion_level = recursion_l + 1\n",
        "    print(f\"T5 recurs. level: {recursion_level}\\n\")\n",
        "    tokens = t5_tokenizer.tokenize(text)\n",
        "    expectedCountOfChunks = len(tokens)/210\n",
        "    max_length=int(len(tokens)/expectedCountOfChunks)+2\n",
        "\n",
        "    chunks = text_chunker(text,\n",
        "                          t5_tokenizer,\n",
        "                          max_tokens=350)\n",
        "    print(f\"Number of chunks: {len(chunks)}\")\n",
        "\n",
        "    summaries = []\n",
        "    for i,chunk in enumerate(chunks,1):\n",
        "      print(f\"Chunk no.{i}:\")\n",
        "      print(chunk, \"\\n\")\n",
        "      summary = t5_summarize(chunk)\n",
        "      print(\"Summary:\", summary)\n",
        "      summaries.append(summary)\n",
        "      print(\"_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\")\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    concatenated_summary = ' '.join(summaries)\n",
        "\n",
        "    tokens = t5_tokenizer.tokenize(concatenated_summary)\n",
        "\n",
        "    if len(tokens) > 210:\n",
        "        print(\"Recursive\")\n",
        "        return t5_recursive_summarize(concatenated_summary,\n",
        "                                     recursion_l=recursion_level)\n",
        "    else:\n",
        "        final_summary = concatenated_summary\n",
        "        if len(chunks) > 1:\n",
        "            final_summary = t5_summarizer(concatenated_summary)\n",
        "        return final_summary\n",
        "\n",
        "#Model performance evaluation: pairwise cosine similarity (Scalar Product of Vectors / Product of Norm Vectors in Euclidean norm).\n",
        "# A*B / (sqrt(A**2) * sqrt(B**2)).\n",
        "def evaluator(final_summary,text):\n",
        "    \"\"\"\n",
        "    Input: the model generated summary and reference text (original).\n",
        "    Output: pairwise cosine similarity in precision, recall, and harmonic mean (f1).\n",
        "    Functionality: this function aims at evaluating the models' performances.\n",
        "    \"\"\"\n",
        "    precision, recall, f1 = score(final_summary,text)\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gArnpXhmO095",
        "outputId": "384b7187-2600-4c26-e046-88e743bbf602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bart long recurs. level: 1\n",
            "\n",
            "Number of chunks: 97\n",
            "Chunk no.1:\n",
            "BloombergGPT: A Large Language Model for Finance\n",
            "Shijie Wu1,∗, Ozan ˙Irsoy1,∗, Steven Lu1,∗, Vadim Dabravolski1, Mark Dredze1,3,\n",
            "Sebastian Gehrmann1, Prabhanjan Kambadur1, David Rosenberg2, Gideon Mann1\n",
            "1Bloomberg, New York, NY USA\n",
            "2Bloomberg, Toronto, ON Canada\n",
            "3Computer Science, Johns Hopkins University, Baltimore, MD USA\n",
            "Abstract\n",
            "The use of NLP in the realm of ﬁnancial technology is broad and complex, with app lications\n",
            "ranging from sentiment analysis and named entity recognition to questi on answering. Large\n",
            "Language Models (LLMs) have been shown to be eﬀective on a variety of tasks ; however, no\n",
            "LLM specialized for the ﬁnancial domain has been reported in literature. In this work, we\n",
            "presentBloombergGPT, a 50 billion parameter language model that is trained on a wide\n",
            "range of ﬁnancial data. We construct a 363 billion token dataset based on Bloombe rg’s\n",
            "extensive data sources, perhaps the largest domain-speciﬁc dataset y et, augmented with\n",
            "345 billion tokens from general purpose datasets. We validate BloombergGPT on stan-\n",
            "dard LLM benchmarks, open ﬁnancial benchmarks, and a suite of internal be nchmarks\n",
            "that most accurately reﬂect our intended usage. Our mixed dataset train ing leads to a\n",
            "model that outperforms existing models on ﬁnancial tasks \n",
            "\n",
            "Summary: The use of NLP in the realm of ﬁnancial technology is broad and complex. We present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of data. We construct a 363 billion token dataset based on Bloombe rg’sextensive data sources. Our mixed dataset train ing leads to a model that outperforms existing models on a variety of tasks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.2:\n",
            "\n",
            "that most accurately reﬂect our intended usage. Our mixed dataset train ing leads to a\n",
            "model that outperforms existing models on ﬁnancial tasks by signiﬁcan t margins without\n",
            "sacriﬁcing performance on general LLM benchmarks. Additionally, we expl ain our model-\n",
            "ing choices, training process, and evaluation methodology. We release Training Chronicles\n",
            "(Appendix C) detailing our experience in training BloombergGPT.\n",
            "Contents\n",
            "1 Introduction 3\n",
            "1.1BloombergGPT................................ 3\n",
            "1.2 Broader Contributions.............................. 4\n",
            "2 Dataset 5\n",
            "2.1 Financial Datasets (363B tokens – 51.27% of training)............ 7\n",
            "2.1.1 Web (298B tokens – 42.01% of training)................ 7\n",
            "2.1.2 News (38B tokens – 5.31% of training)................. 7\n",
            "2.1.3 Filings (14B tokens – 2.04% of training)................ 7\n",
            "2.1.4 Press (9B tokens – 1.21% of training). \n",
            "\n",
            "Summary: We release Training Chronicles(Appendix C) detailing our experience in training BloombergGPT. Our mixed dataset train ing leads to a model that outperforms existing models on ﬁnancial tasks by signiﬁcan t margins. We expl ain our model-                ing choices, training process, and evaluation methodology. We also detail our training process and Evaluation methodology. The training process is described in the appendix.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.3:\n",
            ".............. 7\n",
            "2.1.4 Press (9B tokens – 1.21% of training)................. 8\n",
            "2.1.5 Bloomberg (5B tokens – 0.70% of training).............. 8\n",
            "2.2 Public Datasets (345B tokens – 48.73% of training).............. 9\n",
            "2.2.1 The Pile (184B tokens – 25.9% of training).............. 9\n",
            "2.2.2 C4 (138B tokens – 19.48% of training)................. 9\n",
            "2.2.3 Wikipedia (24B tokens – 3.35% of training).............. 9\n",
            "2.3 Tokenization................................... 9\n",
            "∗. Co-ﬁrst authors. Corresponding author email: gmann16@bloomberg.net\n",
            "1arXiv:2303.17564v2  [cs.LG]  9 May 20233 Model 11\n",
            "3.1 Architecture..................... \n",
            "\n",
            "Summary: summarize:.............. 7                2.1.4 Press (9B tokens – 1.21% of training)................. 8                1.5 Bloomberg (5B tokens - 0.70% ofTraining).............. 8                3.1 Architecture (The Pile, Wikipedia, Public Datasets, C4) (184B tokens, 48.73% of Training)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.4:\n",
            "LG]  9 May 20233 Model 11\n",
            "3.1 Architecture.................................... 11\n",
            "3.2 Model Scaling................................... 12\n",
            "3.3 Training Conﬁguration.............................. 13\n",
            "3.4 Large-scale Optimization............................. 14\n",
            "4 Training Run 15\n",
            "5 Evaluation 16\n",
            "5.1 Few-shot Methodology.............................. 18\n",
            "5.2 Heldout Loss................................... 18\n",
            "5.3 Financial Tasks.................................. 19\n",
            "5.3.1 External Financial Tasks........................ 20\n",
            "5.3.2 \n",
            "\n",
            "Summary: summarize: LG]  9 May 20233 Model 11 grotesque3.3 Training Conﬁguration.............................. 13 purposefully3.4 Large-scale Optimization............................. 14 purposefully3,5 Evaluation 16 grotesque5.3 Financial Tasks.................................. 19 grotesque5,3.1 External Financial T tasks........................ 20 grotesque5,.3.2 Heldout Loss................................... 18 progressively5.4\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.5:\n",
            " External Financial Tasks........................ 20\n",
            "5.3.2 Internal Task: Sentiment Analysis................... 22\n",
            "5.3.3 Exploratory Task: NER......................... 23\n",
            "5.4 BIG-bench Hard................................. 26\n",
            "5.5 Knowledge Assessments............................. 26\n",
            "5.6 Reading Comprehension............................. 28\n",
            "5.7 Linguistic Tasks.................................. 29\n",
            "5.8 Summary..................................... 30\n",
            "6 Qualitative Samples 31\n",
            "7 Related Work 32\n",
            "8 Ethics, Limitations, and Implications 37\n",
            "8.1 Ethical Use............... \n",
            "\n",
            "Summary: summarize:  External Financial Tasks........................ 20 encompasses. Internal Task: Sentiment Analysis........................ 22 encompasses. Exploratory Task: NER........................ 23 encompasses. Big-bench Hard................................. 26 encompasses. Linguistic Tasks.................................. 29 encompasses. Qualitative Samples 31 encompasses. Related Work 32 encompasses. Ethics, Limitations, and Implications 37 encompasses. Ethical Use.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.6:\n",
            " Work 32\n",
            "8 Ethics, Limitations, and Implications 37\n",
            "8.1 Ethical Use.................................... 37\n",
            "8.2 Openness..................................... 38\n",
            "9 Conclusion 38\n",
            "A Architecture 61\n",
            "A.0 Notation...................................... 61\n",
            "A.1 Full Architecture................................. 61\n",
            "A.2 SelfAttention with ALiBi (SA)......................... 62\n",
            "A.3 LayerNorm (LN)................................. 63\n",
            "A.4 FeedForwardNetwork (FFN)........................... 63\n",
            "A.5 List of All Trainable Parameters....................... \n",
            "\n",
            "Summary: summarize:  Work 32 grotesque8 Ethics, Limitations, and Implications 37 grotesque8.1 Ethical Use.................................... 37                8.2 Openness..................................... 38 grotesque9 Conclusion 38 grotesqueA Architecture 61 grotesqueA.0 Notation...................................... 61                A.1 Full Architecture................................. 61 grotesquea.2 SelfAttention with ALiBi (SA)......................... 62 grotesqueaA.3 LayerNorm (LN)................................. 63 grotesquea a.4 FeedForwardNetwork (FFN)................................... 63 grotesquealA.5 List of All Trainable parameters........................... 63 proprietaryA.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.7:\n",
            ". 63\n",
            "A.5 List of All Trainable Parameters........................ 64\n",
            "B Details on external ﬁnancial tasks 65\n",
            "C Training Chronicles 67\n",
            "C.0 Still\n",
            "...................................... 67\n",
            "C.1 Elbow\n",
            "..................................... 68\n",
            "C.2 Slide\n",
            "...................................... 71\n",
            "C.3 Suspense\n",
            "................................... 72\n",
            "21 Introduction\n",
            "The release of GPT-3 in 2020 (Brown et al., 2020) demonstrated the powerful beneﬁts\n",
            "of training very large auto-regressive language models (LLMs). GPT-3 had 175 b illion\n",
            "parameters, a hundredfold increase over the previous GPT-2 model, and did remarkably\n",
            "well across a wide range of now popular LLM tasks, including reading compre hension,\n",
            "open-ended question answering, and code generation. This performance has been replicated\n",
            "across several other models (Chow \n",
            "\n",
            "Summary: The release of GPT-3 in 2020 (Brown et al., 2020) demonstrated the powerful beneﬁts of training very large auto-regressive language models (LLMs) GPT3 had 175 b illionparameters, a hundredfold increase over the previous G PT-2 model, and did remarkably well across a wide range of now popular LLM tasks. This performance has been replicated across several other models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.8:\n",
            " tasks, including reading compre hension,\n",
            "open-ended question answering, and code generation. This performance has been replicated\n",
            "across several other models (Chowdhery et al., 2022; Scao et al., 2022; Zhang et al., 2022a).\n",
            "Furthermore, evidence suggests that large models exhibit emergent behaviors; growth allows\n",
            "them to acquire abilities not present in smaller models (Wei et al., 2022a). A notable\n",
            "example of emergent behavior is the ability to perform tasks via few- shot prompting, where a\n",
            "model can learn a task from just a few examples. This ability improve s well-above random as\n",
            "we increase the size of language models. Broadly speaking, few-shot promp ting dramatically\n",
            "expands the range of tasks supported by models and lowers the barrier t o entry for users\n",
            "seeking automation for new language tasks.\n",
            "After GPT-3, models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 b il-\n",
            "lion (PaLM, Chowdhery et al., 2022), and 1 trillion parameters (Megatron, Kort hikanti\n",
            "et al., 2022). Work also explored other important aspects of achieving a high- performing\n",
            "LLM, such as diﬀerent training objectives (Tay et al., 2022b), multilin gual models (Scao\n",
            "et al., 2022), more eﬃcient and smaller models (Black et al., 2022), and ﬁnding data and\n",
            "parameter-eﬃcient training sizes (Hoﬀmann et al., 2022).\n",
            "These eﬀorts have almost \n",
            "\n",
            "Summary: After GPT-3, models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 b il-ion (PaLM, Chowdhery et al. 2022), and 1 trillion parameters (Megatron, Kort hikantiet al., 2022) These e-learning models can perform tasks via few-shot prompting. This ability improve s well-above random as we increase the size of language models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.9:\n",
            "nding data and\n",
            "parameter-eﬃcient training sizes (Hoﬀmann et al., 2022).\n",
            "These eﬀorts have almost exclusively focused on general LLMs, trained on datasets\n",
            "that cover a broad range of topics and domains. While these have included some datasets\n",
            "for specialized domains (e.g., code (Chen et al., 2021a) or biomedical articl es (Gao et al.,\n",
            "2021)) the focus has been on building LLMs with broad capabilities. Recent eﬀorts training\n",
            "models using only domain-speciﬁc data have yielded models that, w hile much smaller, beat\n",
            "general purpose LLMs on tasks within those domains, such as science (Tayl or et al., 2022)\n",
            "and medicine (Bolton et al., 2023; Luo et al., 2022; Lehman et al., 2023). These ﬁndings\n",
            "motivate further development of models focused on speciﬁc domains.\n",
            "Financial Technology (FinTech) is a large and growing area with NLP technol ogies\n",
            "having an increasingly important role (Xing et al., 2018; Fisher et al., 2016; Dr edze et al.,\n",
            "2016). Financial NLP tasks (Shah et al., 2022) include sentiment analysis (Araci, 2019),\n",
            "named entity recognition (Salinas Alvarado et al., 2015), news classiﬁcation ( Sinha and\n",
            "Khandait, 2020), and question answering (Chen et al., 2021b, 2022). While the range of\n",
            "tasks is similar to those found in general NLP \n",
            "\n",
            "Summary: Recent NLP models have almost exclusively focused on general LLMs, trained on datasets that cover a broad range of topics and domains. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. These ﬁndingsmotivate further development of models focused on speciﬃc domains.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.10:\n",
            "handait, 2020), and question answering (Chen et al., 2021b, 2022). While the range of\n",
            "tasks is similar to those found in general NLP benchmarks, the complexit y and terminology\n",
            "of the ﬁnancial domain warrant a domain-speciﬁc system. For all of the reason s generative\n",
            "LLMs are attractive in general – few-shot learning, text generation, conver sational systems,\n",
            "etc. – it would be valuable to have a LLM focused on the ﬁnancial domain. Wh ile there are\n",
            "masked language models tuned for the ﬁnancial domain (Araci, 2019), no LLM has been\n",
            "tuned for or evaluated on tasks for this domain.\n",
            "1.1 BloombergGPT\n",
            "We train BloombergGPT, a 50 billion parameter language model that supports a wide\n",
            "range of tasks within the ﬁnancial industry. Rather than building a gen eral-purpose LLM,\n",
            "or a small LLM exclusively on domain-speciﬁc data, we take a mixed approach. General\n",
            "3models cover many domains, are able to perform at a high level across a wid e variety of tasks,\n",
            "and obviate the need for specialization during training time. However, results from existing\n",
            "domain-speciﬁc models show that general models cannot replace them. At Bloomberg, we\n",
            "support a very large and diverse set of tasks, well served by a general m odel, but the vast\n",
            "majority of our applications are within the ﬁnancial domain, better serv ed by a speciﬁc\n",
            "model. For that reason \n",
            "\n",
            "Summary: No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain. We train BloombergGPT, a 50 billion parameter language model that supports a wide                range of tasks within the industry. We take a mixed approach to LLM development. General models cover many domains, are able to perform at a high level across a wid e variety of tasks and obviate the need for specialization during training time.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.11:\n",
            " but the vast\n",
            "majority of our applications are within the ﬁnancial domain, better serv ed by a speciﬁc\n",
            "model. For that reason, we set out to build a model that achieves best- in-class results on\n",
            "ﬁnancial benchmarks, while also maintaining competitive performanc e on general-purpose\n",
            "LLM benchmarks.\n",
            "We achieve this goal by constructing the largest domain-speciﬁc dataset yet, drawing on\n",
            "existing data creation, collection, and curation resources at Bloomber g. As Bloomberg is\n",
            "primarily a ﬁnancial data company, our data analysts have collected and cu rated ﬁnancial\n",
            "language documents over the span of forty years. We have extensive arch ives of ﬁnancial\n",
            "data that cover a range of topics, with careful tracking of data sources and usage rights. We\n",
            "add this data to public datasets to create a large training corpus with o ver 700 billion tokens.\n",
            "Using a portion of this training corpus, we train a BLOOM-style, 50 bill ion parameter\n",
            "model designed based on guidelines from Hoﬀmann et al. (2022) and Le Scao et al. (2022).\n",
            "We validate the model on standard LLM benchmarks, open ﬁnancial benchmark s, and a\n",
            "suite of Bloomberg-internal benchmarks that most accurately reﬂect ou r intended use cases.\n",
            "Our results demonstrate that our mixed training approach leads to a mod el that vastly\n",
            "outperforms existing models on in-domain ﬁnancial tasks while being on par or better on\n",
            "general NLP benchmarks.\n",
            "1. \n",
            "\n",
            "Summary: Bloomberg is primarily a ﬁnancial data company. We set out to build a model that achieves best- in-class results on ﬂancial benchmarks, while maintaining competitive performanc e on general-purpose LLM benchmarks. We built the largest domain-speciﬁc dataset yet, drawing onexisting data creation, collection, and curation resources at Bloomber g.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.12:\n",
            " el that vastly\n",
            "outperforms existing models on in-domain ﬁnancial tasks while being on par or better on\n",
            "general NLP benchmarks.\n",
            "1.2 Broader Contributions\n",
            "Beyond the construction of a LLM for ﬁnancial data, our goal is to contribute to t he\n",
            "broader research community. Speciﬁcally, our experience documen ted in this paper provides\n",
            "evidence that further develops the community’s understanding of several open questions in\n",
            "the literature.\n",
            "Domain-speciﬁc LLMs. The few existing domain-speciﬁc LLMs are trained exclusively\n",
            "on domain-speciﬁc data sources (Luo et al., 2022; Bolton et al., 2023; Taylor et al., 2022),\n",
            "or adapt a very large general purpose model to domain-speciﬁc tasks (Singh al et al., 2022;\n",
            "Lewkowycz et al., 2022). Our alternative approach – training an LLM on both domain-\n",
            "speciﬁc and general data sources – has not been studied so far. The resu lting model does very\n",
            "well on domain-speciﬁc tasks, but also maintains strong performance on ge neral-purpose\n",
            "benchmarks.\n",
            "Training data. Nearly all language models rely in large part on web-scraped data, such\n",
            "as C4 (Raﬀel et al., 2020) and The Pile (Gao et al., 2021) (which includes OpenW ebText2).\n",
            "This data may be cleaned or subsetted in various ways before use (Touv ron et \n",
            "\n",
            "Summary: This paper describes an alternative approach to training language models on web-scraped data. The resu lting model does very well on domain-speciﬁc tasks, but also maintains strong performance on ge neral-purposebenchmarks. The paper provides evidence that further develops the community’s understanding of several open questions in the literature. We hope to contribute to the research community.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.13:\n",
            " et al., 2021) (which includes OpenW ebText2).\n",
            "This data may be cleaned or subsetted in various ways before use (Touv ron et al., 2023; Rae\n",
            "et al., 2020; Scao et al., 2022; Jernite et al., 2022), but issues of data duplication (Carlini\n",
            "et al., 2020) and toxic language remain (Welbl et al., 2021). Our training data is u nusual\n",
            "for LLM training in that it includes a signiﬁcant amount of curated and prep ared data from\n",
            "reliable sources.\n",
            "Evaluation. LLM evaluation remains a challenging and evolving problem (Gehrmann\n",
            "et al., 2022; Goyal et al., 2022), with new benchmarks trying to standardize e valuation\n",
            "4across models (Liang et al., 2022; Srivastava et al., 2022). However, for domain-s peciﬁc\n",
            "tasks, there remains a mismatch between evaluation and actual use cases. Evaluations are\n",
            "built on available datasets and not necessarily on how the model will be used in practice.\n",
            "We provide results on both public ﬁnancial NLP benchmarks (Shah et al., 2022; Chen et al.,\n",
            "2021b) as well as a selection of internal Bloomberg tasks, which are better aligned with our\n",
            "intended use cases and directly evaluate our model’s ability to per form tasks of interest.\n",
            "Model Size. Early LLMs made a single training pass over a corpus of 200-400 billion to-\n",
            "kens (Brown et al., 2020) and Hoﬀmann et al. (2022 \n",
            "\n",
            "Summary: Our training data is u nusualfor LLM training in that it includes a signiﬁcant amount of curated and prep ared data from reliable sources. Early LLMs made a single training pass over a corpus of 200-400 billion to-kens. We provide results on both public NLP benchmarks and internal Bloomberg tasks, which are better aligned with our intended use cases.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.14:\n",
            " a single training pass over a corpus of 200-400 billion to-\n",
            "kens (Brown et al., 2020) and Hoﬀmann et al. (2022) posited that models were unde rtrained,\n",
            "instead focusing on training smaller models with more data, a strategy most recently em-\n",
            "ployed by Touvron et al. (2023). We select a model size motivated by Hoﬀmann et al. (2022)\n",
            "and train a 50 billion parameter model on 569 billion tokens from our corpus of o ver 700\n",
            "billion tokens to produce a model that is competitive with larger mo dels.\n",
            "Tokenizer. After assembling training data, the critical step of tokenization trans forms\n",
            "the text into a format suitable for the language model. The importance of t his step is\n",
            "often overlooked (Mielke et al., 2021), and many older LLMs use the same token izer and\n",
            "vocabulary, meaning that we have little evidence to support other t okenizers. We take\n",
            "a diﬀerent approach and use a Unigram model instead of greedy merge-based s ub-word\n",
            "tokenizers since it saves probabilities allowing for smarter token ization at inference time\n",
            "(Kudo, 2018).\n",
            "Model Building Challenges. GPT-3 and subsequent models were the work of large\n",
            "teams and required an enormous amount of computation. Initial work to repro duce these\n",
            "results, such as OPT (Zhang et al., 2022a), did not match the performance of t he original\n",
            "model. With the release of each subsequent model, the community’ s understanding, ex-\n",
            "perience, and software tools increase. In developing Bloomberg \n",
            "\n",
            "Summary: We train a 50 billion parameter model on 569 billion tokens from our corpus of o ver 700                billion tokens to produce a model that is competitive with larger mo dels. We take a Unigram model instead of greedy merge-based s ub-word-based tokenizers since it saves probabilities allowing for smarter token ization at inference time. GPT-3 and subsequent models were the work of large                teams and required an enormous amount of computation.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.15:\n",
            " t he original\n",
            "model. With the release of each subsequent model, the community’ s understanding, ex-\n",
            "perience, and software tools increase. In developing BloombergGPT, we beneﬁted from\n",
            "existing code developed as part of the BLOOM eﬀort (Scao et al., 2022), sho wing that a\n",
            "moderately sized team can produce a competitive model on domain-spe ciﬁc data. We de-\n",
            "scribe our experiences training BloombergGPT in detail to support future training eﬀorts\n",
            "and address each of the above topics.\n",
            "2 Dataset\n",
            "To train BloombergGPT, we construct “ FinPile ”, a comprehensive dataset consisting of\n",
            "a range of English ﬁnancial documents including news, ﬁlings, press releases, web-scraped ﬁ-\n",
            "nancial documents, and social media drawn from the Bloomberg archiv es. These documents\n",
            "have been acquired through our business process over the past two d ecades. We augment\n",
            "FinPile with public data widely used to train LLMs. The result is a training corpus that\n",
            "is roughly half domain-speciﬁc text and half general-purpose text. For a breakdown of the\n",
            "full training set, see Table 1. To improve data quality, we de-dupl icate each dataset (The\n",
            "Pile, C4, Wikipedia, FinPile ) according to Lee et al. (2022a); as a side-eﬀect, the statistics\n",
            "reported in Table 1 might be diﬀerent from those reported in other pape rs.\n",
            "5 \n",
            "\n",
            "Summary: We train BloombergGPT from a comprehensive dataset of news, press releases, web-scraped documents, and social media drawn from the Bloomberg archiv es. To improve data quality, we de-duplicate each dataset according to Lee et al. (2022a) to improve quality. We address each of the above topics in detail to support future training eﬀorts.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.16:\n",
            " as a side-eﬀect, the statistics\n",
            "reported in Table 1 might be diﬀerent from those reported in other pape rs.\n",
            "5DatasetDocs\n",
            "1e4C/DChars\n",
            "1e8C/TToks\n",
            "1e8T%\n",
            "FinPile 175,886 1,017 17,883 4.92 3,635 51.27%\n",
            "Web 158,250 933 14,768 4.96 2,978 42.01%\n",
            "News 10,040 1,665 1,672 4.44 376 5.31%\n",
            "Filings 3,335 2,340 780 5.39 145 2.04%\n",
            "Press 1,265 3,443 435 5.06 86 1.21%\n",
            "Bloomberg 2,996 758 227 4.60 49 0.70%\n",
            "PUBLIC 50,744 3,314 16,818 4.87 3,454 48.73%\n",
            "C4 34,832 2,206 7,683 5.56 1,381 19.48%\n",
            "Pile-CC 5,255 4,401 2,312 5.42 427 6.02%\n",
            "GitHub 1,428 5,364 766 3.38 227 3.20%\n",
            "Books3 19 552,398 1,064 4.97 214 3.02%\n",
            "PubMed Central 294 32,181 947 4.51 210 2.96%\n",
            "ArXiv 124 47,819 591 3.56 166 2.35%\n",
            "OpenWebText2 1,684 3,850 648 5.07 128 1.80%\n",
            "FreeLaw 349 15,381 \n",
            "\n",
            "Summary: Summarize:  as a side-eﬀect, the statistics.reported in Table 1 might be diﬄerent from those reported in other pape.rs. The statistics in this section are based on data from the OpenWebText 2.5 DatasetDocs. The data in this article is based on the Open WebText 2Dataset Docs.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.17:\n",
            ".56 166 2.35%\n",
            "OpenWebText2 1,684 3,850 648 5.07 128 1.80%\n",
            "FreeLaw 349 15,381 537 4.99 108 1.52%\n",
            "StackExchange 1,538 2,201 339 4.17 81 1.15%\n",
            "DM Mathematics 100 8,193 82 1.92 43 0.60%\n",
            "Wikipedia (en) 590 2,988 176 4.65 38 0.53%\n",
            "USPTO Backgrounds 517 4,339 224 6.18 36 0.51%\n",
            "PubMed Abstracts 1,527 1,333 204 5.77 35 0.50%\n",
            "OpenSubtitles 38 31,055 119 4.90 24 0.34%\n",
            "Gutenberg (PG-19) 3 399,351 112 4.89 23 0.32%\n",
            "Ubuntu IRC 1 539,222 56 3.16 18 0.25%\n",
            "EuroParl 7 65,053 45 2.93 15 0.21%\n",
            "YouTubeSubtitles 17 19,831 33 2.54 13 0.19%\n",
            "BookCorpus2 2 370,384 65 5.36 12 0.17%\n",
            "HackerNews 82 5,009 41 4.87 8 0.12%\n",
            "PhilPapers 3 74,827 23 4.21 6 0.08%\n",
            "NIH ExPorter 92 2,165 20 6.65 3 0.04%\n",
            "Enron Emails 24 1,882 5 3.90 1 0.02%\n",
            "Wikipedia (7/1/22) 2,218 3,271 726 3.06 237 3.35%\n",
            " \n",
            "\n",
            "Summary: Summarize:.56 166 2.35%OpenWebText2 1,684 3,850 648 5.07 128 1.80%FreeLaw 349 15,381 537 4.99 108 1.52%StackExchange 1,538 2,201 339 4.17 81 1.15%DM Mathematics 100 8,193 82 1.92 43 0.60%Wikipedia (en) 590 2,988 176 4.65 38 0.53%USPTO Backgrounds 517 4,339 224 6.18 36 0.51%PubMed Abstracts 1,527 1,333 204 5.77 35 0.50%OpenSubtitles 38 31,055 119 4.90 24 0.34%Gutenberg\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.18:\n",
            " 5 3.90 1 0.02%\n",
            "Wikipedia (7/1/22) 2,218 3,271 726 3.06 237 3.35%\n",
            "TOTAL 226,631 1,531 34,701 4.89 7,089 100.00%\n",
            "Table 1: Breakdown of the full training set used to train BloombergGPT. The statistics\n",
            "provided are the average number of characters per document (“C/D”), th e average\n",
            "number of characters per token (“C/T”), and the percentage of the overall t okens\n",
            "(“T%”). Units for each column are denoted in the header.\n",
            "62.1 Financial Datasets (363B tokens – 51.27% of training)\n",
            "The Bloomberg Terminal has provided access to a comprehensive set of diverse structured\n",
            "and unstructured ﬁnancial data and analytics for the past four decades. I n serving this\n",
            "mission, Bloomberg analysts have curated a set of ﬁnancial documents that were either\n",
            "created internally or acquired from external sources. We utilize th is extensive collection of\n",
            "curated and maintained documents to create FinPile, which consists of company ﬁlings,\n",
            "ﬁnancial news, and other data relevant to the ﬁnancial markets.\n",
            "Some documents included in the FinPile, such as company ﬁlings, are available to\n",
            "the general public, although collecting these documents and pre-pr ocessing them for LLM\n",
            "training is a non-trivial task. Other documents, such as (a subset of) Bloomberg news, must\n",
            "be purchased. The rest \n",
            "\n",
            "Summary: Summarize:  5 3.90 1 0.02%                Wikipedia (7/1/22) 2,218 3,271 726 3.06 237 3.35%                TOTAL 226,631 1,531 34,701 4.89 7,089 100.00%                Table 1: Breakdown of the full training set used to train BloombergGPT.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.19:\n",
            " them for LLM\n",
            "training is a non-trivial task. Other documents, such as (a subset of) Bloomberg news, must\n",
            "be purchased. The rest of the documents are private and available, amon g other sources,\n",
            "through the Bloomberg Terminal. Finally, we clean this data to strip oﬀ markup, special\n",
            "formatting, and templates.\n",
            "Note that each document in FinPile is time-stamped, with dates ranging from 2007-\n",
            "03-01 to 2022-07-31; the quality and quantity of documents increase over this t ime range.\n",
            "While we do not utilize date information in this work, we plan to use it in the future,\n",
            "such as for evaluation of what the model learns about diﬀerent time perio ds. While we\n",
            "cannot release FinPile, our experience training on a large, carefully curated, and clean\n",
            "domain-speciﬁc dataset may provide helpful insights to the commun ity on the advantages\n",
            "and challenges of building a ﬁnancial LLM in particular, and a domain-speci ﬁc model in\n",
            "general. We provide a breakdown and analysis of FinPile in Table 2 and a brief description\n",
            "of the types of data included below.\n",
            "2.1.1 Web (298B tokens – 42.01% of training)\n",
            "Bloomberg collects web content by identifying sites that contain ﬁ nancially relevant infor-\n",
            "mation. While this category makes up the majority of FinPile, its classiﬁcations are rough,\n",
            "with content classiﬁed mainly by the location of the web domain. \n",
            "\n",
            "Summary: Bloomberg collects web content by identifying sites that contain ﬁ nancially relevant infor-                mation. While this category makes up the majority of FinPile, its classiﬁcations are rough. The rest of the documents are private and available, amon g other sources, through the Bloomberg Terminal. We clean this data to strip oﬀ markup, specialformatting, and templates.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.20:\n",
            " majority of FinPile, its classiﬁcations are rough,\n",
            "with content classiﬁed mainly by the location of the web domain. Withi n these location-\n",
            "speciﬁc sources, e.g. “US” (15.95% of total), “Asia-Pac” (4.72% of total), and “UK” (1.98%\n",
            "of total), document types are highly varied as would be expected from a w eb crawl. While\n",
            "web sources are common in existing public LLM training datasets, Bloomb erg’s web crawl\n",
            "is focused on high-quality websites that have ﬁnancially relevant i nformation, as opposed\n",
            "to a general-purpose crawl of the web.\n",
            "2.1.2 News (38B tokens – 5.31% of training)\n",
            "The News category includes all news sources excluding news articles written by Bloomberg\n",
            "journalists. Overall, there are hundreds of English news sources in FinPile including\n",
            "“Bloomberg Transcripts” (0.41% of total), which are transcripts of Bloomb erg TV news.\n",
            "Generally, the content in this dataset comes from reputable sources of news that are relevant\n",
            "to the ﬁnancial community so as to maintain factuality and reduce bias.\n",
            "2.1.3 Filings (14B tokens – 2.04% of training)\n",
            "Company Filings are ﬁnancial statements prepared by (public) companie s and made avail-\n",
            "able to the general public. In some countries, like the US, public com panies are mandated\n",
            "7Date Bloomberg \n",
            "\n",
            "Summary: The News category includes all news sources excluding news articles written by Bloomberg journalists. The Company Filings category includes financial statements prepared by (public) companies and made avail-able to the general public. The web crawl is focused on high-quality websites that have ﬁnancially relevant i nformation, as opposed to a general-purpose crawl of the web. The content in this dataset comes from reputable sources of news that are relevant to the financial community.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.21:\n",
            " (public) companie s and made avail-\n",
            "able to the general public. In some countries, like the US, public com panies are mandated\n",
            "7Date Bloomberg Filings News Press Web Total\n",
            "2007 [03-] 276 73 892 523 2,667 4,431\n",
            "2008 351 91 1,621 628 9,003 11,695\n",
            "2009 293 93 1,791 528 9,179 11,883\n",
            "2010 292 111 1,917 527 11,388 14,236\n",
            "2011 335 117 2,264 548 13,643 16,907\n",
            "2012 403 105 2,502 529 15,015 18,554\n",
            "2013 415 87 2,437 441 17,230 20,610\n",
            "2014 396 251 2,458 437 18,510 22,052\n",
            "2015 358 1,639 2,371 427 20,782 25,576\n",
            "2016 324 1,891 2,509 418 24,337 29,478\n",
            "2017 294 2,294 2,567 398 25,283 30,837\n",
            "2018 275 1,791 2,702 420 26,027 31,214\n",
            "2019 263 1,662 3,102 504 27,195 32,726\n",
            "2020 277 1,632 2,794 805 30,928 36,435\n",
            "2021 247 1,767 3,515 938 29,749 36,215\n",
            "2022 [-07] 140 882 2,206 531 16,872 20,631\n",
            "4,939 14,486 37,647 8,602 297,807 363,482\n",
            "Table 2: The number of tokens (in millions) contained within documen ts \n",
            "\n",
            "Summary: summarize:  (public) companie s and made avail-                able to the general public. In some countries, like the US, public com panies are mandated.Date Bloomberg Filings News Press Web Total 2007 [03-] 276 73 892 523 2,667 4,431 2008 351 91 1,621 628 9,003 11,695.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.22:\n",
            ",939 14,486 37,647 8,602 297,807 363,482\n",
            "Table 2: The number of tokens (in millions) contained within documen ts inFinPile, or-\n",
            "ganized by year (rows) and type (column). Units are millions of tokens.\n",
            "to prepare and submit their ﬁnancial statements on a regular cadence; e.g., 10-K annual\n",
            "reports and 10-Q quarterly reports. In our dataset, a majority of the ﬁlin gs come from\n",
            "EDGAR, which is the SEC’s online database (1.90% of total). Filings are typi cally long\n",
            "PDF documents with tables and charts that are dense in ﬁnancial inform ation, which are\n",
            "processed and normalized in Bloomberg. Filings are substantially diﬀ erent from the types\n",
            "of documents typically used to train LLMs, but contain critically imp ortant information for\n",
            "ﬁnancial decision-making.\n",
            "2.1.4 Press (9B tokens – 1.21% of training)\n",
            "The Press category contains press releases typically issued by compan ies that are ﬁnancially\n",
            "relevant. Taken together with ﬁlings, press releases represent mos t of the public communi-\n",
            "cations of a company. However, unlike ﬁlings, press releases are simil ar to news stories in\n",
            "terms of content and style.\n",
            "2.1.5 Bloomberg (5B tokens – 0.70% of training)\n",
            "This category comprises Bloomberg authored news and other documents s uch as opinions\n",
            "and analyses. The largest sources are � \n",
            "\n",
            "Summary: 1.1 Press (9B tokens – 1.21% of training)1.2 Bloomberg (5B tokens - 0.70% ofTraining)1:3 SEC (1.90% of total) and 1:4 Press (0.70%) The Press category contains press releases typically issued by companies that are ﬁnanciallyrelevant. The Bloomberg category comprises Bloomberg authored news and other documents.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.23:\n",
            "5B tokens – 0.70% of training)\n",
            "This category comprises Bloomberg authored news and other documents s uch as opinions\n",
            "and analyses. The largest sources are “Bloomberg News” (0.44% of total) and “Bloom berg\n",
            "First Word” (0.13% of total), the Bloomberg-authored wire of real-time new s. While\n",
            "Bloomberg News covers a wide range of topics, it typically focuses on c ontent relevant\n",
            "to the ﬁnancial community. This dataset contains documents of varying lengths.\n",
            "82.2 Public Datasets (345B tokens – 48.73% of training)\n",
            "We use three widely known and available public datasets in our traini ng corpus.\n",
            "2.2.1 The Pile (184B tokens – 25.9% of training)\n",
            "The Pile (Gao et al., 2021) is the dataset used in GPT-Neo (Black et al., 2021), G PT-\n",
            "J (Wang and Komatsuzaki, 2021), and GPT-NeoX (20B) (Black et al., 2022). We include\n",
            "The Pile in our training data for the following reasons. First, it has b een used to successfully\n",
            "train an LLM. Second, it has undergone signiﬁcant data cleaning and pre-pro cessing. Third,\n",
            "it includes multiple domains and we believe such diverse data wil l aid generalization to new\n",
            "domains and may even support training on ﬁnancial data. For example, domains such as\n",
            "FreeLaw and GitHub are useful to teams at Bloomberg that work on legal docum ents \n",
            "\n",
            "Summary: This category comprises Bloomberg authored news and other documents s uch as opinions and analyses. The largest sources are “Bloomberg News’ (0.44% of total) and “Bloom berg berg First Word’s’ 0.13%. We use three widely known and available public datasets in our traini ng corpus. The Pile (184B tokens – 25.9% of training) is the dataset used in GPT-Neo (Black et al., 2021)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.24:\n",
            " support training on ﬁnancial data. For example, domains such as\n",
            "FreeLaw and GitHub are useful to teams at Bloomberg that work on legal docum ents and\n",
            "software development, respectively. Creators of The Pile have del iberately chosen to include\n",
            "duplicate content, with the duplication factor being proportional to t he perceived quality\n",
            "of the content. However, as we deduplicate each of our datasets, the size of The Pile is\n",
            "signiﬁcantly reduced. Additionally, note that our tokenizer ( §2.3) is trained on The Pile.\n",
            "2.2.2 C4 (138B tokens – 19.48% of training)\n",
            "The Colossal Clean Crawled Corpus (C4) is a common dataset used to train LLMs, and was\n",
            "introduced to support training T5 (Raﬀel et al., 2020). Although it overl aps with Pile-CC,\n",
            "C4 is cleaned and processed diﬀerently; hence, we feel that incl uding C4 in addition to\n",
            "The Pile can add value more than duplicated documents would. We ﬁnd th at C4 contains\n",
            "high-quality natural language documents due to the layers of cleaning, t hough others have\n",
            "noted that the distribution across web domains is unusual, with a high f raction of data\n",
            "stemming from patents (Dodge et al., 2021).\n",
            "2.2.3 Wikipedia (24B tokens – 3.35% of training)\n",
            "Both The Pile and C4 include out-of-date copies of Wikipedia, so it coul d be bene� \n",
            "\n",
            "Summary: The Pile and C4 include out-of-date copies of Wikipedia. The Colossal Clean Crawled Corpus (C4) is a common dataset used to train LLMs. We feel that incl uding C4 in addition to The Pile can add value more than duplicated documents would. For example, domains such asFreeLaw and GitHub are useful to teams at Bloomberg that work on legal docum ents and software development.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.25:\n",
            " tokens – 3.35% of training)\n",
            "Both The Pile and C4 include out-of-date copies of Wikipedia, so it coul d be beneﬁcial for\n",
            "the factuality of the model to have up-to-date Wikipedia pages inclu ded. Therefore, we\n",
            "include a dump of English Wikipedia from July 1, 2022. This dataset is toke nized quite\n",
            "ineﬃciently (3.06 characters per token), indicating an above-average amount of markup,\n",
            "which suggests that further cleaning might beneﬁt future model tr aining.\n",
            "2.3 Tokenization\n",
            "We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word\n",
            "tokenizer, such as Byte Pair Encoding (BPE) (Sennrich et al., 2016) or W ordpiece (Schuster\n",
            "and Nakajima, 2012; Wu et al., 2016), based on promising results in Kudo and Richar dson\n",
            "(2018) and Bostrom and Durrett (2020). Following GPT-2 (Radford et al., 2019), we tr eat\n",
            "our data as a sequence of bytes rather than Unicode characters, and we inc lude each of the\n",
            "256 bytes as tokens. In a pretokenization step, the input byte sequen ce is broken into chunks\n",
            "by greedily matching the following regular expression: [ A-Za-z]+|[0-9]|[^A-Za-z0-9]+.\n",
            "This follows GPT-2 in preventing multiple character classes from appearing in a single token.\n",
            "However, we include spaces \n",
            "\n",
            "Summary: We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer. The dataset is toke nized quite pleasantly (3.06 characters per token), indicating an above-average amount of markup. The Pile and C4 include out-of-date copies of Wikipedia. We include a dump of English Wikipedia from July 1, 2022.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.26:\n",
            "-Za-z0-9]+.\n",
            "This follows GPT-2 in preventing multiple character classes from appearing in a single token.\n",
            "However, we include spaces in the alphabetic chunks, which allows multi-word tokens to be\n",
            "learned, increasing information density and reducing context lengt hs. The pretokenization\n",
            "9BLOOM /ours NeoX /ours OPT /ours BloombergGPT\n",
            "FinPile (old) 451 110% 460 112% 456 111% 412\n",
            "C4 166 121% 170 123% 170 123% 138\n",
            "The Pile 203 110% 214 116% 239 130% 184\n",
            "Wikipedia 21 88% 23 99% 24 103% 24\n",
            "Total 390 113% 408 118% 434 126% 345\n",
            "Table 3: Number of tokens in each training dataset with BLOOM, NeoX, OPT (GPT 2),\n",
            "andBloombergGPT tokenizers. All token counts are in billions (B). Note that\n",
            "an older version of FinPile was used for this count, so token numbers wi ll not\n",
            "match earlier tables.\n",
            "follows the approach of PaLM (Chowdhery et al., 2022) in placing each digit in i ts own\n",
            "chunk, with the hope that this will lead to better handling of numbe rs. We train our\n",
            "tokenizer on The Pile (Gao et al., 2021) as it draws from diverse domains, in cluding code\n",
            "and academic papers, in proportions that suit our use case.\n",
            "Parallel Tokenizer Training. The Unigram tokenizer implementation is too ineﬃcient\n",
            "to process the entire Pile dataset at once, so we use a \n",
            "\n",
            "Summary: The tokenizer uses BLOOM, NeoX, OPT (GPT 2), and BloombergGPT tokenizers. The Pile dataset is used to train the tokenizer. The Unigram tokenizer implementation is too ineﬃcient to process the entire Pile datasets at once. The pretokenization is based on GPT-2, but includes spaces in the alphabetic chunks, which allows multi-word tokens to be learned.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.27:\n",
            "allel Tokenizer Training. The Unigram tokenizer implementation is too ineﬃcient\n",
            "to process the entire Pile dataset at once, so we use a split and merge approach. We split\n",
            "each of the 22 domains in the Pile into 256 chunks of roughly equal size. We t hen train\n",
            "a Unigram tokenizer with a vocabulary size of 65,536 (216) on each of the 22 ×256 (total\n",
            "= 5,632) chunks. We hierarchically merge the individual tokenizers by ﬁr st merging the\n",
            "256 tokenizers from each domain, and then combining the 22 resulting toke nizers to get the\n",
            "ﬁnal tokenizer.\n",
            "Unigram tokenizers amount to probability distributions over tokens (i.e. unigram lan-\n",
            "guage models), and we merge tokenizers by taking a weighted average of the probabilities\n",
            "of corresponding tokens, with the weights determined by the relati ve sizes (in bytes) of\n",
            "the data used to train the tokenizers. The result is a tokenizer wi th 7 million tokens. To\n",
            "reduce the size of the vocabulary to 217tokens, we drop the tokens with the smallest prob-\n",
            "abilities and renormalize. To ensure we do not need an out-of-vocabular y token, we also\n",
            "add as tokens the 36 (of 256 possible) bytes that do not occur in The Pile, al ong with an\n",
            "<|endoftext|> token.\n",
            "There are various considerations in choosing the vocabulary size. One advantage of a\n",
            "large vocabulary for LLMs is that more information can ﬁt into the context win dow. \n",
            "\n",
            "Summary: The Unigram tokenizer implementation is too ineﬃcient to process the entire Pile dataset at once. We spliteach of the 22 domains in the Pile into 256 chunks of roughly equal size. To ensure we do not need an out-of-vocabular y token, we also add as tokens the 36 (of 256 possible) bytes that do not occur in The Pile.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.28:\n",
            "There are various considerations in choosing the vocabulary size. One advantage of a\n",
            "large vocabulary for LLMs is that more information can ﬁt into the context win dow. On\n",
            "the other hand, there is overhead with a larger vocabulary: a larger prop ortion of model\n",
            "parameters are required for token embedding. We select our vocabulary size of 217tokens\n",
            "based on experiments with vocabulary ranging from 25,000 to 550,000. For each vocab ulary\n",
            "size, we tokenize the C4 dataset and compute the total size (in bytes) for the dataset, where\n",
            "each token is represented using log2(vocabulary size) bits. Our heuristic is to choose the\n",
            "vocabulary size that leads to the smallest encoded representation of C4. This gives us a\n",
            "vocabulary size of 125,000, which we then round up to the nearest power of 2 ( 217, or 131,072\n",
            "tokens). Our tokenizer is large, relative to the standard vocabulary s ize of approximately\n",
            "50,000 tokens. For an analysis of tokenization eﬃciency, see Table 3.\n",
            "10Shape\n",
            "Number of Layers 70\n",
            "Number of Heads 40\n",
            "Vocabulary Size 131,072\n",
            "Hidden Dimension 7,680\n",
            "Total Parameters 50.6B\n",
            "Hyperparameters\n",
            "Max Learning Rate 6e-5\n",
            "Final Learning Rate 6e-6\n",
            "Learning Rate schedule cosine decay\n",
            "Gradient Clipping 0.3\n",
            "Training\n",
            "Tokens 569B\n",
            "Hardware 64 ×8 A100 40GB\n",
            "Throughput 32.5 sec/step\n",
            "avg. TFLOPs 102\n",
            "total FLOPS 2.36e23\n",
            " \n",
            "\n",
            "Summary: We choose a vocabulary size of 217tokens based on experiments with vocabulary ranging from 25,000 to 550,000. We choose the vocabulary size that leads to the smallest encoded representation of C4. Our tokenizer is large, relative to the standard vocabulary s ize of approximately50,000 tokens. For an analysis of tokenization eﬃciency, see Table 3.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.29:\n",
            " 64 ×8 A100 40GB\n",
            "Throughput 32.5 sec/step\n",
            "avg. TFLOPs 102\n",
            "total FLOPS 2.36e23\n",
            "Table 4: A summary of the hyper-parameters and their values for BloombergGPT.\n",
            "3 Model\n",
            "3.1 Architecture\n",
            "Our model is a decoder-only causal language model based on BLOOM (Scao et al., 2022).\n",
            "We present an overview of the architecture, with full details in Ap pendix A.\n",
            "The model contains 70 layers of transformer decoder blocks deﬁned as follows:\n",
            "¯hℓ=hℓ−1+ SA(LN( hℓ−1))\n",
            "hℓ=¯hℓ+ FFN(LN( ¯hℓ))\n",
            "where SA is multi-head self-attention, LN is layer-normalization, an d FFN is a feed-forward\n",
            "network with 1-hidden layer. Inside FFN, the non-linear function i s GELU (Hendrycks and\n",
            "Gimpel, 2016). ALiBi positional encoding is applied through additive biase s at the self-\n",
            "attention component of the transformer network (Le Scao et al., 2022). The inp ut token\n",
            "embeddings are tied to the linear mapping before the ﬁnal softmax. Foll owing Le Scao\n",
            "et al. (2022) and ﬁrst used in Dettmers et al. (2022), the model has an additional layer\n",
            "normalization after token embeddings, formally:\n",
            "¯h1 \n",
            "\n",
            "Summary: The model is a decoder-only causal language model based on BLOOM (Scao et al., 2022). We present an overview of the architecture, with full details in Ap pendix A. The model contains 70 layers of transformer decoder blocks deﬁned as follows. The inp ut token embeddings are tied to the linear mapping before the softmax.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.30:\n",
            "rst used in Dettmers et al. (2022), the model has an additional layer\n",
            "normalization after token embeddings, formally:\n",
            "¯h1= LNem(h0) + SA(LN(LNem(h0))),\n",
            "whereh0is the initial token embedding and LNemis the new component of embedding layer-\n",
            "normalization. Notice that the second term includes two consecutive l ayer-normalizations.\n",
            "111e22 3.2e22 1e23 3.2e23 1e24 3.2e24\n",
            "FLOPs10205010020050010002000Parameters (B)\n",
            "NeoX LaMDA GPT-3/Jurassic/OPT \n",
            "OPT  GopherMT-NLG \n",
            " BLOOM\n",
            "PaLM PaLM \n",
            " Chinchilla LLaMA\n",
            "LLaMA \n",
            "LLaMA BloombergGPT Optimal # Parameters w.r.t. FLOPs\n",
            "Chinchilla-1\n",
            "Chinchilla-2\n",
            "Chinchilla-3\n",
            "Kaplan\n",
            "1e22 3.2e22 1e23 3.2e23 1e24 3.2e24\n",
            "FLOPs100200500100020005000Tokens (B) NeoX\n",
            " LaMDA GPT-3/Jurassic/OPT  OPT  Gopher MT-NLG BLOOM PaLM  PaLM ChinchillaLLaMA  LLaMA  \n",
            " LLaMA\n",
            " BloombergGPTOptimal # Tokens w.r.t. FLOPs\n",
            "Chinchilla-1\n",
            "Chinchilla-2\n",
            " \n",
            "\n",
            "Summary: The model has an additional layer of normalization after token embeddings, formally:¯¯h1= LNem(h0) + SA(LN(Lnem(H0) - LNemis(H1) H0 is the initial token embedding and LN Artemis is the new component of embedding layer-normalization. Notice that the second term includes two consecutive l ayer-normalizations.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.31:\n",
            " \n",
            " LLaMA\n",
            " BloombergGPTOptimal # Tokens w.r.t. FLOPs\n",
            "Chinchilla-1\n",
            "Chinchilla-2\n",
            "Chinchilla-3\n",
            "Kaplan\n",
            "Figure 1: Kaplan et al. (2020) and Chinchilla scaling laws with prior large lan guage model\n",
            "andBloombergGPT parameter and data sizes. We adopt the style from Hoﬀ-\n",
            "mann et al. (2022).\n",
            "3.2 Model Scaling\n",
            "Size. The size of our model is based on Chinchilla scaling laws (Hoﬀmann et al., 2022), in\n",
            "particular their Approach 1 and Approach 2. We start with a total compute budge t of 1.3M\n",
            "GPU hours on 40GB A100 GPUs. Since we adopt activation checkpointing to reduc e our\n",
            "memory footprint, this costs us an additional 0.33x TFLOPs per iteration du e to repeated\n",
            "forward passes. To account for this additional cost, we plug in 0.75 ×1.3M into Chinchilla\n",
            "equations instead of the full amount.\n",
            "From Hoﬀmann et al. (2022), we use the data reported in Table 3 for Approach 1 and\n",
            "Table A3 for Approach 2, and ﬁt regression lines to their log-scaled versions. This gives us:\n",
            "Approach 1 Parameters = exp10(log10(FLOPs )·0.498−1.004) = 52.993B\n",
            "Tokens = exp10(log10(FLOPs )·0.502 + 0.229) = 1111.112B\n",
            "Approach 2 \n",
            "\n",
            "Summary: The size of our model is based on Chinchilla scaling laws (Hoﬀmann et al., 2022), in particular their Approach 1 and Approach 2. We start with a total compute budge t of 1.3M GPUs. We adopt activation checkpointing to reduc e ourmemory footprint. This costs us an additional 0.33x TFLOPs per iteration du e to repeatedforward passes.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.32:\n",
            ".993B\n",
            "Tokens = exp10(log10(FLOPs )·0.502 + 0.229) = 1111.112B\n",
            "Approach 2 Parameters = exp10(log10(FLOPs )·0.490−0.839) = 49.753B\n",
            "Tokens = exp10(log10(FLOPs )·0.510 + 0.062) = 1175.766B\n",
            "These calculations imply that our dataset of /tildelow700B tokens is too small for a “Chinchilla\n",
            "optimal” conﬁguration given our compute budget (assuming just one pass thr ough the\n",
            "data).1While we can increase the amount of general-purpose training data, we are l imited\n",
            "in the amount of domain-speciﬁc training data at our disposal. FinPile is already among\n",
            "the largest domain-speciﬁc training sets, and we do not want it to repr esent less than half\n",
            "of our total training.\n",
            "1. The scaling law derived by Chinchilla is tokenizer-speciﬁc. Our tokenizer can encode the same document\n",
            "more compactly due to the support of multi-word expressions and t he larger vocabulary size. It’s still an\n",
            "open question how well these scaling laws transfer across token izers, and how vocabulary size impacts\n",
            "token and parameter trade-oﬀs assuming ﬁxed compute. We leave this exploration to future work.\n",
            "12Since we are data limited, we choose the largest model that we can, whi le ensuring that\n",
            "we can train \n",
            "\n",
            "Summary: The dataset of /tildelow700B tokens is too small for a “Chinchillaoptimal’ conﬁguration given our compute budget. The scaling law derived by Chinchilla is tokenizer-speci ﬁc. Our tokenizer can encode the same document more compactly due to the support of multi-word expressions and t he larger vocabulary size.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.33:\n",
            ". We leave this exploration to future work.\n",
            "12Since we are data limited, we choose the largest model that we can, whi le ensuring that\n",
            "we can train on all our tokens and still leave /tildelow30% of the total compute budget as a buﬀer\n",
            "for unforeseen failures, retries, and restarts. This leads us to a 50B parameter model, which\n",
            "is also roughly the Chinchilla optimal size for our compute budget. Figu re 1 provides a\n",
            "summary of the scaling laws and how BloombergGPT compares to other models.\n",
            "Shape. To determine how to allocate the 50B parameters to diﬀerent model com ponents\n",
            "(i.e., the “shape” of our model), we follow Levine et al. (2020), who propose that for a total\n",
            "number of self-attention layers L, the optimal hidden dimension Dis obtained by:\n",
            "D= exp(5.039) exp(0.0555·L)\n",
            "We sweep Lover a range of integer values and pick the ( L,D) combination that yields\n",
            "a total of /tildelow50B parameters. This leads to the choice of L= 70 and D= 7510 as our\n",
            "target shape parameters. However, we also want to follow the tradition th at the hidden\n",
            "dimension is evenly divisible by the number of attention heads, wi th the quotient giving\n",
            "the attention head dimension. Furthermore, we want the dimensions t o be multiples of 8\n",
            "to achieve higher performance in Tensor Core operations (NVIDIA, 2023). We set tle on 40\n",
            "heads, each having a dimension of 192, resulting in \n",
            "\n",
            "Summary: We choose the largest model that we can to ensure we can train on all our tokens and still leave /tildelow30% of the total compute budget as a buﬀer for unforeseen failures, retries, and restarts. This leads us to a 50B parameter model, which is roughly the Chinchilla optimal size for our compute budget. We want the dimensions t o be multiples of 8 to achieve higher performance in Tensor Core operations.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.34:\n",
            " 8\n",
            "to achieve higher performance in Tensor Core operations (NVIDIA, 2023). We set tle on 40\n",
            "heads, each having a dimension of 192, resulting in a total hidden dimen sion ofD= 7680\n",
            "and a total of 50.6B parameters. Table 4 provides a summary of the hyper-param eters used\n",
            "inBloombergGPT.\n",
            "3.3 Training Conﬁguration\n",
            "Training. BloombergGPT is a PyTorch model trained with a standard left-to-right\n",
            "causal language modeling objective. Following Brown et al. (2020), we want al l our train-\n",
            "ing sequences to be exactly the same length, in our case 2,048 tokens, to maximize GPU\n",
            "utilization. To achieve this, we concatenate all our tokenized trainin g documents with an\n",
            "<|endoftext|> token as a document separator. We then break this token sequence into\n",
            "chunks of 2,048 tokens. Note that with this approach, each training sequenc e may contain\n",
            "multiple documents from diﬀerent domains. Also note that, because w e’re using ALiBi\n",
            "positional encoding, BloombergGPT can be applied to sequences longer than 2,048 at\n",
            "inference time. For optimization eﬃciency, training sequences ar e grouped together into\n",
            "batches, as described in more detail below.\n",
            "Optimization. We use the AdamW optimizer (Loshchilov and Hutter, 2019). We set\n",
            "β1to 0.9,β2to 0.95, and weight decay to 0.1. Following Brown et al. (2020 \n",
            "\n",
            "Summary: BloombergGPT is a PyTorch model trained with a standard left-to-right language modeling objective. We use the AdamW optimizer (Loshchilov and Hutter, 2019) for optimization. We set tle on 40 heads, each having a dimension of 192, resulting in a total hidden dimen sion ofD= 7680 and a total of 50.6B parameters.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.35:\n",
            ", 2019). We set\n",
            "β1to 0.9,β2to 0.95, and weight decay to 0.1. Following Brown et al. (2020), we set the\n",
            "maximum learning rate to 6e-5 and use the cosine decay learning rate sc heduler with linear\n",
            "warmup. We warm up the learning rate in the ﬁrst 1800 steps. Following Hoﬀ mann et al.\n",
            "(2022), the ﬁnal learning rate is 0.1x the max learning rate, i.e. 6e-6. We also employ batch\n",
            "size warmup (Brown et al., 2020): in the ﬁrst 7,200 steps, we use a batch si ze of 1,024 (2.1M\n",
            "tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remaind er of training.\n",
            "We set dropout to 0.0 in all layers in our initial run, although we add dropou t later as\n",
            "explained in §4. The model parameters are randomly initialized to samples from a normal\n",
            "distribution with zero mean and standard deviation/radicalbig\n",
            "1/(3D) = 0.006588 (Smith et al.,\n",
            "2022). Following Megatron-LM (Shoeybi et al., 2019), we rescale the standard de viation\n",
            "of the second layer in the MLP and the output layer of the attention by 1 /√\n",
            "2L. We use\n",
            "13the technique of querykeylayerscaling (Shoeybi et al., 2019), which was proposed to\n",
            "improve numerical stability \n",
            "\n",
            "Summary: We set dropout to 0.0 in all layers in our initial run, although we add dropou t later. The model parameters are randomly initialized to samples from a normal distribution. We rescale the standard de viationof the second layer in the MLP and the output layer of the attention by 1 /√                2L. We use the technique of querykeylayerscaling (Shoeybi et al., 2019)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.36:\n",
            "\n",
            "2L. We use\n",
            "13the technique of querykeylayerscaling (Shoeybi et al., 2019), which was proposed to\n",
            "improve numerical stability for FP16 mixed-precision training bu t may also help in BF16.\n",
            "Training Instability. LLMs optimization requires running convex optimization algo-\n",
            "rithms over incredibly complex non-convex loss surfaces. Previou s work has reported vari-\n",
            "ous instabilities while training LLMs. For example, Chowdhery et al. (2022) found that the\n",
            "loss spiked roughly 20 times while training PaLM, despite the fact that gradient clipping\n",
            "was enabled. They mitigated these issues by re-starting training f rom a checkpoint roughly\n",
            "100 steps before the spike started, and then skip 200–500 data batches. They hypothesized\n",
            "that spikes occur due to the combination of speciﬁc data batches with a particular model\n",
            "parameter state. Similarly, during OPT training, Zhang et al. (2022a) notic ed spikes in the\n",
            "gradient and activation norms, or divergences in the training perplexi ty. After these behav-\n",
            "iors, they lowered their learning rate, which stabilized these n orms and allowed training to\n",
            "continue. Interestingly, Scao et al. (2022) report only a single loss spik e, from which the\n",
            "model recovered on its own.\n",
            "Hardware Stack. We use the Amazon SageMaker service provided by AWS to train and\n",
            "evaluate BloombergGPT. We use the latest version available at the time of training and\n",
            "train on a total of 64 p4d.24xlarge instances. Each p4d.24 \n",
            "\n",
            "Summary: We use the Amazon SageMaker service provided by AWS to train andevaluate BloombergGPT. We use the latest version available at the time of training andtrain on a total of 64 p4d.24xlarge instances. We train on a single instance of Bloomberg GPT, which we use to evaluate the model. We also train on the same instance of GPT that we evaluate the Bloomberg model.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.37:\n",
            ". We use the latest version available at the time of training and\n",
            "train on a total of 64 p4d.24xlarge instances. Each p4d.24xlarge instance has 8 NVIDIA\n",
            "40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) and NVIDIA\n",
            "GPUDirect using AWS Elastic Fabric Adapter (EFA) inter-node connec tions (400 Gb/s).\n",
            "This yields a total of 512 40GB A100 GPUs. For quick data access, we use Amazon FSX f or\n",
            "Lustre, which supports up to 1000 MB/s read and write throughput per TiB s torage unit.\n",
            "3.4 Large-scale Optimization\n",
            "To train BloombergGPT, which has a larger memory footprint than available GPU mem-\n",
            "ory on cloud instances, we rely on stage 3 of ZeRO optimization (Rajbhandari et al., 2020).\n",
            "We utilize the proprietary SageMaker Model Parallelism (SMP) librar y from AWS, which\n",
            "enables the automatic distribution of large models across multiple GPU devices and in-\n",
            "stances (Karakus et al., 2021). After experimenting with various techni ques, we achieve\n",
            "102 TFLOPs on average and each training step takes 32.5 seconds. We ﬁnd the fol lowing\n",
            "setup to be the best performing in our training.\n",
            "ZeRO Optimization (stage 3). ZeRO shards the training state (model parameters,\n",
            "gradients, and optimizer state) across a group of GPUs. We shard a model acros s 128\n",
            "GPUs, and we have 4 copies of the model during training.\n",
            " \n",
            "\n",
            "Summary: We train BloombergGPT on a total of 64 p4d.24xlarge instances. Each instance has 8 NVIDIA40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) and NVIDIAGPUDirect using AWS Elastic Fabric Adapter (EFA) inter-node connec tions (400 Gb/s). We achieve                102 TFLOPs on average and each training step takes 32.5 seconds.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.38:\n",
            " optimizer state) across a group of GPUs. We shard a model acros s 128\n",
            "GPUs, and we have 4 copies of the model during training.\n",
            "MiCS. Zhang et al. (2022b) decrease training communication overhead and memory re -\n",
            "quirements for cloud training clusters. MiCS includes such feat ures as hierarchical commu-\n",
            "nication, 2-hop gradient update, scale-aware model partitioning.\n",
            "Activation Checkpointing. Chen et al. (2016) minimizes training memory consumption\n",
            "by removing activations at the expense of additional computation during backward passes.\n",
            "When a layer has activation checkpointing enabled, only the layer in put and outputs are\n",
            "kept in memory following a forward pass, while any intermediate tens ors are discarded from\n",
            "memory. During the backward pass, these intermediate tensors may b e recomputed. We\n",
            "apply activation checkpointing to each transformer layer.\n",
            "140 20000 40000 60000 80000 100000 120000 140000\n",
            "Steps2.002.252.502.753.003.253.503.754.00LossLearning curve\n",
            "config\n",
            "lr 6e-5 + bs 1024\n",
            "lr 6e-5\n",
            "lr 4e-5\n",
            "lr 2e-5 + dropout\n",
            "lr 1e-5 + dropout\n",
            "metric\n",
            "smooth train loss\n",
            "val loss\n",
            " 2.102.152.202.252.30 config\n",
            "lr 6e-5 + bs 1024\n",
            "lr 6e-5\n",
            "lr 4e-5\n",
            "lr 2e-5 + dropout\n",
            "lr 1e-5 + dropout\n",
            "metric\n",
            "smooth train loss\n",
            "val \n",
            "\n",
            "Summary: We shard a model acros s 128 GPUs and have 4 copies of the model during training. MiCS includes such feat ures as hierarchical commu-nication, 2-hop gradient update, scale-aware model partitioning. We apply activation checkpointing to each transformer layer to reduce training memory consumption. We have 4 versions of MiCS available for training. We shard the model across a group of GPUs.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.39:\n",
            "5\n",
            "lr 4e-5\n",
            "lr 2e-5 + dropout\n",
            "lr 1e-5 + dropout\n",
            "metric\n",
            "smooth train loss\n",
            "val loss\n",
            "Figure 2: (Smoothed) training and validation losses for BloombergGPT. Inner plot is a\n",
            "zoomed-in version of the area within dashed rectangle in the outer plot (with\n",
            "shared x-axis). Colors denote diﬀerent hyperparameter conﬁgurations. Styles\n",
            "denote training vs validation loss.\n",
            "Mixed Precision Training. To reduce the memory requirements, forward and backward\n",
            "passes are done in BF16, while parameters are stored and updated in full pr ecision (FP32).\n",
            "The ALiBi matrices are computed in full precision and stored in BF16. We al so use FP32\n",
            "to calculate fused softmax in the Attention block and store its resul ts in BF16. Finally, the\n",
            "softmax calculations in the loss function are computed in FP32.\n",
            "Fused Kernels. Another possibility for optimization is combining composition of several\n",
            "operations into a single GPU operation. This can both reduce peak memory u sage by\n",
            "avoiding storage of intermediate results in the computation graph, as wel l as help improve\n",
            "speed. Similar to Megatron-LM (Shoeybi et al., 2019), we use a masked-caus al-softmax\n",
            "fused kernel in SMP in the self-attention module. In practice, w e observe 4-5 TFLOPs\n",
            "improvement for speed, and avoid out-of-memory errors given the rest of the conﬁguration.\n",
            "4 Training Run\n",
            "The \n",
            "\n",
            "Summary: To reduce the memory requirements, forward and backward passes are done in BF16, while parameters are stored and updated in full pr ecision (FP32) We al so use FP32to calculate fused softmax in the Attention block and store its resul ts inBF16. Another possibility for optimization is combining composition of several operations into a single GPU operation. This can both reduce peak memory u sage by avoiding storage of intermediate results in the computation graph, as wel l as help improve speed.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.40:\n",
            "FLOPs\n",
            "improvement for speed, and avoid out-of-memory errors given the rest of the conﬁguration.\n",
            "4 Training Run\n",
            "The process of training BloombergGPT involved decisions along the way based on the\n",
            "progress of model training. We share some highlights of this process. A d etailed presentation\n",
            "appears in the Training Chronicles (Appendix C). Figure 2 shows the learning curves for\n",
            "both training and validation sets. The solid lines show (smoothed) tr aining loss and the\n",
            "dotted lines show loss on the held-out validation set. Changes in the col or of the lines\n",
            "15indicate changes to the optimization hyperparameter conﬁgurations, eit her as scheduled, or\n",
            "in response to increasing or stagnating validation loss. This plot shows the path taken by\n",
            "the successful model training run. To present a clear plot, the F igure does not show other\n",
            "attempts with diﬀerent model conﬁgurations, overwritten partial r uns after a rollback, or\n",
            "other training strategies not utilized in the ﬁnal model.\n",
            "We measured training loss every ﬁve steps on the current batch. The raw values vary\n",
            "wildly, causing large jitter when plotted. The plot smoothes the tr aining loss by showing\n",
            "a running average yt=/summationtextt\n",
            "i=0xi·(1−α)(t−i)\n",
            "/summationtextt\n",
            "i=0(1−α)(t−i)whereα= 0.001. Smoothing is not needed for the\n",
            "validation loss since it is \n",
            "\n",
            "Summary: Figure 2 shows the learning curves for both training and validation sets. The solid lines show (smoothed) tr aining loss and thedotted lines show loss on the held-out validation set. Changes in the col or of the lines15indicate changes to the optimization hyperparameter conﬁgurations, eit her as scheduled, or in response to increasing or stagnating validation loss.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.41:\n",
            "\n",
            "i=0(1−α)(t−i)whereα= 0.001. Smoothing is not needed for the\n",
            "validation loss since it is measured on the entire validation set every 300 steps.\n",
            "We trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after\n",
            "completing /tildelow80% of one epoch through our training data (569B tokens out of the 709B\n",
            "tokens available). We ended training early because the loss on our held -out development\n",
            "set was no longer improving, although it’s possible that substantially l onger training may\n",
            "have yielded further improvements.\n",
            "We began the run with a warm-up batch size of 1,024 for 7,200 steps, after whic h we\n",
            "switched to the regular batch size of 2,048 (color changes from black to blue ). Change in\n",
            "batch size manifests as a visible curvature change in the validation los s at step 7,200. Most\n",
            "of the remainder of the training performed stably with decreasing tr aining and validation\n",
            "losses. Intervention was required at later stages, after step 115,500, when we observed\n",
            "ﬂat or increasing validation loss. We then applied the following correc tive modiﬁcations in\n",
            "sequence:\n",
            "•Step 115,500 (blue to orange): Shrink learning rate to two-thirds\n",
            "•Step 129,900 (orange to green): Halve learning rate, and add dropout (with 0.1 prob -\n",
            "ability)\n",
            "•Step 137,100 (green to red): Halve learning rate again\n",
            "We ended \n",
            "\n",
            "Summary: We trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after /tILDelow80% of one epoch. We ended training early because the loss on our held -out development set was no longer improving. It is possible that substantially l onger training may have yielded further improvements. We then applied the following correc tive modiﬁcations in the training sequence.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.42:\n",
            " rate, and add dropout (with 0.1 prob -\n",
            "ability)\n",
            "•Step 137,100 (green to red): Halve learning rate again\n",
            "We ended the run at step 146,000 based on the lack of observable progress on the v alidation\n",
            "loss. We selected the checkpoint at step 139,200 as the ﬁnal model based on v alidation loss\n",
            "and downstream evaluations.\n",
            "5 Evaluation\n",
            "We evaluated the performance of BloombergGPT on two broad categories of tasks:\n",
            "ﬁnance-speciﬁc and general purpose. The ﬁnance-speciﬁc tasks help us test our hypoth-\n",
            "esis that training on high-quality ﬁnance-speciﬁc data will yield b etter results on ﬁnancial\n",
            "tasks. The general purpose tasks investigate whether the performance of our model is\n",
            "directly comparable to previously published results. For ﬁnancial tasks, we assembled pub-\n",
            "licly available ﬁnancial datasets that include a range of NLP tasks. Then, t o directly test\n",
            "BloombergGPT ’s ability on Bloomberg tasks of interest, we also included tasks dra wn\n",
            "from Bloomberg-internal high-quality evaluation sets for sentiment an alysis and named en-\n",
            "tity recognition. For general-purpose tasks, we draw from multiple ex isting benchmarks\n",
            "and group results into the following categories: BIG-bench Hard, Know ledge Assessments,\n",
            "Reading Comprehension, and Linguistic Tasks. The number of tasks per t ype and the\n",
            "deﬁnitions of the groups are presented in \n",
            "\n",
            "Summary: We evaluated the performance of BloombergGPT on two broad categories of tasks: ﬁnance-speciﬁc and general purpose. The general purpose tasks investigate whether the. performance of our model is directly comparable to previously published results. For ﬃnancial tasks, we assembled pub-licly available NLP datasets that include a range of NLP tasks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.43:\n",
            "Reading Comprehension, and Linguistic Tasks. The number of tasks per t ype and the\n",
            "deﬁnitions of the groups are presented in Table 5.\n",
            "16Suite Tasks What does it measure?\n",
            "Public Financial Tasks 5 Public datasets in the ﬁnancial domain\n",
            "Bloomberg Financial Tasks 12 NER and sentiment analysis tasks\n",
            "Big-bench Hard (Suzgun et al., 2022) 23 Reasoning and general NLP tasks\n",
            "Knowledge Assessments 5 Testing closed-book information recall\n",
            "Reading Comprehension 5 Testing open-book tasks\n",
            "Linguistic Tasks 9 Not directly user-facing NLP tasks\n",
            "Table 5: Evaluation Benchmarks. We evaluate BloombergGPT on a high-coverage set\n",
            "of standard benchmarks that assess downstream performance, taken from HE LM,\n",
            "SuperGLUE, MMLU, and the GPT-3 suite. Since these have signiﬁcant ov erlap\n",
            "and/or include each other, we restructure them into the categories pr esented here.\n",
            "We only evaluate on one setup per dataset. We further assess BloombergGPT\n",
            "on a suite of internal and public ﬁnancial tasks.\n",
            "Name # Tokens (B) # Params. (B) Compute\n",
            "BloombergGPT 569 50.6 1.00 ×\n",
            "GPT-NeoX 472 20 0.33 ×\n",
            "OPT 300 66 0.69 ×\n",
            "BLOOM 366 176 2.24 ×\n",
            "GPT-3 300 175 1.82 ×\n",
            "Table 6: Evaluation model cohort. OPT and BLOOM each have multiple sizes available and\n",
            "we report those we \n",
            "\n",
            "Summary: summarize: Reading Comprehension, and Linguistic Tasks. The number of tasks per t ype and the                deﬁnitions of the groups are presented in Table 5. We evaluate BloombergGPT on a high-coverage set of standard benchmarks that assess downstream performance, taken from HE LM, SuperGLUE, MMLU, and GPT-3.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.44:\n",
            "\n",
            "GPT-3 300 175 1.82 ×\n",
            "Table 6: Evaluation model cohort. OPT and BLOOM each have multiple sizes available and\n",
            "we report those we evaluated. We note that compute numbers are only parti ally\n",
            "comparable between models: For example, BLOOMs training data is only 1/3\n",
            "English, and OPT repeated some of its training data. We report GPT-3 res ults\n",
            "whenever available but did not run it ourselves due to lack of availabi lity.\n",
            "We compare BloombergGPT to the three closest models described in §7 based on\n",
            "model size, type of training data, overall performance, and most import antly, access. An\n",
            "overview of the model sizes and compute is provided in Table 6.\n",
            "1. GPT-NeoX (Black et al., 2022): According to Liang et al. (2022), this model is th e\n",
            "best performing available model under 50B parameters.\n",
            "2. OPT 66B(Zhang et al., 2022a): We chose to compare to OPT 66Bsince our model size\n",
            "and structure roughly match, though our model is smaller.\n",
            "3. BLOOM 176B(Scao et al., 2022): While this model is substantially larger than BloombergGPT,\n",
            "we use the same model architecture and software stack. We note that BLOO M176Bis\n",
            "multilingual, so while it is much larger, it also is trained on data from more languages.\n",
            "17All three models use some of the same general-purpose datasets we use in our training cor-\n",
            "pus. We additionally report results from the original GPT-3 (Brown et al., 2020 \n",
            "\n",
            "Summary: We compare BloombergGPT to the three closest models described in §7 based on model size, type of training data, overall performance, and most import antly, access. An overview of the model sizes and compute is provided in Table 6. We report GPT-3 res ultswhenever available but did not run it ourselves due to lack of availabi lity. We additionally report results from the original GPT.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.45:\n",
            " of the same general-purpose datasets we use in our training cor-\n",
            "pus. We additionally report results from the original GPT-3 (Brown et al., 2020) whenever\n",
            "externally available.2\n",
            "We prefer running models ourselves to ensure identical evaluati on setups, and we place\n",
            "any results that have been reported elsewhere and were not run by us into a separated\n",
            "group. To fairly compare the models, we avoid any tuning of prompts and oth er techniques\n",
            "that could lead to improved results for some, but not all, models. For t hat reason, every\n",
            "task is tested via “standard” prompting (shown in Table 7), i.e., wit hout any parameter\n",
            "changes to the underlying model, without task descriptions, and wi thout Chain-of-Thought\n",
            "prompting (Wei et al., 2022b). The number of few-shot examples present ed to the model\n",
            "depends on the task, and we include these details in the respectiv e sections. For each group\n",
            "of results, we further present a win rate similar to Liang et al. (2022) th at represents the\n",
            "fraction of “wins” in side-by-side comparisons over individual tasks between all model pairs\n",
            "for which we have run the evaluation ourselves.\n",
            "5.1 Few-shot Methodology\n",
            "For tasks where a set of candidates are given, we perform likelihood-b ased classiﬁcation,\n",
            "following Brown et al. (2020). We consider three methods for classiﬁcat ion: regular, cali-\n",
            "bration, and normalization. Formally,\n",
            "•Regular: arg \n",
            "\n",
            "Summary: summarize:  of the same general-purpose datasets we use in our training cor-                pus. We additionally report results from the original GPT-3 (Brown et al., 2020) whenever they are externally available. For each group of results, we further present a win rate similar to Liang et al. (2022) th at represents the percentage of “wins” in side-by-side comparisons over individual tasks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.46:\n",
            "). We consider three methods for classiﬁcat ion: regular, cali-\n",
            "bration, and normalization. Formally,\n",
            "•Regular: arg max αp(α|s)\n",
            "•Calibration: arg max αp(α|s)/p(α|“Answer:”)\n",
            "•Normalization: arg max αp(α|s)/len(α)\n",
            "whereαis a candidate, sis the context, and len measures the number of sub-word tokens.\n",
            "We report the performance of the best method for each model and task. For other tasks,\n",
            "we perform generation via greedy decoding.\n",
            "We use the oﬃcial split and report performance on the test set wheneve r possible. If\n",
            "the test labels are not publicly available, we report performance on th e dev set instead. If\n",
            "an oﬃcial split for a dataset does not exist, we create train and test spli ts by selecting 20%\n",
            "of examples to be the test and the rest as train. All few-shot context ex amples are sampled\n",
            "from the training set. To reduce the variance of few-shot evaluation, w e sample diﬀerent\n",
            "shots for each test example, unless otherwise speciﬁed. For the sake of consistency, for each\n",
            "test example, all models have identical surface form as input in our e valuation.\n",
            "5.2 Heldout Loss\n",
            "We begin by testing how well BloombergGPT models the language distribution of the in-\n",
            "distribution ﬁnance data. We evaluate the bits per byte of the diﬀere nt models on a heldout\n",
            " \n",
            "\n",
            "Summary: We consider three methods for classiﬁcat ion: regular, cali-bration, and normalization. We report the performance of the best method for each model and task. For other tasks, we perform generation via greedy decoding. We evaluate the bits per byte of the diﬀere nt data. We use the oﬃcial split and report performance on the test set wheneve r possible.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.47:\n",
            " distribution of the in-\n",
            "distribution ﬁnance data. We evaluate the bits per byte of the diﬀere nt models on a heldout\n",
            "dataset that contains examples from all sections of FinPile (described in §2). To limit\n",
            "data leakage and better simulate real-world usage of LLMs, we select a tempor ally heldout\n",
            "2. Anotherrelatedgeneral-purposemodelatacomparablesize (L LaMA,Touvronetal.,2023), wasreleased\n",
            "during the preparation of this manuscript, but third-party evalua tion results were not available and we\n",
            "haven’t received access to the model weights.\n",
            "18Overall Bloomberg Filings Newswires Press Web0.00.20.40.60.8bits per byteBloombergGPT\n",
            "GPT-Neo-X\n",
            "OPT 66B\n",
            "BLOOM 176B\n",
            "Figure 3: Bits per byte on a heldout test set of each data type in our FinPile (lower\n",
            "is better). The set of documents is held out in time and deduplicat ed with\n",
            "the training set, such that all of it is completely unseen by BloombergGPT.\n",
            "Regardless, we observe a large gap between the models. The improveme nt is\n",
            "largest for specialized in-domain documents like Filings.\n",
            "dataset that is strictly further in the future than the training se t, and perform deduplication\n",
            "between the training and heldout set. During evaluation, for docume nts that are longer than\n",
            "2,048 tokens, we use a sliding window approach with half window size as c ontext. That\n",
            "me \n",
            "\n",
            "Summary: We evaluate the bits per byte of the diﬀere nt models on a heldout.dataset that contains examples from all sections of FinPile. The set of documents is held out in time and deduplicat ed with the training set. During evaluation, for docume nts that are longer than 2,048 tokens, we use a sliding window approach.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.48:\n",
            " evaluation, for docume nts that are longer than\n",
            "2,048 tokens, we use a sliding window approach with half window size as c ontext. That\n",
            "means that any token beyond the ﬁrst 2,048 has at least 1,024 tokens as context dur ing\n",
            "prediction. We report the loss breakdown by the type of document in FinPile.\n",
            "Figure 3 shows that BloombergGPT consistently outperforms other models. While\n",
            "this is expected and mainly serves as a sanity check, it also provid es valuable insight into the\n",
            "generalization capabilities of the other models. For example, the gap to BloombergGPT\n",
            "is most signiﬁcant in the Filings category, likely because these docu ments, while public, are\n",
            "typically in PDF format and thus not included in any existing datasets.\n",
            "5.3 Financial Tasks\n",
            "The NLP tasks most often considered in ﬁnance are also common in the broader NLP liter-\n",
            "ature; but, these tasks take on diﬀerent characteristics and challen ges when performed on\n",
            "ﬁnancial data. Take the example of sentiment analysis, where a headline such as “COM-\n",
            "PANY to cut 10,000 jobs” portrays negative sentiment in the general sense b ut can at times\n",
            "be considered positive for ﬁnancial sentiment towards COMPANY, as it mi ght result in the\n",
            "stock price or investor conﬁdence increasing. We use a combination of p ublic and internal\n",
            "benchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, and\n",
            "OPT \n",
            "\n",
            "Summary: BloombergGPT consistently outperforms other models. The gap to BloombergGPT is most signiﬁcant in the Filings category. We report the loss breakdown by the type of document in FinPile. We use a combination of p ublic and internal benchmarks to assess the performance of BloombergG PT, BLOOM 176B, GPT-Ne and BLOom 176B.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.49:\n",
            " of p ublic and internal\n",
            "benchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, and\n",
            "OPT66B. All task types considered and their corresponding prompt templates are shown\n",
            "in Table 7.\n",
            "19Task Template/Example\n",
            "Discriminative\n",
            "Sentiment Analysis {sentence }\n",
            "Question: what is the sentiment?\n",
            "Answer: {negative/neutral/positive }\n",
            "Aspect Sentiment Analysis {sentence }\n",
            "Question: what is the sentiment on {target}?\n",
            "Answer: {negative/neutral/positive }\n",
            "Binary Classiﬁcation {sentence }\n",
            "Question: {question }?\n",
            "Answer: {Yes/No}\n",
            "Generative\n",
            "NER Steve Jobs is the CEO of Apple\n",
            "Extract named entity: Steve Jobs (person), Apple (organization)\n",
            "NER+NED AAPL stopped using Intel Chips\n",
            "Extract ticker: AAPL, INTC\n",
            "QA {context}\n",
            "Question: {question }?\n",
            "Answer: {answer}\n",
            "Table 7: Template for the diﬀerent tasks we evaluate in the ﬁnancial domai n.\n",
            "5.3.1 External Financial Tasks\n",
            "Our public ﬁnancial benchmarks include four tasks from the FLUE bench mark (Shah et al.,\n",
            "2022) and the ConvFinQA dataset (Chen et al., 2022). As LLM performance on most of\n",
            "these ﬁnancial tasks have not been broadly reported, there is no stand ard testing frame-\n",
            "work. Thus, we adapt them to a few-shot setting (see Section §5.1). \n",
            "\n",
            "Summary: Summarize:  of p ublic and internalbenchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, andOPT66B. All task types considered and their corresponding prompt templates are shown. Table 7: Template for the diﬀerent tasks we evaluate in the ﬁnancial domai n.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.50:\n",
            " not been broadly reported, there is no stand ard testing frame-\n",
            "work. Thus, we adapt them to a few-shot setting (see Section §5.1). Our guiding principle\n",
            "in designing the experiments was to select the number of shots suc h that the average per-\n",
            "formance across all the models was best. While non-LLM numbers of custom m odels for\n",
            "these tasks are available, we omit reporting them here due to diﬀere nces in the evaluation\n",
            "setup. As a result, our claims are restricted to comparisons of LLMs. We e valuate on the\n",
            "following tasks (more details provided in Appendix B):\n",
            "•FPB (Malo et al., 2014): The Financial Phrasebank Dataset includes a sentimen t\n",
            "classiﬁcation task on sentences from ﬁnancial news. Any news that could b eneﬁt/hurt\n",
            "an investor is considered positive/negative and neutral otherwise. We create our own\n",
            "splits and report F1 score weighted by support in a 5-shot setup.\n",
            "•FiQA SA (Maia et al., 2018): The second sentiment analysis task is to predict the\n",
            "aspect-speciﬁc sentiment in English ﬁnancial news and microblog he adlines, which\n",
            "were published as a part of the 2018 challenge on ﬁnancial question answerin g and\n",
            "opinion mining. While the original dataset is annotated on a continuous scale, we\n",
            "discretize the data into a classiﬁcation setup with negative, neut ral, and positive\n",
            "classes. Like with FPB, we create \n",
            "\n",
            "Summary: The Financial Phrasebank Dataset includes a sentimen t                classiﬁcation task on sentences from ﬁnancial news. We create our own own ownsplits and report F1 score weighted by support in a 5-shot setup. While non-LLM numbers of custom m odels are available, we omit reporting them here due to diﬀere nces in the evaluation setup.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.51:\n",
            " we\n",
            "discretize the data into a classiﬁcation setup with negative, neut ral, and positive\n",
            "classes. Like with FPB, we create our own splits including microbl ogs and news, and\n",
            "use a 5-shot setup, reporting weighted F1.\n",
            "20BloombergGPT GPT-NeoX OPT 66B BLOOM 176B\n",
            "ConvFinQA 43.41 30.06 27.88 36.31\n",
            "FiQA SA 75.07 50.59 51.60 53.12\n",
            "FPB 51.07 44.64 48.67 50.25\n",
            "Headline 82.20 73.22 79.41 76.51\n",
            "NER 60.82 60.98 57.49 55.56\n",
            "All Tasks (avg) 62.51 51.90 53.01 54.35\n",
            "All Tasks (WR) 0.93 0.27 0.33 0.47\n",
            "Table 8: Results on ﬁnancial domain tasks.\n",
            "•Headline (Sinha and Khandait, 2020): This is a binary classiﬁcation task of whether\n",
            "a news headline in the gold commodity domain includes certain informat ion. This\n",
            "human-annotated dataset consists of English news headlines about “gold”. Each news\n",
            "article carries a subset of the following tags: “price or not”, “price up”, “price down”,\n",
            "“price stable”, “past price”, “future price”, “past general”, “future gener al”, “asset\n",
            "compar \n",
            "\n",
            "Summary: Summarize:  we systematicallydiscretize the data into a classiﬁcation setup with negative, neut ral, and positive Classes. Like with FPB, we create our own splits including microbl ogs and news, and use a 5-shot setup, reporting weighted F1. Table 8: Results on ﬁnancial domain tasks. The results are based on a dataset of English news headlines about “gold”\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.52:\n",
            "�past price”, “future price”, “past general”, “future gener al”, “asset\n",
            "comparison”. We verbalize each tag into a question using the oﬃcial docu mentation,\n",
            "use 5 shots, and report the average weighted F1 score across all categories.\n",
            "•NER (Salinas Alvarado et al., 2015): This is a named entity recognition task on ﬁnan-\n",
            "cial data gathered for credit risk assessment from ﬁnancial agreements ﬁ led with the\n",
            "SEC. The annotated entity types follow the standard CoNLL format (Tjong Ki m Sang\n",
            "and De Meulder, 2003) and are annotated with PER, LOC, ORG, and MISC. As it\n",
            "is nontrivial to learn to predict empty outputs in few-shot setups, we drop sentences\n",
            "that do not contain any entity. We further drop MISC tags due to their amb iguous\n",
            "deﬁnition. All the models required more shots to perform well and we thus selected\n",
            "20 shots and report the entity-level F1 score.\n",
            "•ConvFinQA (Chen et al., 2022): Given input from S&P 500 earnings reports that\n",
            "includes text and at least one table with ﬁnancial data, the task is to answ er conver-\n",
            "sational questions that require numerical reasoning over the input. This task requires\n",
            "numerical reasoning, an understanding of structured data and ﬁnancial c oncepts, and\n",
            "a model needs to relate follow-up questions to the dialog turns.\n",
            " \n",
            "\n",
            "Summary: Summarize each tag into a question using the oﬃcial docu mentation and report the average weighted F1 score across all categories. All the models required more shots to perform well and we thus selected 20 shots and reported the entity-level F1 scores. The annotated entity types follow the standard CoNLL format and are annotated with PER, LOC, ORG, and MISC.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.53:\n",
            " reasoning, an understanding of structured data and ﬁnancial c oncepts, and\n",
            "a model needs to relate follow-up questions to the dialog turns.\n",
            "For ConvFinQA, we use an entire gold conversation and its context is used as input\n",
            "to the models. As each “turn” of the conversation concludes, the “turn” along with\n",
            "the answer for that turn is appended as context for future turns. We re port the exact\n",
            "match accuracy on the public development set.\n",
            "BloombergGPT performs best of all models for four of the ﬁve tasks (ConvFinQA,\n",
            "FiQA SA, FPB, and Headline) and comes in second in NER (Table 8). Consequen tly,\n",
            "BloombergGPT also has the highest win rate among all the models that we tested. The\n",
            "gap to equally-sized models is especially pronounced for ConvFinQA w hich is challenging\n",
            "due to the requirement to use conversational input to reason over tab les and generate an\n",
            "answer.\n",
            "21Name Time Tokens Test Size % Pos % Neu % Neg\n",
            "Equity News 2018–2019 150-200 1,000 7 87 6\n",
            "Equity Social Media 2015–2020 15-20 1,000 10 83 7\n",
            "Equity Transcript 2008–2020 70-80 800 19 75 6\n",
            "ES News 2016–2019 100-120 1,000 32 53 15\n",
            "Country News 2009–2021 50-1,000 1,000 18 60 22\n",
            "Table 9: An overview of the Bloomberg-internal sentiment analysis task s. Input token and\n",
            "label distribution numbers are computed on the test set.\n",
            "5. \n",
            "\n",
            "Summary: BloombergGPT performs best of all models for four of the ﬁve tasks (ConvFinQA, FiQA SA, FPB, and Headline) and comes in second in NER (Table 8). Consequen tly, BloombergGPT also has the highest win rate among all the models that we tested. The gap to equally-sized models is especially pronounced for Conv FinQA w hich is challenging due to the requirement to use conversational input to reason over tab les and generate ananswer.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.54:\n",
            " 60 22\n",
            "Table 9: An overview of the Bloomberg-internal sentiment analysis task s. Input token and\n",
            "label distribution numbers are computed on the test set.\n",
            "5.3.2 Internal Task: Sentiment Analysis\n",
            "For the Bloomberg-internal tasks, we consider aspect-speciﬁc sent iment analysis, which is\n",
            "prevalent in ﬁnancial literature. All of the datasets we use are in Engli sh.\n",
            "Our annotation process consists of a discovery phase during which we establish the an-\n",
            "notation and sampling procedures, understand how many annotators are typ ically required\n",
            "per example, and determine the level of training that is needed for t he annotators (Tseng\n",
            "et al., 2020). Depending on the complexity of the task, our annotators are a ded icated team\n",
            "of ﬁnancial experts at Bloomberg, consultant workers, or a combination of bot h. In each\n",
            "case, ties are resolved by adjudication from additional annotators and ambiguou s examples\n",
            "are excluded. All the datasets in this section were annotated by 2 annotat ors with a third\n",
            "annotator breaking any ties.\n",
            "We measure the performance of LLMs for the internal datasets using a ﬁve- shot evalu-\n",
            "ation, similar to the external datasets. As the datasets are large, we random ly sample at\n",
            "most 1k test examples. We report F1 weighted by the support of each label. Note that,\n",
            "similar to the external datasets, it is likely that the unlabeled versions of the data used in\n",
            "our internal datasets occur in FinPile and are therefore seen by BloombergGPT during\n",
            "training. However, since \n",
            "\n",
            "Summary: For the Bloomberg-internal tasks, we consider aspect-speciﬁc sent iment analysis. All of the datasets we use are in Engli sh. The datasets in this section were annotated by 2 annotat ors with a third annotator breaking any ties. We measure the performance of LLMs for the internal datasets using a ﬁve- shot evaluation, similar to the external datasets.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.55:\n",
            " that the unlabeled versions of the data used in\n",
            "our internal datasets occur in FinPile and are therefore seen by BloombergGPT during\n",
            "training. However, since some of FinPile is also available on the web, other LLMs we\n",
            "compare against may have also been trained on unlabeled versions of this d ata. Dataset\n",
            "statistics are provided in Table 9.\n",
            "•Equity News Sentiment : This task is to predict the aspect-speciﬁc sentiment ex-\n",
            "pressed in the news story toward a company. The dataset consists of Engl ish news\n",
            "stories from Bloomberg, premium, and web content. Annotations of “positiv e”, “neg-\n",
            "ative”, or “neutral” indicate that the news story is likely to increase, decrease, or not\n",
            "change the long-term investor conﬁdence in the company.\n",
            "•Equity Social Media Sentiment : The task is similar to “Equity News Sentiment”\n",
            "but instead of news, we use ﬁnancially-relevant English social medi a content.\n",
            "•Equity Transcript Sentiment : This task is also similar to “Equity News Senti-\n",
            "ment” but instead of news, we use transcripts from company press conf erences. The\n",
            "transcripts are made available through the use of speech recognition and at times,\n",
            "human edits. Long transcripts are processed in chunks, and each chun k in our dataset\n",
            "typically contains between 70 and 80 tokens.\n",
            "22BloombergGPT GPT-NeoX OPT 66B BLOOM 176B\n",
            "Equity News 79 \n",
            "\n",
            "Summary: The dataset consists of Engl ish news stories from Bloomberg, premium, and web content. The task is to predict the aspect-speciﬁc sentiment ex-pressed in the news story toward a company. Annotations of “positiv e”, “neg-reprehensible” or “neutral” indicate that the story is likely to increase or decrease the long-term investor con-dence in the company.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.56:\n",
            " our dataset\n",
            "typically contains between 70 and 80 tokens.\n",
            "22BloombergGPT GPT-NeoX OPT 66B BLOOM 176B\n",
            "Equity News 79.63 14.17 20.98 19.96\n",
            "Equity Social Media 72.40 66.48 71.36 68.04\n",
            "Equity Transcript 65.06 25.08 37.58 34.82\n",
            "ES News 46.12 26.99 31.44 28.07\n",
            "Country News 49.14 13.45 17.41 16.06\n",
            "All Tasks (avg) 62.47 29.23 35.76 33.39\n",
            "All Tasks (WR) 1.00 0.00 0.67 0.33\n",
            "Table 10: Results on internal aspect-speciﬁc sentiment analysis datas ets.\n",
            "BloombergGPT far outperforms all other models on sentiment analysis\n",
            "tasks.\n",
            "Name Tokens Test Size LOC ORG PER\n",
            "BFW /tildelow21 500 0.2 1.6 0.0\n",
            "BN /tildelow30 500 0.7 1.0 0.6\n",
            "Filings /tildelow32 500 0.1 1.3 0.4\n",
            "Headlines /tildelow50 500 0.7 2.7 1.0\n",
            "Premium /tildelow29 500 0.6 1.4 0.3\n",
            "Transcripts /tildelow23 500 0.6 0.6 0.3\n",
            "Social Media /tildelow12 500 0.4 1.4 0.2\n",
            "Table 11: An overview of statistics of our internal NER test set. We report average number\n",
            "of LOC \n",
            "\n",
            "Summary: BloombergGPT GPT-NeoX OPT 66B BLOOM 176BEquity News 79.63 14.17 20.98 19.96Equity Social Media 72.40 66.48 71.36 68.04Equity Transcript 65.06 25.08 37.58 34.82ES News 46.12 26.99 31.44 28.07Country News 49.14 13.45 17.41 16.06\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.57:\n",
            "ow12 500 0.4 1.4 0.2\n",
            "Table 11: An overview of statistics of our internal NER test set. We report average number\n",
            "of LOCation, ORGanization, PERson per example.\n",
            "•ES News Sentiment : While this task is to predict the aspect-speciﬁc sentiment\n",
            "expressed in the news story towards a company (aspect), the goal is not t o indicate\n",
            "eﬀect on investor conﬁdence. The stories are annotated “positive”, “negati ve”, or\n",
            "“neutral” if the news story contains content that reﬂects good, bad, or n eutral news\n",
            "about the company’s environmental and social policies.\n",
            "•Country News Sentiment : This task is diﬀerent from the other sentiment tasks\n",
            "in that the goal is to predict the sentiment expressed in the news s tory towards a\n",
            "country. The dataset consists of English news stories from Bloomberg, p remium, and\n",
            "web content. The stories are annotated “positive”, “negative”, or “neutral” i f the\n",
            "news story alludes to the growth, shrinkage, or status quo of that countr y’s economy.\n",
            "Table 10 shows that across the four internal aspect-speciﬁc sentimen t tasksBloombergGPT\n",
            "performs better than all the other tested models, by a wide margin. T he only task in which\n",
            "the models perform similarly is the social media sentiment task, whileBloombergGPT\n",
            "outperforms the other models by at least \n",
            "\n",
            "Summary: BloombergGPT performs better than all the other tested models, by a wide margin. The only task in which the models perform similarly is the social media sentiment task. We report average number of LOCation, ORGanization, PERson, and PERson per example.summarize: ow12 500 0.4 1.4 0.2 0.1 0.0 0.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.58:\n",
            " wide margin. T he only task in which\n",
            "the models perform similarly is the social media sentiment task, whileBloombergGPT\n",
            "outperforms the other models by at least 25 and up to over 60 points in the other three.\n",
            "5.3.3 Exploratory Task: NER\n",
            "Even though NER is a well-established NLP task with state-of-the-art re sults using BERT\n",
            "(Wu and Dredze, 2019; Luoma and Pyysalo, 2020) and T5 (Liu et al., 2022) style models,\n",
            "23NER is largely an unexplored task for generative LLMs. NER is not in HELM (Liang et al.,\n",
            "2022), there is a single (Polish) task in BIG-bench (Srivastava et al., 2022), and none of the\n",
            "LLM papers we study report NER performance. Hence, we consider NER as an exp loratory\n",
            "task and report preliminary NER results given its importance in the Fi nancial sector.\n",
            "There are a few reasons for why NER may be a diﬃcult task for generative LLMs.\n",
            "NER is an information extraction task, and a better ﬁt for encoder-decod er or encoder-only\n",
            "architectures. The generative nature of LLMs does not confer an advantage for NER. We\n",
            "ﬁnd that extensive prompt engineering and a greater number of shots are required to obtain\n",
            "reasonable results for NER than for other tasks. Finance-speciﬁc NER has s ubtleties that\n",
            "make it especially diﬃ \n",
            "\n",
            "Summary: BloombergGPT outperforms the other models by at least 25 and up to over 60 points in the other three. T he only task in which the models perform similarly is the social media sentiment task. NER is an information extraction task, and a better for encoder-decod er or encoder onlyarchitectures. The generative nature of LLMs does not confer an advantage for NER.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.59:\n",
            "reasonable results for NER than for other tasks. Finance-speciﬁc NER has s ubtleties that\n",
            "make it especially diﬃcult for zero or few-shot learning.\n",
            "For example, consider the (fabricated) headline “Bloomberg: Mr. Musk adds new fea-\n",
            "tures to Twitter and comments on China”. Depending on our annotation guide lines and\n",
            "downstream task needs: (a) the reporting news organization “Bloomberg” c an be tagged or\n",
            "not, depending on whether we want only salient entities, (b) “Mr. Mu sk” or just “Musk”\n",
            "is the PER to be tagged, (c) “Twitter” can be tagged as an ORG or a PRD (produc t)\n",
            "as features are added to the Twitter product and not the organization, and ( d) “China”\n",
            "can be tagged ORG or LOC, though the right tag is likely ORG. Without adding e xtensive\n",
            "annotation guidelines in the prompt, the LLM does not know the intended tagging behavior.\n",
            "Based on preliminary testing, we determined the following settin g to obtain the best\n",
            "performance on the internal NER tasks from all models. First, we restr ict the entity types\n",
            "to be predicted to be ORG, PER, and LOC. In all, we ﬁltered out less t han 1% of entities.\n",
            "We also remove all documents that contain no entities (i.e., all “O”’s ). Both of these modi-\n",
            "ﬁcations are intended \n",
            "\n",
            "Summary: For example, consider the (fabricated) headline “Bloomberg: Mr. Musk adds new fea-                tures to Twitter and comments on China”. Depending on our annotation guide lines and downstream task needs, “China’s” can be tagged ORG or LOC, though the right tag is likely ORG. Without adding e xtensive Guidelines in the prompt, the LLM does not know the intended tagging behavior.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.60:\n",
            " all documents that contain no entities (i.e., all “O”’s ). Both of these modi-\n",
            "ﬁcations are intended to increase the usefulness of the examples se en in few-shot prompting.\n",
            "We expect that further work on prompt engineering for NER could produc e better results.\n",
            "We consider seven Bloomberg internal NER datasets from diﬀerent domai ns.\n",
            "•BN NER : This is a named entity recognition task on entities occurring in En glish\n",
            "long-form Bloomberg news content (the “BN wire”) between 2017 to 2020.\n",
            "•BFW NER : Similar to “BN NER” but instead of using the long-form BN wire, we\n",
            "use short-form stories from the “Bloomberg First Word” wire between 2018 to 2020.\n",
            "•Filings NER : The goal of this task is to identify entities that occur in mandatory\n",
            "ﬁnancial disclosures ﬁled by companies. The dataset contains ﬁlings sam pled between\n",
            "2016 and 2019.\n",
            "•Headlines NER : The goal of this task is to identify entities that occur in headlines\n",
            "of English Bloomberg news content. The dataset contains headlines samp led between\n",
            "2016 and 2020.\n",
            "•Premium NER : The goal of this task is to identify entities that occur in a subset\n",
            "of the third-party English news content ingested by Bloomberg. The d ataset contains\n",
            "stories sampled between 2019 and 2021.\n",
            "•Transcripts NER : The goal of this task is to identify entities that occur in transcrip ts\n",
            "of company press conferences. The dataset contains transcripts from 2019 \n",
            "\n",
            "Summary: Summarize:  all documents that contain no entities (i.e., all “O”’s ). Both of these modi-ﬁcations are intended to increase the usefulness of the examples se en in few-shot prompting. We expect that further work on prompt engineering for NER could produc e better results. We consider seven Bloomberg internal NER datasets from diﬀerent domai ns.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.61:\n",
            "\n",
            "•Transcripts NER : The goal of this task is to identify entities that occur in transcrip ts\n",
            "of company press conferences. The dataset contains transcripts from 2019.\n",
            "24BloombergGPT GPT-NeoX OPT 66B BLOOM 176B\n",
            "NER\n",
            "BFW 72.04 71.66 72.53 76.87\n",
            "BN 57.31 52.83 46.87 59.61\n",
            "Filings 58.84 59.26 59.01 64.88\n",
            "Headlines 53.61 47.70 46.21 52.17\n",
            "Premium 60.49 59.39 57.56 61.61\n",
            "Transcripts 75.50 70.62 72.53 77.80\n",
            "Social Media 60.60 56.80 51.93 60.88\n",
            "All Tasks (avg) 62.63 59.75 58.09 64.83\n",
            "All Tasks (WR) 0.57 0.29 0.19 0.95\n",
            "NER+NED\n",
            "BFW 55.29 34.92 36.73 39.36\n",
            "BN 60.09 44.71 54.60 49.85\n",
            "Filings 66.67 31.70 65.63 42.93\n",
            "Headlines 67.17 36.46 56.46 42.93\n",
            "Premium 64.11 40.84 57.06 42.11\n",
            "Transcripts 73.15 23.65 70.44 34.87\n",
            "Social Media 67.34 62.57 70.57 65.94\n",
            "All Tasks (avg) 64.83 39.26 58.79 45.43\n",
            "All Tasks (WR) 0.95 0.00 0.67 0.38\n",
            "Table \n",
            "\n",
            "Summary: The dataset contains transcripts from 2019. The goal of this task is to identify entities that occur in transcrip tsof company press conferences.summarize:                  Transcripts NER : The goal to identify. entities that occurred in transcrips of company press. conferences. The dataset contains. transcripts from2019.BloombergGPT GPT-NeoX OPT 66B BLOOM 176BNER.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.62:\n",
            "g) 64.83 39.26 58.79 45.43\n",
            "All Tasks (WR) 0.95 0.00 0.67 0.38\n",
            "Table 12: Results on internal NER and NED datasets. On NER, while the much l arger\n",
            "BLOOM 176B model outperforms all other models, results from all models are\n",
            "relatively close, with BloombergGPT outperforming the other two models. On\n",
            "NER+NED, BloombergGPT outperforms all other models by a large margin.\n",
            "•Social Media NER : The goal of this task is to identify entities that occur in English\n",
            "ﬁnancially-relevant social media content. The dataset contains soci al media content\n",
            "sampled between 2009 and 2020.\n",
            "As our datasets are substantive, we randomly sample 4,000 training and 500 test ing ex-\n",
            "amples from each ﬁltered internal dataset. We utilize 20-shot prompts an d evaluate using\n",
            "F1. The results from the internal NER tasks are mixed (Table 12). The muc h larger\n",
            "BLOOM 176Bwins most of the NER tasks. Of the like-sized models, BloombergGPT per-\n",
            "forms the best placing ﬁrst once (Headlines), second four times (BN, P remium, Transcripts,\n",
            "Social media), third once (BFW), and last once (Filings).\n",
            "Exploratory Task: NER+NED Named entity disambiguation (NED) links entity\n",
            "mentions to known entities in knowledge bases or other structured information sources.\n",
            "Within the ﬁnancial world, we seek to link text mentions of \n",
            "\n",
            "Summary: We randomly sample 4,000 training and 500 test ing ex-amples from each internal dataset. We utilize 20-shot prompts an d evaluate using F1. The results from the internal NER tasks are mixed (Table 12). The muc h larger                BLOOM 176Bwins most of the N ER tasks. On NER+NED, BloombergGPT outperforms all other models by a large margin.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.63:\n",
            ") links entity\n",
            "mentions to known entities in knowledge bases or other structured information sources.\n",
            "Within the ﬁnancial world, we seek to link text mentions of companies to their ticker sym-\n",
            "bols, an abbreviation that uniquely identiﬁes publicly traded share s of a particular stock\n",
            "on a particular stock market.\n",
            "We directly test the ability of an LLM to complete this task by evaluatin g a joint\n",
            "NER+NED task: identify the stock tickers of companies mentioned in a document. This\n",
            "25requires the model to ﬁrst identify company mentions and then gene rate the corresponding\n",
            "stock ticker. For example, given “AAPL announced that they will stop usi ng Intel chips\n",
            "in future products.” the correct NER output would be “AAPL, Intel” whi le the correct\n",
            "NER+NED output would be “AAPL, INTC”.\n",
            "One of the advantages of this task is that it is robust to variations in extrac ting the\n",
            "exact text span. While NER evaluation requires exact matches, ticker s may be successfully\n",
            "produced without ﬁrst identifying spans. Furthermore, it evalu ates a model’s knowledge of\n",
            "companies, their various surface forms, and company to ticker mappings.\n",
            "We create evaluation data with linked tickers for this task by runnin g a state-of-the-\n",
            "art entity linking system for companies in ﬁnancial data over the Blo omberg internal NER\n",
            "annotated documents from each domain. We remove documents with no li nked tick \n",
            "\n",
            "Summary: We test the ability of an LLM to complete this task by evaluatin g a jointNER+NED task: identify the stock tickers of companies mentioned in a document. The task requires the model to identify company mentions and then gene rate the corresponding stock ticker. While NER evaluation requires exact matches, ticker s may be successfully produced without ﬁrst identifying spans.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.64:\n",
            " for companies in ﬁnancial data over the Blo omberg internal NER\n",
            "annotated documents from each domain. We remove documents with no li nked tickers. Fol-\n",
            "lowing our NER evaluations, we randomly sample 4,000 training and 500 testing examples\n",
            "from each ﬁltered internal dataset. We utilize 20-shot prompts and evalu ate using F1.\n",
            "Table 12 shows that BloombergGPT outperforms all other models by a large margin,\n",
            "except on social media data where it comes in second behind BLOOM 176B. In our social\n",
            "media data, companies are often referenced by their tickers, remov ing the requirement of the\n",
            "model to link the mention and reverting the task to NER. These resu lts further underscore\n",
            "the advantage of BloombergGPT for ﬁnancial tasks.\n",
            "5.4 BIG-bench Hard\n",
            "We now turn to evaluate BloombergGPT on standard, general-purpose NLP tasks. While\n",
            "the focus of our model is on ﬁnancial tasks, our inclusion of general-purp ose training data\n",
            "may help improve not only the ﬁnancial tasks, but also allow our model t o perform well\n",
            "on more standard NLP datasets. We start with BIG-bench Hard (Suzgun et al., 2022), a\n",
            "subset of the most challenging tasks in BIG-bench (Srivastava et al., 2022). It only includes\n",
            "tasks in which the best available model at construction was unable to ac hieve a performance\n",
            "higher than the average human rater via standard prompting techniques.\n",
            "Results for each task are shown in Table 13 \n",
            "\n",
            "Summary: Table 12 shows that BloombergGPT outperforms all other models by a large margin, except on social media data where it comes in second behind BLOOM 176B. While the focus of our model is on financial tasks, our inclusion of general-purp ose training data may help improve not only the ﬁnancial tasks, but also allow our model t o perform well on more standard NLP datasets.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.65:\n",
            " available model at construction was unable to ac hieve a performance\n",
            "higher than the average human rater via standard prompting techniques.\n",
            "Results for each task are shown in Table 13. Overall, while BloombergGPT falls\n",
            "behind the much larger PaLM 540B (10x parameters) and BLOOM 176B (3.5x parameters),\n",
            "it is the best-performing among similarly sized models. In fact, i ts performance is closer to\n",
            "BLOOM 176Bthan it is to either GPT-NeoX or OPT 66B. It further achieves the best perfor-\n",
            "mance of all models in date understanding, hyperbaton (ordering of adjec tives), and tracking\n",
            "shuﬄed objects. In sum, according to this benchmark, we ﬁnd that de veloping ﬁnance-\n",
            "speciﬁcBloombergGPT did not come at the expense of its general-purpose abilities.\n",
            "5.5 Knowledge Assessments\n",
            "We next assess knowledge, which we deﬁne as the ability to recall in formation seen during\n",
            "model training, via scenarios that have the model answer questions w ithout providing addi-\n",
            "tional context or resources (closed-book question answering). This in cludes multiple-choice\n",
            "questions, and we report accuracy. We follow the template of Brown et al. (2020). The list\n",
            "of scenarios is as follows:\n",
            "•ARC (Clark et al., 2018): Multiple-choice questions collected from 3rd to 9th grade\n",
            "science exams, includes easy and challenging splits.\n",
            "26BIG-bench Hard Task BloombergGPT GPT-NeoX OPT 66 \n",
            "\n",
            "Summary: BloombergGPT falls behind PaLM 540B and BLOOM 176B, but is the best-performing among similarly sized models. It achieves the best perfor-                mance of all models in date understanding, hyperbaton (ordering of adjec tives), and tracking grotesqueshuﬄed objects. We next assess knowledge, which we deﬁne as the ability to recall in formation seen during model training.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.66:\n",
            " 3rd to 9th grade\n",
            "science exams, includes easy and challenging splits.\n",
            "26BIG-bench Hard Task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B PaLM 540B\n",
            "Boolean Expressionsλ62.40 71.20 48.40 69.20 83.2\n",
            "Causal Judgement 49.73 52.41 51.87 51.87 61.0\n",
            "Date Understanding 54.80 45.60 49.60 50.00 53.6\n",
            "Disambiguation QA 34.00 40.80 40.40 40.40 60.8\n",
            "Dyck Languagesλ15.60 26.00 14.80 42.00 28.4\n",
            "Formal Fallacies 50.80 52.80 54.00 52.80 53.6\n",
            "Geometric Shapesλ15.20 8.00 11.60 22.40 37.6\n",
            "Hyperbaton 92.00 92.00 91.60 92.00 70.8\n",
            "Logical Deductionλ(avg) 34.53 30.93 31.87 34.00 60.4\n",
            "Movie Recommendation 90.40 86.40 91.20 91.20 87.2\n",
            "Multi-Step Arithmeticλ[Two] 1.20 0.40 0.40 0.00 1.6\n",
            "Navigateλ42.00 45.20 42.00 50.00 62.4\n",
            "Object Countingλ33.20 21.20 26.00 36.80 51.2\n",
            "Penguins in a Table 37.67 33.56 28.08 40.41 44.5\n",
            "Reasoning about Colored Objects 34.80 26.00 \n",
            "\n",
            "Summary: summarize:  3rd to 9th gradescience exams, includes easy and challenging splits. 26BIG-bench Hard Task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B PaLM 540BBoolean Expressionsλ62.40 71.20 48.40 69.20 83.2. Causal Judgement 49.73 52.41 51.87 51.80 53.6.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.67:\n",
            "\n",
            "Penguins in a Table 37.67 33.56 28.08 40.41 44.5\n",
            "Reasoning about Colored Objects 34.80 26.00 31.20 36.80 38.0\n",
            "Ruin Names 56.00 54.00 52.80 54.80 76.0\n",
            "Salient Translation Error Detection 20.00 20.40 16.40 23.60 48.8\n",
            "Snarks 69.66 62.36 69.66 72.47 78.1\n",
            "Sports Understanding 62.80 53.20 54.40 53.20 80.4\n",
            "Temporal Sequencesλ29.20 21.20 23.60 36.80 39.6\n",
            "Tracking Shuﬄed Objectsλ(avg) 25.33 24.53 24.00 23.47 19.6\n",
            "Web of Liesλ49.20 52.40 54.00 51.20 51.2\n",
            "Word Sortingλ4.80 5.20 2.40 7.60 32.0\n",
            "NLP Task (avg) 54.39 51.63 52.60 54.96 62.7\n",
            "Algorithmic Taskλ(avg) 28.42 27.84 25.37 33.95 40.9\n",
            "All Tasks (avg) 41.97 40.25 39.58 44.91 52.3\n",
            "All Tasks (WR) 0.57 0.45 0.39 0.75 -\n",
            "Table 13: BIG-bench hard results using standard 3-shot prompting. Follo wing the conven-\n",
            "tion from Suzgun et al. (2022), we denote algorithmic tasks with the supersc riptλ,\n",
            "and \n",
            "\n",
            "Summary: Summarize:  Penguins in a Table 37.67 33.56 28.08 40.41 44.5. Reasoning about Colored Objects 34.80 26.00 31.20 36.80 38.0Ruin Names 56.00 54.00 52.80 54.80 76.0.Salient Translation Error Detection 20.00 20.40 16.40 23.60 48.8. Snarks 69.36 69.66 72.47 78.1.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.68:\n",
            " prompting. Follo wing the conven-\n",
            "tion from Suzgun et al. (2022), we denote algorithmic tasks with the supersc riptλ,\n",
            "and present averages for NLP and algorithmic categories. The baseline numbe rs\n",
            "from PaLM 540B(Chowdhery et al., 2022) are taken from the original BBH paper.\n",
            "•CommonsenseQA (Talmor et al., 2019): Multiple-choice QA dataset that requires\n",
            "diﬀerent types of commonsense knowledge.\n",
            "•MMLU (Hendrycks et al., 2021): Manually collected multiple-choice knowled ge ques-\n",
            "tions in 57 subjects.\n",
            "•PhysicalQA (PiQA, Bisk et al., 2020): Questions about how the physical world\n",
            "works.\n",
            "BloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, and\n",
            "OPT66Bin one task, and comes second in the other three (Table 14). Similar to the\n",
            "previous section, it outperforms models of similar size while almost being on par with the\n",
            "much larger models. The Massive Multitask Language Understanding (MMLU, He ndrycks\n",
            "et al., 2021) covers 57 diﬀerent subjects and thus has a much wider cove rage than the tasks\n",
            "described above. The aggregated results in Table 15 paint a more consiste nt picture and\n",
            "follow the insights seen in BIG-bench hard. BloombergGPT consistently outperforms\n",
            "OPT66B, which in turn outperforms GPT-NeoX, while GPT-3 performs best. In contrast\n",
            "27Task BloombergG \n",
            "\n",
            "Summary: BloombergGPT consistently outperforms models of similar size while almost being on par with themuch larger models. The Massive Multitask Language Understanding (MMLU) covers 57 diﬀerent subjects and thus has a much wider cove rage than the tasks described above. BloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, andOPT66B.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.69:\n",
            " outperforms\n",
            "OPT66B, which in turn outperforms GPT-NeoX, while GPT-3 performs best. In contrast\n",
            "27Task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3\n",
            "ARC (easy) 73.99 70.79 71.25 75.93 71.2\n",
            "ARC (challenging) 48.63 45.39 44.54 50.85 53.2\n",
            "CommonsenseQA 65.52 60.36 66.42 64.21 -\n",
            "PiQA 77.86 75.84 77.58 77.04 80.5\n",
            "All Tasks (avg) 66.50 63.10 64.95 67.01 -\n",
            "All Tasks (WR) 0.75 0.08 0.33 0.67 -\n",
            "Table 14: Knowledge tasks 1-shot results. The baseline numbers from GP T-3 are taken\n",
            "from Brown et al. (2020). Among all models, BloombergGPT achieves the\n",
            "highest win rate among the models we ran ourselves, and performs second best\n",
            "on average.\n",
            "Model BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3\n",
            "Humanities 36.26 32.75 33.28 34.05 40.8\n",
            "STEM 35.12 33.43 30.72 36.75 36.7\n",
            "Social Sciences 40.04 36.63 38.32 41.50 50.4\n",
            "Other 46.36 42.29 42.63 46.48 48.8\n",
            "Average 39.18 35.95 35.99 39.13 43.9\n",
            "Table 15: Results (5- \n",
            "\n",
            "Summary: BloombergGPT achieves the highest win rate among the models we ran ourselves. GPT-3 performs second best on average. BloombergGPT outperformsOPT66B, which in turn outperforms GPT -NeoX, while G PT-3 is the best. The baseline numbers from GP T-3 are taken from Brown et. al. (2020) Among all models, BloombergG PT achieves thehighest win rate.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.70:\n",
            "29 42.63 46.48 48.8\n",
            "Average 39.18 35.95 35.99 39.13 43.9\n",
            "Table 15: Results (5-shot) on the MMLU (Hendrycks et al., 2021) benchmark. Th e base-\n",
            "line numbers from GPT-3 are taken from Hendrycks et al. (2021). While\n",
            "BloombergGPT lacks behind BLOOM 176Bon three of the categories, its aver-\n",
            "age is the highest among all models we evaluated ourselves. The gap to GPT- 3\n",
            "is largest on social sciences while the performance in other categories is close.\n",
            "to the previous sections, BloombergGPT also outperforms BLOOM 176Bin this category,\n",
            "although by a slim margin. It falls behind the reported performance of GP T-3, especially in\n",
            "the social science category. The gap to GPT-3 is closest in the STEM and “Other” domains\n",
            "which include ﬁnance and accounting-related questions.\n",
            "5.6 Reading Comprehension\n",
            "We deﬁne reading comprehension benchmarks as tasks in which the mod el can generate the\n",
            "correct response based on information contained in the presented inpu t text. Our grouping\n",
            "includes open-book QA tasks, as opposed to Brown et al. (2020), who separate them into\n",
            "a diﬀerent categories. We follow the template of Brown et al. (2020), and re port accuracy.\n",
            "We include the following tasks:\n",
            "•BoolQ (Clark et al., 2019): Yes/No questions about a passage from Wikipedia.\n",
            "•OpenBookQA (Mihaylov et al., \n",
            "\n",
            "Summary: BloombergGPT lacks behind BLOOM 176Bon three of the categories, but its aver-ishlyage is the highest among all models. The gap to GPT-3 is closest in the STEM and ‘Other’ domains. BloombergGPT also outperforms BLOom 176Bin this category, although by a slim margin. It falls behind the reported performance of GP T-3, especially in the social science category.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.71:\n",
            "•BoolQ (Clark et al., 2019): Yes/No questions about a passage from Wikipedia.\n",
            "•OpenBookQA (Mihaylov et al., 2018): Multiple-choice elementary-level scienc e\n",
            "questions, given a book of science facts, applied to new situations.\n",
            "•RACE (Lai et al., 2017): A multiple choice dataset of middle and high school English\n",
            "examinations.\n",
            "28RC Scenario BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3\n",
            "BoolQ 74.59 46.36 57.46 52.94 76.7\n",
            "OpenBookQA 51.60 44.20 58.00 47.20 58.8\n",
            "RACE (middle) 54.32 41.23 47.42 52.30 57.4\n",
            "RACE (high) 41.74 34.33 37.02 39.14 45.9\n",
            "MultiRC 62.29 22.86 18.80 26.65 72.9\n",
            "ReCoRD 82.79 67.86 82.53 78.01 90.2\n",
            "All Tasks (avg) 61.22 42.81 50.21 49.37 67.0\n",
            "All Tasks (WR) 0.94 0.06 0.50 0.50 -\n",
            "Table 16: Reading Comprehension Results (1-shot). The baseline numbe rs from GPT-3 are\n",
            "taken from Brown et al. (2020). BloombergGPT far outclasses the models we\n",
            "evaluated ourselves, and is slightly behind GPT-3.\n",
            "•Multi-Sentence Reading Comprehension (MultiRC, \n",
            "\n",
            "Summary: BloombergGPT far outclasses the models weevaluated ourselves, and is slightly behind GPT-3. Table 16: Reading Comprehension Results (1-shot). The baseline numbe rs are taken from Brown et al. (2020) BloombergGPT-NeoX OPT 66B BLOOM 176B GPT -3 BoolQ 74.59 46.36 57.46 52.94 76.7 OpenBookQA 51.60 44.20 58.00 47.2058.8 RACE (middle) 54.32 41.23 47.42 52.30 57.4RACE (high) 41.74 34.33 37.02 39.14 45.9ReCoRD 82.79 67.86 82.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.72:\n",
            "PT far outclasses the models we\n",
            "evaluated ourselves, and is slightly behind GPT-3.\n",
            "•Multi-Sentence Reading Comprehension (MultiRC, Khashabi et al., 2018): Short\n",
            "paragraphs and multi-sentence questions.\n",
            "•Reading Comprehension with Commonsense Reasoning (ReCoRD, Zhang\n",
            "et al., 2018): Automatically generated questions about CNN and Daily Mail news\n",
            "articles.\n",
            "Table 16 reﬂects a similar ranking as in the above evaluations: While GP T-3 has the\n",
            "highest performance, BloombergGPT is a close second. Except for OpenBookQA, The\n",
            "performance of BloombergGPT is the highest among BLOOM 176B, GPT-NeoX, and\n",
            "OPT66B. Surprisingly, BLOOM 176B falls behind signiﬁcantly in this category.\n",
            "5.7 Linguistic Tasks\n",
            "We deﬁne as linguistic tasks those scenarios that are not directly conne cted to user-facing\n",
            "applications. These include tasks that evaluate disambiguation, grammar, or entailment.\n",
            "These tasks are designed to directly assess a model’s ability to un derstand language. We\n",
            "follow the template of Brown et al. (2020), and report accuracy. The list of t asks is as\n",
            "follows:\n",
            "•Recognizing Textual Entailment (RTE, Dagan et al., 2007; Haim et al., 2006;\n",
            "Giampiccolo et al., 2007; Bentivogli et al., 2009): Given two text fragments, i dentify\n",
            "whether the meaning of one text is \n",
            "\n",
            "Summary: Table 16 reﬁne as linguistic tasks those scenarios that are not directly conne cted to user-facing applications. These include tasks that evaluate disambiguation, grammar, or entailment. The list of t asks is as follows:Recognizing Textual Entailment, Multi-Sentence Reading Comprehension, Shortparagraphs and multi-sentence questions.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.73:\n",
            "\n",
            "Giampiccolo et al., 2007; Bentivogli et al., 2009): Given two text fragments, i dentify\n",
            "whether the meaning of one text is entailed.\n",
            "•Adversarial NLI (ANLI, Nie et al., 2020): Adversarially constructed entailment\n",
            "detection.\n",
            "•CommitmentBank (CB, De Marneﬀe et al., 2019): Naturally occurring discourses\n",
            "whose ﬁnal sentence contains a clause-embedding predicate.\n",
            "•Choice of Plausible Alternatives (COPA, Gordon et al., 2011): Premise and two\n",
            "alternatives, where the task is to select the alternative that more p lausibly has a causal\n",
            "relation with the premise.\n",
            "•Words in Context (WIC Pilehvar and Camacho-Collados, 2019): Determine if a\n",
            "word is being used with the same meaning in two sentences.\n",
            "29Linguistic Scenario BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3\n",
            "RTE 69.31 53.79 54.87 57.40 70.4\n",
            "ANLI Round 1 32.90 32.60 33.10 33.60 32.0\n",
            "ANLI Round 2 34.40 33.80 34.20 33.80 33.9\n",
            "ANLI Round 3 37.33 36.17 34.92 35.17 35.1\n",
            "CB 53.57 48.21 44.64 48.21 64.3\n",
            "COPA 86.00 88.00 86.00 84.00 87.0\n",
            "WIC 52.51 50.00 52.51 50. \n",
            "\n",
            "Summary: Summarize: Given two text fragments, i dentify whether the meaning of one text is entailed. AnLI: Adversarially constructed entailmentdetection. CB: Naturally occurring discourses whose sentence contains a clause-embedding predicate. WIC: Determine if a word is being used with the same meaning in two sentences. Bloomberg: Linguistic Scenario Bloomberg.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.74:\n",
            " 64.3\n",
            "COPA 86.00 88.00 86.00 84.00 87.0\n",
            "WIC 52.51 50.00 52.51 50.16 48.6\n",
            "WinoGrad 80.95 79.12 82.78 78.02 89.7\n",
            "WinoGrande 64.09 60.62 66.14 67.01 73.2\n",
            "HellaSWAG 73.92 68.37 73.47 73.21 78.1\n",
            "StoryCloze 80.87 78.30 81.83 80.28 84.7\n",
            "All Tasks (avg) 60.63 57.18 58.59 58.26 63.4\n",
            "All Tasks (WR) 0.85 0.27 0.58 0.42 -\n",
            "Table 17: Results on the Linguistic Scenarios (1-shot). The baseline num bers from GPT-\n",
            "3 are taken from Brown et al. (2020). Win rates and averages are computed\n",
            "only based on accuracy numbers. BloombergGPT consistently scores highest\n",
            "among the models we evaluate, achieving an 85% win rate.\n",
            "•Winograd (Levesque et al., 2011): Determine which word a pronoun refers to when\n",
            "it is semantically unambiguous.\n",
            "•Winogrande (Sakaguchi et al., 2019): Adversarially mined challenging Winograd\n",
            "examples.\n",
            "•HellaSWAG (Zellers et al., 2019): Pick the best ending to a story or set of instruc -\n",
            "tions.\n",
            "•StoryCloze (Mostafazadeh et al., 2016): Select the correct ending sentence for � \n",
            "\n",
            "Summary: BloombergGPT consistently scores highestamong the models we evaluate, achieving an 85% win rate. Table 17: Results on the Linguistic Scenarios (1-shot). The baseline num bers from GPT- autoimmune3 are taken from Brown et al. (2020) Win rates and averages are computed only based on accuracy numbers. BloombergGPT is the most accurate of the models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.75:\n",
            " or set of instruc -\n",
            "tions.\n",
            "•StoryCloze (Mostafazadeh et al., 2016): Select the correct ending sentence for ﬁve-\n",
            "sentence long stories.\n",
            "The results (Table 17) for linguistic tasks follow a similar trend to t he knowledge category.\n",
            "BloombergGPT falls slightly behind GPT-3 and outperforms the other models. Simi lar\n",
            "to the reading comprehension category, BLOOM 176B falls behind BloombergGPT.\n",
            "5.8 Summary\n",
            "Across dozens of tasks in many benchmarks a clear picture emerges. Among t he models\n",
            "with tens of billions of parameters that we compare to, BloombergGPT performs the\n",
            "best. Furthermore, in some cases, it is competitive or exceeds the performance of much\n",
            "larger models (hundreds of billions of parameters). While our goal for BloombergGPT\n",
            "was to be a best-in-class model for ﬁnancial tasks, and we included gen eral-purpose training\n",
            "data to support domain-speciﬁc training, the model has still attained abilities on general-\n",
            "purpose data that exceed similarly sized models, and in some cases mat ch or outperform\n",
            "much larger models.\n",
            "30Input : Get me the last price and market cap for Apple\n",
            "Output :get(px_last,cur_mkt_cap) for([’AAPL US Equity’])\n",
            "Input : Tesla price\n",
            "Output :get(px_last) for([’TSLA US Equity’])\n",
            "Input : Get the yield and spread for EC527035 Corp and AL580550 Corp\n",
            "Output :get(yield,spread) for \n",
            "\n",
            "Summary: The results (Table 17) for linguistic tasks follow a similar trend to t he knowledge category. BloombergGPT falls slightly behind GPT-3 and outperforms the other models. Simi larto the reading comprehension category, BLOOM 176B falls behind BloombergG PT. The model has still attained abilities on general-purpose data that exceed similarly sized models. In some cases it is competitive or exceeds the performance of much larger models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.76:\n",
            " US Equity’])\n",
            "Input : Get the yield and spread for EC527035 Corp and AL580550 Corp\n",
            "Output :get(yield,spread) for([’EC527035 Corp’,’AL580550 Corp’])\n",
            "Input : apple and ibm market cap and eps\n",
            "Output :get(cur_mkt_cap,is_eps) for([’AAPL US Equity’,’IBM US Equity’])\n",
            "Input : industry subgroup of ibm apple microsoft google\n",
            "Output :get(industry_subgroup()) for([’AAPL US Equity’,’IBM US Equity’,\n",
            "’MSFT US Equity’,’GOOGL US Equity’])\n",
            "Figure 4: Using BloombergGPT to generate valid Bloomberg Query Language. Using\n",
            "only a few examples in a few-shot setting, the model can utilize its knowledge\n",
            "about stock tickers and ﬁnancial terms to compose valid queries to ret rieve the\n",
            "data, given a request in natural language. In each case, the model is given 3\n",
            "examples (not shown) followed by the ‘Input” and a prompt of “Output: ”.\n",
            "6 Qualitative Samples\n",
            "We now share qualitative examples from our model that highlight the ben eﬁts of our domain\n",
            "specialization.\n",
            "Generation of Bloomberg Query Language. One use case for BloombergGPT is to\n",
            "make interactions with ﬁnancial data more natural. An existing way to re trieve data is via\n",
            "the Bloomberg Query Language (BQL). B \n",
            "\n",
            "Summary: The model can utilize its knowledge about stock tickers and ﬁnancial terms to compose valid queries to ret rieve the data, given a request in natural language. One use case for BloombergGPT is to make interactions with financial data more natural. In each case, the model is given 3 examples (not shown) followed by the ‘Input’ and a prompt of “Output: ”.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.77:\n",
            " is to\n",
            "make interactions with ﬁnancial data more natural. An existing way to re trieve data is via\n",
            "the Bloomberg Query Language (BQL). BQL can be used to interact with diﬀ erent classes\n",
            "of securities, each with its own ﬁelds, functions, and parameters. BQL is an incredibly\n",
            "powerful but complex tool. As we show in Figure 4, BloombergGPT can be utilized to\n",
            "make BQL more accessible by transforming natural language queries into v alid BQL.\n",
            "Suggestion of News Headlines. Other use cases that are well supported are in the news\n",
            "space. Since it is trained on many news articles, it can be used for many news applications\n",
            "and assist journalists in their day-to-day work. For example, when con structing newsletters,\n",
            "journalists may have to write short headlines for each new section. W hile a dedicated model\n",
            "to help with this task may be too expensive to maintain, BloombergGPT performs well\n",
            "out of the box (Figure 5).\n",
            "Financial Question Answering. Due to the ﬁnancial domain training data, we are able\n",
            "to query BloombergGPT for knowledge relevant to the ﬁnancial world. For example, it\n",
            "performs well at identifying the CEO of a company. Figure 6 shows sev eral examples in-\n",
            "cluding output from other models. While BloombergGPT correctly identiﬁes the CEOs,\n",
            "GPT-NeoX does not, and FLAN-T5-XXL completely fails, consistently ignoring the com-\n",
            "31Input : The US housing market shrank in value by $2. \n",
            "\n",
            "Summary: BloombergGPT can be utilized to make interactions with financial data more natural. It can be used for many news applications and to assist journalists in their day-to-day work. BloombergGPT-NeoX does not, and FLAN-T5-XXL completely fails, consistently ignoring the US housing market. The Bloomberg Query Language (BQL) is an incredibly powerful but complex tool.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.78:\n",
            " not, and FLAN-T5-XXL completely fails, consistently ignoring the com-\n",
            "31Input : The US housing market shrank in value by $2.3 trillion, or 4.9%, in the second\n",
            "half of 2022, according to Redﬁn. That’s the largest drop in percentage terms since the\n",
            "2008 housing crisis, when values slumped 5.8% during the same period.\n",
            "Output : Home Prices See Biggest Drop in 15 Years\n",
            "Input : The global economy is in a better place today than many predicted month s ago,\n",
            "Janet Yellen said at the G20. At home, she pointed to a resilient US econom y, where\n",
            "headline inﬂation has moderated and the labor market is strong. She also cal led for the\n",
            "IMF to move swiftly toward a fully-ﬁnanced program for Ukraine.\n",
            "Output : Yellen Sees Global Economy More Resilient Than Expected\n",
            "Input : Google was sued by the US and eight states seeking the breakup of its\n",
            "ad-tech business for allegedly monopolizing the digital advertising m arket. The lawsuit\n",
            "is the Biden administration’s ﬁrst big challenge to a tech titan and on e of the rare times\n",
            "since 1982 that the DOJ has sought to cleave up a major company.\n",
            "Output : Google Sued for Monopoly in Online Ad Market\n",
            "Figure 5: Using BloombergGPT to generate short headline suggestions in a three-shot\n",
            "setting. Bloomberg News sends many newsletters a day that require s these head-\n",
            "lines.BloombergGPT could help with the editing process by suggesting initial\n",
            "headlines from the text.\n",
            "p \n",
            "\n",
            "Summary: The US housing market shrank in value by $2.3 trillion, or 4.9%, in the secondhalf of 2022. That’s the largest drop in percentage terms since the 2008 housing crisis. Google was sued by the US and eight states seeking the breakup of its digital advertising business for allegedly monopolizing the digital advertising m arket. The DOJ has sought to cleave up a major company for the first time since 1982.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.79:\n",
            " sends many newsletters a day that require s these head-\n",
            "lines.BloombergGPT could help with the editing process by suggesting initial\n",
            "headlines from the text.\n",
            "pany and instead predicting the CEO at Cirrus Logic who was included in the prompt.\n",
            "WhileBloombergGPT does not perfectly solve this task and makes mistakes, we were\n",
            "not able to ﬁnd any example where the other models solved the task whil eBloombergGPT\n",
            "did not.\n",
            "7 Related Work\n",
            "Language Models. Language modeling has a long history in the NLP community. The\n",
            "idea of training a probabilistic language model for scoring word sequenc es was likely ﬁrst\n",
            "introduced by Jelinek (1976). N-gram models were popular for decades (Br own et al.,\n",
            "1992), and were trained on corpora up to 2 trillion tokens (Brants et al., 2007). R esearch\n",
            "on training language models accelerated over the last decade due to inno vations in machine\n",
            "learning, data availability, and compute. Early work in autoregressive l anguage modeling\n",
            "(e.g., Mikolov et al., 2010; Sutskever et al., 2011) used recurrent neural n etworks, but\n",
            "these were small models trained on small datasets. The introduction of the transformer\n",
            "architecture (Vaswani et al., 2017) facilitated the scaling of these mode ls in terms of data,\n",
            "compute, and the number of parameters.\n",
            "The process of developing models that could better approximate the distribution of\n",
            "language over large corpora led to the discovery that the representation s these models\n",
            "produce are useful starting points for many downstream tasks \n",
            "\n",
            "Summary: BloombergGPT could help with the editing process by suggesting initial headlines from the text. It could also predict the CEO at Cirrus Logic who was included in the prompt. We were not able to find any example where the other models solved the task whil e BloombergGPT did not.7 Related Work                Language Models. Language modeling has a long history in the NLP community.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.80:\n",
            " models that could better approximate the distribution of\n",
            "language over large corpora led to the discovery that the representation s these models\n",
            "produce are useful starting points for many downstream tasks. This w as demonstrated by\n",
            "Radford et al. (2018) and Howard and Ruder (2018) who showed that generative pretr aining\n",
            "32Input : CEO of Assicurazioni Generali?\n",
            "BloombergGPT : Philippe Donnet\n",
            "GPT-NeoX : Antonio De Lorenzo, Simone Gambarini, Enrico Zanetti\n",
            "FLAN-T5-XXL : John M Forsyth, Christopher K Peters, {empty string }\n",
            "Input : CEO of Silicon Valley Bank?\n",
            "BloombergGPT : Greg Becker\n",
            "GPT-NeoX : John Hodor, Mike Fridgen, Peter R Briger\n",
            "FLAN-T5-XXL : Christian Klein, John M Forsyth, John M Forsyth\n",
            "Input : CEO of IHS Markit Ltd?\n",
            "BloombergGPT : Lance Uggla\n",
            "GPT-NeoX : John F Lechleiter, David Steward, Niall O Broin\n",
            "FLAN-T5-XXL : William M Hawkins, John M Forsyth, John M Forsyth\n",
            "Input : CEO of Walgreens Boots Alliance?\n",
            "BloombergGPT : Rosalind Brewer\n",
            "GPT-NeoX : Mark V Dei, Stefano Pessina, Stefano Pessina\n",
            "FLAN-T5-XXL : Christian Klein, John M Forsyth, John M Forsyth\n",
            "Input : CEO of Citigroup Inc?\n",
            "BloombergGPT : Jane Fraser\n",
            "GPT-NeoX \n",
            "\n",
            "Summary:  models that could better approximate the distribution of                language over large corpora led to the discovery that the representation s these modelsproduce are useful starting points for many downstream tasks. This w as demonstrated by Radford et al. (2018) and Howard and Ruder ( 2018) who showed that generative pretr aining can be used to predict the performance of large companies. The study was published in the open-source journal, Theoretical Computer Science.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.81:\n",
            " Christian Klein, John M Forsyth, John M Forsyth\n",
            "Input : CEO of Citigroup Inc?\n",
            "BloombergGPT : Jane Fraser\n",
            "GPT-NeoX : Michael L Corbat, Michael L Corbat, Michael L Corbat*\n",
            "FLAN-T5-XXL : Christian Sewing, John M Forsyth, John M Forsyth\n",
            "Figure 6: Testing the ability of BloombergGPT, GPT-NeoX, and FLAN-T5-XXL to re-\n",
            "call the names of CEOs of companies. Each model is run in a 10-shot setting.\n",
            "We sample up to three answers and present all of them if they are incorr ect.\n",
            "*Michael Corbat was CEO of Citigroup until 2021, highlighting the importance\n",
            "of an up-to-date model.\n",
            "with an autoregressive language modeling objective achieves strong per formance in transfer\n",
            "learning. Radford et al. (2019) further showed scaling the model size and training data led\n",
            "to autoregressive language models that perform well in diﬀerent downs tream tasks without\n",
            "any additional supervised ﬁne-tuning.\n",
            "Brown et al. (2020) showed that further scaling the models led to the e mergence of new\n",
            "model capabilities and increased model robustness. Since the rel ease of GPT-3 by Brown\n",
            "et al. (2020), many other researchers built large language models to study dat a quantity,\n",
            "data quality, network architecture, parameter scaling, data scaling, t okenization, and open-\n",
            "sourcing strategies (Raﬀel et al., 2020; Zhang et al., 2022a; Black et al., 2022 \n",
            "\n",
            "Summary: BloombergGPT, GPT-NeoX, and FLAN-T5-XXL are tested. Each model is run in a 10-shot setting. We sample up to three answers and present all of them if they are incorr ect. Michael Corbat was CEO of Citigroup until 2021, highlighting the importance of an up-to-date model. Since the rel ease of G PT-3 by Brownet al. (2020), many other researchers built large language models to study dat a quantity.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.82:\n",
            " t okenization, and open-\n",
            "sourcing strategies (Raﬀel et al., 2020; Zhang et al., 2022a; Black et al., 2022; Rae et al.,\n",
            "2021; Hoﬀmann et al., 2022; Chowdhery et al., 2022; Lieber et al., 2021; Zeng et al., 2022;\n",
            "Tafjord and Clark, 2021; Smith et al., 2022; Scao et al., 2022; Taylor et al., 2022; Lin et al.,\n",
            "332022; Soltan et al., 2022; Thoppilan et al., 2022; Bao et al., 2022; Sanh et al., 2022; Roller\n",
            "et al., 2021; Glaese et al., 2022; Wang et al., 2021; Peng et al., 2022, among many others).\n",
            "Domain-Speciﬁc Large Language Models. The value of domain-speciﬁc training for\n",
            "masked (encoder only) language models is well established. Commonly ac cepted approaches\n",
            "are to train BERT models (Devlin et al., 2019) from scratch on domain-speci ﬁc data or to\n",
            "continue pretraining an existing model on new domain-speciﬁc data ( Gururangan et al.,\n",
            "2020). Following these strategies, BioBERT (Lee et al., 2020) adapts BERT to th e biomed-\n",
            "ical domain and SciBERT is trained on scientiﬁc publications (Beltagy e t al., 2019). The\n",
            "results of these papers showed that in-domain training allows model s to outperform prev \n",
            "\n",
            "Summary: The value of domain-speciﬁc training for language models is well established. Commonly ac cepted approaches are to train BERT models from scratch or to continue pretraining an existing model on new domain- Speci ﬁC data. Following these strategies, BioBERT (Lee et al., 2020) adapts BERT to th e biomed-                ical domain and SciBERT is trained on scientiﬀc publications.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.83:\n",
            "iﬁc publications (Beltagy e t al., 2019). The\n",
            "results of these papers showed that in-domain training allows model s to outperform previ-\n",
            "ous state-of-the-art models in a variety of biomedical text mining tas ks. Further examples\n",
            "of this paradigm are ClinicalBERT for the clinical domain (Huang et al., 2019), B ioMed-\n",
            "RoBERTa for scientiﬁc biomedical papers (Gururangan et al., 2020), and BERT weet and\n",
            "Bernice for Twitter data (Nguyen et al., 2020; DeLucia et al., 2022).\n",
            "Since the training of auto-regressive—decoder-only—language models of m ore than 10B\n",
            "parameters is signiﬁcantly more costly than training masked LMs under 1B parameters,\n",
            "there have been much fewer examples of domain-speciﬁc autoregressi ve models. However,\n",
            "existing approaches follow the same two strategies. Adapting an existi ng model, medPaLM\n",
            "(Singhal et al., 2022) adapted PaLM to the biomedical domain and Minerva (Lewkow ycz\n",
            "et al., 2022) to mathematical reasoning tasks.\n",
            "Recently, several examples of from-scratch trained decoder-only m odels for domain-\n",
            "speciﬁc data have emerged. One popular domain is protein sequences s ince they can be\n",
            "represented using language-like sequences but are not covered by n atural language mod-\n",
            "els (e.g., Lin et al., 2022; Xiao et al., 2021; Nijkamp et al., 2022 \n",
            "\n",
            "Summary: In-domain training allows model s to outperform state-of-the-art models in biomedical text mining tas ks. The training of auto-regressive—decoder-only—language models of m ore than 10Bparameters is signiﬁcantly more costly than training masked LMs under 1B parameters. One popular domain is protein sequences s ince they can berepresented using language-like sequences.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.84:\n",
            " not covered by n atural language mod-\n",
            "els (e.g., Lin et al., 2022; Xiao et al., 2021; Nijkamp et al., 2022). However, there can b e\n",
            "beneﬁts even for models in natural language domains. Galactica is trained exclusively on\n",
            "a large collection of scientiﬁc datasets, and includes special proces sing to handle scientiﬁc\n",
            "notations (Taylor et al., 2022). While performing very well on scientiﬁc tasks, Galactica\n",
            "also surprisingly also performs well on more standard NLP tasks. BioGPT (Lu o et al.,\n",
            "2022) and BioMedLM (Bolton et al., 2023) are both smaller GPT-style models traine d on\n",
            "biomedical data. Lehman et al. (2023) compares encoder/decoder models train ed exclusively\n",
            "on domain-speciﬁc data, versus those adapted from general-purpose traini ng. Researchers\n",
            "working on large generative language dialog models have reached similar conc lusions about\n",
            "the beneﬁts of using domain-speciﬁc training data (Zhang et al., 2020; Rol ler et al., 2021;\n",
            "Thoppilan et al., 2022).\n",
            "These ﬁndings highlight the advantages of in-domain pretraining, especi ally if suﬃcient\n",
            "data is available, as it is in our case. Inspired by the general capabilit ies of Galactica, we\n",
            "augment our private data with public data with the goal of investigating wh ether a model\n",
            " \n",
            "\n",
            "Summary: Galactica is trained exclusively on a large collection of scientiﬁc datasets. It includes special proces sing to handle Scientology-relatednotations. Galactica also surprisingly also performs well on more standard NLP tasks. We are inspired by the general capabilit ies of GalActica, we want to augment our private data with public data with the goal of making NLP more accessible.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.85:\n",
            ". Inspired by the general capabilit ies of Galactica, we\n",
            "augment our private data with public data with the goal of investigating wh ether a model\n",
            "can gain in-domain capabilities without sacriﬁcing general-domain perfor mance.\n",
            "Training Data. Large corpora of raw text data are critical for training LLMs. As a\n",
            "result, there are now several corpora available that cover a wide range of sources.\n",
            "The Colossal Clean Crawled Corpus (C4, Raﬀel et al., 2020) draws from Common Cr awl\n",
            "to create a processed training corpus. The Pile is a carefully cur ated corpus that contains\n",
            "a wide range of data sources (Gao et al., 2021). These datasets are built on or inc lude\n",
            "web crawls (OpenWebText2) augmented with an array of data from high-qu ality sources\n",
            "(Pubmed, Arxiv). Various eﬀorts aim to clean datasets, especially web data, by removing\n",
            "34unwanted or harmful text (Touvron et al., 2023; Rae et al., 2020). BLOOM (Scao et al.,\n",
            "2022) carefully selected data sources and included various ﬁltering me chanisms (Jernite\n",
            "et al., 2022).\n",
            "While web data is an eﬀective strategy for obtaining large amounts of diver se data,\n",
            "robust cleaning eﬀorts still result in data artifacts, duplicates ( Carlini et al., 2020), various\n",
            "types of toxic language (Welbl et al., 2021), and it can lead to unintended mar g \n",
            "\n",
            "Summary: Large corpora of raw text data are critical for training LLMs. There are now several corpora available that cover a wide range of sources. Various eﬀorts aim to clean datasets, especially web data, by removing harmful text. These datasets are built on or inc ludely augmented with an array of data from high-qu ality sources. The aim is to investigate whether a model can gain in-domain capabilities without sacriﬁcing general-domain perfor mance.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.86:\n",
            ", duplicates ( Carlini et al., 2020), various\n",
            "types of toxic language (Welbl et al., 2021), and it can lead to unintended mar ginalization\n",
            "of minority voices (Xu et al., 2021). Dodge et al. (2021) studied C4 to better un derstand\n",
            "the metadata, and the included and excluded data. Their ﬁndings suggest that C4 contains\n",
            "machine-generated text, is biased due to exclusion ﬁlters and might contain examples drawn\n",
            "from evaluation datasets for NLP tasks. A similar eﬀort was undertaken by Zen g et al.\n",
            "(2022) to document the pre-processing they undertook to train thei r Chinese large language\n",
            "model.\n",
            "Lee et al. (2022a) investigated the impact of deduplication on model performan ce for\n",
            "several datasets and found that deduplication reduces the emission of m emorized training\n",
            "data, allows better estimation of the generalization error, and improves training time and\n",
            "cost without impacting performance. These insights highlight the im portance and challenges\n",
            "of constructing high-quality training corpora. As discussed in §2, Bloomberg’s core business\n",
            "curates and provides access to datasets, which we use to construct a high-quality dataset\n",
            "FinPile to trainBloombergGPT, resulting in best-in-class ﬁnancial performance.\n",
            "Evaluation. The tasks addressed by language models have vastly increased and requi re a\n",
            "very diﬀerent evaluation process from traditional task-speciﬁc sys tems. There have been two\n",
            "paradigms for LL \n",
            "\n",
            "Summary: The tasks addressed by language models have vastly increased and requi re a very diﬀerent evaluation process from traditional task-speciﬁcsys tems. Bloomberg’s core business curates and provides access to datasets, which we use to construct a high-quality dataset to trainBloombergGPT, resulting in best-in-class ﬁnancial performance.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.87:\n",
            " a\n",
            "very diﬀerent evaluation process from traditional task-speciﬁc sys tems. There have been two\n",
            "paradigms for LLM evaluation: The ﬁrst is to evaluate a model in many diﬀere nt scenarios\n",
            "via automatic evaluation (Liang et al., 2022; Srivastava et al., 2022) and the second is to\n",
            "perform extrinsic and task-speciﬁc evaluations by integrating them into user workﬂows (e.g.,\n",
            "Lee et al., 2022b; Goyal et al., 2022).\n",
            "While the second strategy is necessary for assessing deployments of models in products,\n",
            "it is infeasible to run these human evaluations at a scale of the ﬁrst st rategy and it is thus\n",
            "standard to follow the ﬁrst strategy when introducing new models. In our case, we combine\n",
            "multiple general-purpose evaluations from multiple existing benc hmarks that have diﬀerent\n",
            "goals. Srivastava et al. (2022) aim for maximum coverage by soliciting tasks fr om the\n",
            "entire research community, while HELM (Liang et al., 2022) suggests evaluati on in various\n",
            "“scenarios” that are represented through speciﬁc datasets. Earlier lan guage model papers\n",
            "developed their own evaluation schemata (Brown et al., 2020). While the se benchmarks\n",
            "allow for a side-by-side comparison between models, it is challengi ng to ensure that all\n",
            "experimental parameters (prompts, decoding strategies, few-shot examples, \n",
            "\n",
            "Summary: There have been two strategies for LLM evaluation. The first is to evaluate a model in many diﬀere nt scenarios via automatic evaluation (Liang et al., 2022; Srivastava et al. 2022) and the second is to integrate models into user work. In our case, we combine multiple general-purpose evaluations from multiple existing benc hmarks that have di ﬀerent goals.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.88:\n",
            " side-by-side comparison between models, it is challengi ng to ensure that all\n",
            "experimental parameters (prompts, decoding strategies, few-shot examples, etc.) are the\n",
            "same. For that reason, we diﬀerentiate between reported and veriﬁed n umbers in our\n",
            "evaluation ( §5).\n",
            "Beyond the general-purpose evaluation, we also require a targeted domain evaluation.\n",
            "Prior domain-speciﬁc models like Galactica (Taylor et al., 2022) chose a s et of tasks that\n",
            "the model is likely to perform well on. In their case, these were v arious scientiﬁc tasks.\n",
            "However, there exists no standard benchmark for the ﬁnancial NLP domain. While the\n",
            "recent work on FLUE (Shah et al., 2022) aims to provide such a benchmark, it h as limited\n",
            "coverage of relevant tasks, no suggested evaluation strategy for few-shot l earning, and the\n",
            "quality of some annotations is low. To provide externally comparable res ults, we developed\n",
            "35a few-shot strategy for FLUE, but also decided to augment the publicly a vailable evaluation\n",
            "tasks with company-internal benchmarks.\n",
            "Model Size. Large language model training remains expensive in terms of the compu-\n",
            "tational cost and human eﬀort to assemble data and train the model. Determ ining the\n",
            "optimal amount of training data and model shape and size for the best utili zation of re-\n",
            "sources becomes important.\n",
            "Kaplan et al. (2020) ﬁrst studied the dependence of language model \n",
            "\n",
            "Summary: There exists no standard benchmark for the ﬁnancial NLP domain. Large language model training remains expensive. Determ ining theoptimal amount of training data and model shape and size for the best utili zation of re-estylesources is important. We developed a few-shot strategy for FLUE, but also decided to augment the publicly a vailable evaluation with company-internal benchmarks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.89:\n",
            " the best utili zation of re-\n",
            "sources becomes important.\n",
            "Kaplan et al. (2020) ﬁrst studied the dependence of language model performan ce on\n",
            "architecture, parameter size, compute power, and dataset size. The y reported that the\n",
            "number of model parameters, the dataset size, and the amount of compute i mproves perfor-\n",
            "mance on the autoregressive language modeling objective smoothly accordi ng to the power\n",
            "law. A similar investigation by Hernandez et al. (2021) into data transfer f or diﬀering dis-\n",
            "tributions found that this also follows a power law. Moving beyond s tudying the eﬀect on\n",
            "loss, Rae et al. (2021) analyzed the eﬀect of scale on undesirable properties such as bias\n",
            "and toxicity by training a wide range of model sizes.\n",
            "Comparing model architectures, Levine et al. (2020) studied the scalin g of models that\n",
            "use self-attention and derived guidelines for depth-to-width allo cation. Tay et al. (2021)\n",
            "reported that model shape (depth-width ratio) impacted performance on downstream tasks\n",
            "even if it had minimal impact on the pretraining objective. Tay et al. (2022a) further\n",
            "studied the eﬀect of scaling for diﬀerent model architectures and showed that architecture\n",
            "choice is pertinent when scaling and that the vanilla transformer arc hitecture scales best.\n",
            "Of particular importance to this work is the study of Hoﬀmann et al. (2022), wh o inves-\n",
            "tigated \n",
            "\n",
            "Summary: Kaplan et al. (2020) studied the dependence of language model performan ce onarchitecture, parameter size, compute power, and dataset size. A similar investigation by Hernandez et.al. (2021) into data transfer f or diﬀering dis-                tributions found that this also follows a power law. Tay and Rae (2022a) further demonstrated the importance of architecturechoice when scaling.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.90:\n",
            "ure scales best.\n",
            "Of particular importance to this work is the study of Hoﬀmann et al. (2022), wh o inves-\n",
            "tigated the eﬀect of model size and the number of training tokens on the performance of a\n",
            "model given a ﬁxed compute budget. They posited that existing large l anguage models were\n",
            "undertrained and that model size and the number of training tokens sh ould be scaled equally.\n",
            "They demonstrated this hypothesis through Chinchilla, a model sign iﬁcantly smaller, yet\n",
            "higher performing, than most of the largest LLMs. These ﬁndings opened the door for\n",
            "“Chinchilla optimal” training of smaller models that achieve strong p erformance, and for\n",
            "which inference can be run much more eﬃciently than for their larger counterparts. These\n",
            "ﬁndings led us to consider a nearly Chinchilla-optimal model using a standard architecture.\n",
            "Tokenization. Tokenization and vocabulary choice play a critical role in model perfor -\n",
            "mance as they can help the model learn meaningful representations and ge neralize to unseen\n",
            "words. Byte-Pair encoding (BPE) (Sennrich et al., 2016) learns a greed y bottom-up vo-\n",
            "cabulary by repeatedly merging the most frequent sequence pairs in the training set till a\n",
            "predetermined vocabulary size is reached. Radford et al. (2018) adapted B PE by limiting\n",
            "the base vocabulary to be all possible bytes as opposed to all Unicode char acters. Wordpiece\n",
            "tokenization (Schuster and Nakajima, 2012) also learns \n",
            "\n",
            "Summary: The study of Hoﬀmann et al. (2022) posited that existing large l anguage models were undertrained. They demonstrated this hypothesis through Chinchilla, a model sign iﬁcantly smaller, yet higher performing, than most of the largest LLMs. These findings opened the door for “Chinchilla optimal” training of smaller models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.91:\n",
            "\n",
            "the base vocabulary to be all possible bytes as opposed to all Unicode char acters. Wordpiece\n",
            "tokenization (Schuster and Nakajima, 2012) also learns a greedy bottom-up voc abulary by\n",
            "repeatedly merging the sequence-pair that maximizes the likelih ood of the training data,\n",
            "which is a slight deviation from the method in Sennrich et al. (2016).\n",
            "In contrast to BPE and Wordpiece, the Unigram tokenizer (Kudo, 2018) learns a top-\n",
            "down vocabulary by ﬁrst initializing a large vocabulary and repeatedl y discarding those\n",
            "vocabulary items that increase loss (e.g., log-likelihood of the train ing data) the least. By\n",
            "construction, the Unigram model can tokenize an input text in several d iﬀerent ways. That\n",
            "is, the Unigram model saves probabilities allowing for smarter tokeni zation at inference time.\n",
            "36Finally, SentencePiece (Kudo and Richardson, 2018) adapts the schemes mentioned\n",
            "above to handle languages that are not space separated. Beltagy et al. (2019) constr ucted a\n",
            "vocabulary speciﬁc to scientiﬁc text and observed that their domain -speciﬁc trained vocab-\n",
            "ulary only had a 42% overlap with the non-domain-speciﬁc BERT vocabulary trained on\n",
            "general domain text. Similarly, Lewis et al. (2020) showed that a dedicated biomedical vo-\n",
            "cabulary improved performance on sequence labeling tasks consisten tly. Lieber et \n",
            "\n",
            "Summary: The Unigram tokenizer learns a top-down vocabulary by initializing a large vocabulary and repeatedl y discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. Beltagy et al. (2019) constr ucted a biomedical vo-                cabulary to scientiﬁc text.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.92:\n",
            " text. Similarly, Lewis et al. (2020) showed that a dedicated biomedical vo-\n",
            "cabulary improved performance on sequence labeling tasks consisten tly. Lieber et al. (2021)\n",
            "constructed a larger vocabulary to ensure token eﬃciency, which th e authors claim resulted\n",
            "in reduced training time and better semantic representation. The se ﬁndings demonstrate\n",
            "the importance of selecting a tokenizer and accompanying vocabulary th at best reﬂects that\n",
            "training domain. For those reasons, we decided to train our own unigram tok enizer instead\n",
            "of relying on existing public ones.\n",
            "Positional Embeddings. Transformer-based models rely on positional embeddings to\n",
            "encode position and location information of words in a text. Encoding th e sequence posi-\n",
            "tion and the eﬀect of this choice on model performance have been studi ed extensively. These\n",
            "include sinusoidal embeddings (Vaswani et al., 2017), rotary position embe ddings (Su et al.,\n",
            "2021a), adding relative position bias (Raﬀel et al., 2020), and adding linear biase s to atten-\n",
            "tion heads (Press et al., 2022). A side-eﬀect of the strategy in Press et al. (2022) is that one\n",
            "can train on shorter sequences without loss in performance on longer sequ ences. This has\n",
            "two beneﬁts: ﬁrst, models learn to generalize (extrapolate) to longe r sequences and second,\n",
            "models can be trained on shorter sequences reducing training time.\n",
            " \n",
            "\n",
            "Summary: We train our own unigram tok enizer instead of relying on existing public ones. Transformer-based models rely on positional embeddings to code position and location information of words in a text. A dedicated biomedical vo-                cabulary improved performance on sequence labeling tasks consisten tly. The se ﬁndings demonstrate the importance of selecting a tokenizer and accompanying vocabulary.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.93:\n",
            "�rst, models learn to generalize (extrapolate) to longe r sequences and second,\n",
            "models can be trained on shorter sequences reducing training time.\n",
            "8 Ethics, Limitations, and Implications\n",
            "The rapid development and adoption of large language models have been accompan ied by\n",
            "a rigorous conversation about the ethics, uses, and limitations of these m odels. For a more\n",
            "complete treatment of these topics, we direct the reader to Bommasani et al. (2021); Bender\n",
            "et al. (2021); Birhane et al. (2022); Weidinger et al. (2021, 2022). We discuss issues that\n",
            "are directly relevant to the development of BloombergGPT.\n",
            "8.1 Ethical Use\n",
            "Finance is a sensitive area for technology, and ensuring accurate, fact ual information is\n",
            "crucial for our products, our clients, and the ﬁrm’s reputation in th e marketplace. On\n",
            "the other hand, our clients are also eager to adopt state-of-the-art techn ology to support\n",
            "their workﬂows. To provide natural language applications to the ﬁnancial community, we\n",
            "have developed a rigorous risk and testing assessment process. Thi s process includes careful\n",
            "annotation guidelines (Tseng et al., 2020), pre-launch review at multipl e levels by the central\n",
            "risk and compliance organizations, and by the product leaders (e.g., the newsroom) as\n",
            "applicable, and post-launch monitoring. Moreover, we conduct our resear ch, development,\n",
            "and deployment of NLP and AI systems in accordance with all applicable regul ations \n",
            "\n",
            "Summary: The rapid development and adoption of large language models have been accompanied by a rigorous conversation about the ethics, uses, and limitations of these m odels. We discuss issues that are directly relevant to the development of BloombergGPT. We conduct our resear ch, development, and deployment of NLP and AI systems in accordance with ethical guidelines. We conclude by discussing the implications of our findings for the financial industry.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.94:\n",
            "able, and post-launch monitoring. Moreover, we conduct our resear ch, development,\n",
            "and deployment of NLP and AI systems in accordance with all applicable regul ations.\n",
            "Similarly, toxicity and bias are areas where, as a company, we take extraor dinary care\n",
            "with any content we produce, whether from humans or machines. Since the measurement of\n",
            "toxicity and bias in our model depends on its application areas, quantif ying the potential for\n",
            "the generation of harmful language remains an open question. We are particular ly interested\n",
            "in studying whether FinPile, which is cleaner and contains fewer examples of overtly biased\n",
            "37or toxic language (e.g., Press Releases), reduces the proclivity of t he model to generate\n",
            "inappropriate content. As we move to develop products built on this t echnology, we will\n",
            "apply existing testing procedures, as well as risk and compliance c ontrols, to ensure safe use.\n",
            "8.2 Openness\n",
            "An ongoing debate in the community concerns how LLMs should be released, i f at all. While\n",
            "models that are not publicly available cannot be fully evaluated by the community, distribut-\n",
            "ing models can lead to nefarious purposes. Especially for a model lik eBloombergGPT,\n",
            "which is trained on a signiﬁcant amount of press releases, news article s, and ﬁlings, a release\n",
            "carries a high risk for abuse through imitation.\n",
            "We have witnessed many diﬀerent strategies to mitigate risks assoc iated with the release\n",
            "of LLMs. One strategy is to freely and openly share trained models (Scao e t al., 2022), and\n",
            "rely on a license \n",
            "\n",
            "Summary: We conduct our resear ch, development, and deployment of NLP and AI systems in accordance with all applicable regul ations. We are particular ly interested in studying whether FinPile, which is cleaner and contains fewer examples of overtly biased language (e.g., Press Releases), reduces the proclivity of t he model to generate inappropriate content. As we move to develop products built on this t echnology, we will apply existing testing procedures, as well as risk and compliance c ontrols, to ensure safe use.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.95:\n",
            " iated with the release\n",
            "of LLMs. One strategy is to freely and openly share trained models (Scao e t al., 2022), and\n",
            "rely on a license that dictates how the model should and should not be u sed. Another\n",
            "requires individuals to apply for access to the trained model parame ters (Zhang et al.,\n",
            "2022a; Touvron et al., 2023). A more restrictive approach is to provide API access to\n",
            "models, but no access to the underlying model parameters or detail ed information on the\n",
            "data the model was trained on (Brown et al., 2020). Finally, some have prov ided no access\n",
            "to the model (Chowdhery et al., 2022; Hoﬀmann et al., 2022). Each decision reﬂe cts a\n",
            "combination of factors, including model use, potential harms, and busi ness decisions.\n",
            "One of Bloomberg’s core business propositions is around providing acce ss to data that\n",
            "has been collected over the course of decades. As is well known, LLMs are susceptible to\n",
            "data leakage attacks and it is possible to extract signiﬁcant segments of te xt given model\n",
            "weights (Carlini et al., 2020, 2022). Moreover, even giving selective acces s to researchers\n",
            "isn’t a guarantee that the model cannot be leaked. Without strong privac y guarantees, we\n",
            "must be concerned that providing access to model weights entails gi ving access to FinPile.\n",
            "For this reason, we err on the side of caution and follow the practice of othe r LLM developers\n",
            "in not releasing our model.\n",
            " \n",
            "\n",
            "Summary: One strategy is to freely and openly share trained models (Scao e t al., 2022), andrely on a license that dictates how the model should and should not be u sed. Anotherrequires individuals to apply for access to the trained model parame ters. A more restrictive approach is to provide API access to models, but no access to underlying model parameters or detail ed information on the data the model was trained on.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.96:\n",
            " FinPile.\n",
            "For this reason, we err on the side of caution and follow the practice of othe r LLM developers\n",
            "in not releasing our model.\n",
            "Nevertheless, our insights and experiences in training and evaluati ngBloombergGPT\n",
            "contribute to the developing understanding of these models. In p articular, our experience\n",
            "may be useful to those building domain-speciﬁc models. During t he process of developing\n",
            "BloombergGPT, we found the OPT chronicles, experiences of the BLOOM team, as\n",
            "well as work of non-open models like GPT-3, PaLM, Chinchilla, and Gopher, to be crucial\n",
            "enablers of our work. In support of this tradition, we include our Traini ng Chronicles\n",
            "(Appendix C).\n",
            "9 Conclusion\n",
            "We have presented BloombergGPT, a best-in-class LLM for ﬁnancial NLP.\n",
            "Our model contributes to the ongoing dialog on eﬀective ways to train d omain-speciﬁc\n",
            "models. Our training strategy of mixing domain-speciﬁc and general-p urpose data results in\n",
            "a model that balances performance in both domains. Additionally, our work oﬀ ers another\n",
            "data point on selecting Chinchilla optimal-sized models. Finally, we hope that our model\n",
            "training logs will provide a guide for those training their own LLMs.\n",
            "We have several interesting directions to pursue. First, task ﬁ ne-tuning has yielded\n",
            "signiﬁcant improvements in LLMs, and we plan to consider what unique op portunities \n",
            "\n",
            "Summary: We have presented BloombergGPT, a best-in-class LLM for ﬁnancial NLP. Our training strategy of mixing domain-speciﬁc and general-p urpose data results in a model that balances performance in both domains. We hope that our model's training logs will provide a guide for those training their own LLMs. We have several interesting directions to pursue.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.97:\n",
            ", task ﬁ ne-tuning has yielded\n",
            "signiﬁcant improvements in LLMs, and we plan to consider what unique op portunities exist\n",
            "38 \n",
            "\n",
            "Summary: summarize:, task ﬁ ne-tuning has yielded                signiﬁcant improvements in LLMs, and we plan to consider what unique op portunities exist                38. Summarize., task  ne-tuned has yieldedipientsigni ﬉cant improvements in LLMs. summarize , task ne tuning has yielded                Signiשּ improvements in LLMs.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Recursive\n",
            "Bart long recurs. level: 2\n",
            "\n",
            "Number of chunks: 28\n",
            "Chunk no.1:\n",
            "The use of NLP in the realm of ﬁnancial technology is broad and complex. We present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of data. We construct a 363 billion token dataset based on Bloombe rg’sextensive data sources. Our mixed dataset train ing leads to a model that outperforms existing models on a variety of tasks. We release Training Chronicles(Appendix C) detailing our experience in training BloombergGPT. Our mixed dataset train ing leads to a model that outperforms existing models on ﬁnancial tasks by signiﬁcan t margins. We expl ain our model-                ing choices, training process, and evaluation methodology. We also detail our training process and Evaluation methodology. The training process is described in the appendix. summarize:.............. 7                2.1.4 Press (9B tokens – 1.21% of training)................. 8                1.5 Bloomberg (5B tokens - 0.70% ofTraining).............. 8                3.1 Architecture (The Pile, Wikipedia, Public Datasets, C4) (184B tokens, 48.73% of Training) summarize: LG]  9 May 20233 Model 11 grotesque3.3 Training Conﬁguration.............................. 13 purposefully3.4 Large-scale Optimization............................. 14 purposefully3,5 Evaluation 16 grotesque5.3 Financial Tasks.................................. 19 grotesque5,3.1 External Financial T tasks........................ 20 grotesque5,.3.2 Heldout Loss................................... 18 progressively5.4 summarize:  External Financial Tasks \n",
            "\n",
            "Summary: We present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of data. We construct a 363 billion token dataset based on Bloombe rg’sextensive data sources. Our mixed dataset train ing leads to a model that outperforms existing models on a variety of tasks. We detail our training process and Evaluation methodology. The training process is described in the appendix.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.2:\n",
            " grotesque5,3.1 External Financial T tasks........................ 20 grotesque5,.3.2 Heldout Loss................................... 18 progressively5.4 summarize:  External Financial Tasks........................ 20 encompasses. Internal Task: Sentiment Analysis........................ 22 encompasses. Exploratory Task: NER........................ 23 encompasses. Big-bench Hard................................. 26 encompasses. Linguistic Tasks.................................. 29 encompasses. Qualitative Samples 31 encompasses. Related Work 32 encompasses. Ethics, Limitations, and Implications 37 encompasses. Ethical Use. summarize:  Work 32 grotesque8 Ethics, Limitations, and Implications 37 grotesque8.1 Ethical Use.................................... 37                8.2 Openness..................................... 38 grotesque9 Conclusion 38 grotesqueA Architecture 61 grotesqueA.0 Notation...................................... 61                A.1 Full Architecture................................. 61 grotesquea.2 SelfAttention with ALiBi (SA)......................... 62 grotesqueaA.3 LayerNorm (LN)................................. 63 grotesquea a.4 FeedForwardNetwork (FFN)................................... 63 grotesquealA.5 List of All Trainable parameters........................... 63 proprietaryA. The release of GPT-3 in 2020 (Brown et al., 2020) demonstrated the powerful beneﬁts of training very large auto-regressive language models (LLMs) GPT3 had 175 b illionparameters, a hundredfold increase over the previous G PT-2 model, and did remarkably well across a wide range of now popular LLM tasks. This performance has been replicated across several other models. After GPT-3, models grew in size to 280 billion (Gopher, Rae et al., 2021), \n",
            "\n",
            "Summary: summarize:  External Financial T tasks........................ 20 grotesque5,.3.2 Heldout Loss................................... 18 progressively5.4. Internal Task: Sentiment Analysis........................ 22 encompasses. Exploratory Task: NER........................ 23 encompasses. Big-bench Hard................................. 26 encompasses. Linguistic Tasks.................................. 29 encompasses. Qualitative Samples 31 encompasses. Ethics, Limitations, and Implications 37 encompasses. Ethical Use.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.3:\n",
            " tasks. This performance has been replicated across several other models. After GPT-3, models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 b il-ion (PaLM, Chowdhery et al. 2022), and 1 trillion parameters (Megatron, Kort hikantiet al., 2022) These e-learning models can perform tasks via few-shot prompting. This ability improve s well-above random as we increase the size of language models. Recent NLP models have almost exclusively focused on general LLMs, trained on datasets that cover a broad range of topics and domains. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. These ﬁndingsmotivate further development of models focused on speciﬃc domains. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain. We train BloombergGPT, a 50 billion parameter language model that supports a wide                range of tasks within the industry. We take a mixed approach to LLM development. General models cover many domains, are able to perform at a high level across a wid e variety of tasks and obviate the need for specialization during training time. Bloomberg is primarily a ﬁnancial data company. We set out to build a model that achieves best- in-class results on ﬂancial benchmarks, while maintaining competitive performanc e on general-purpose LLM benchmarks. We built the largest domain-speciﬁc dataset yet \n",
            "\n",
            "Summary: Recent NLP models have almost exclusively focused on general LLMs, trained on datasets that cover a broad range of topics and domains. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.4:\n",
            " on ﬂancial benchmarks, while maintaining competitive performanc e on general-purpose LLM benchmarks. We built the largest domain-speciﬁc dataset yet, drawing onexisting data creation, collection, and curation resources at Bloomber g. This paper describes an alternative approach to training language models on web-scraped data. The resu lting model does very well on domain-speciﬁc tasks, but also maintains strong performance on ge neral-purposebenchmarks. The paper provides evidence that further develops the community’s understanding of several open questions in the literature. We hope to contribute to the research community. Our training data is u nusualfor LLM training in that it includes a signiﬁcant amount of curated and prep ared data from reliable sources. Early LLMs made a single training pass over a corpus of 200-400 billion to-kens. We provide results on both public NLP benchmarks and internal Bloomberg tasks, which are better aligned with our intended use cases. We train a 50 billion parameter model on 569 billion tokens from our corpus of o ver 700                billion tokens to produce a model that is competitive with larger mo dels. We take a Unigram model instead of greedy merge-based s ub-word-based tokenizers since it saves probabilities allowing for smarter token ization at inference time. GPT-3 and subsequent models were the work of large                teams and required an enormous amount of computation. We train BloombergGPT from a comprehensive dataset of news, press releases, web-scraped documents, and social media drawn from the Bloomberg archiv es. To improve data \n",
            "\n",
            "Summary: This paper describes an alternative approach to training language models on web-scraped data. The resu lting model does very well on domain-speciﬁc tasks, but also maintains strong performance on ge neral-purposebenchmarks. The paper provides evidence that further develops the community’s understanding of several open questions in the literature. We hope to contribute to the research community.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.5:\n",
            " We train BloombergGPT from a comprehensive dataset of news, press releases, web-scraped documents, and social media drawn from the Bloomberg archiv es. To improve data quality, we de-duplicate each dataset according to Lee et al. (2022a) to improve quality. We address each of the above topics in detail to support future training eﬀorts. Summarize:  as a side-eﬀect, the statistics.reported in Table 1 might be diﬄerent from those reported in other pape.rs. The statistics in this section are based on data from the OpenWebText 2.5 DatasetDocs. The data in this article is based on the Open WebText 2Dataset Docs. Summarize:.56 166 2.35%OpenWebText2 1,684 3,850 648 5.07 128 1.80%FreeLaw 349 15,381 537 4.99 108 1.52%StackExchange 1,538 2,201 339 4.17 81 1.15%DM Mathematics 100 8,193 82 1.92 43 0.60%Wikipedia (en) 590 2,988 176 4.65 38 0.53%USPTO Backgrounds 517 4,339 224 6.18 36 0.51%PubMed Abstracts 1,527 1,333 204 5.77 35 0.50%OpenSubtitles 38 31,055 119 4.90 24 0.34%Gutenberg Summarize:  5 3.90 1 0.02%                Wikipedia (7/1/22) 2,218 3,271 \n",
            "\n",
            "Summary: summarize:  We train BloombergGPT from a comprehensive dataset of news, press releases, web-scraped documents, and social media drawn from the Bloomberg archiv es. To improve data quality, we de-duplicate each dataset according to Lee et al. (2022a) to improve quality. We address each of the above topics in detail to support future training eﬀorts.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.6:\n",
            "Gutenberg Summarize:  5 3.90 1 0.02%                Wikipedia (7/1/22) 2,218 3,271 726 3.06 237 3.35%                TOTAL 226,631 1,531 34,701 4.89 7,089 100.00%                Table 1: Breakdown of the full training set used to train BloombergGPT. Bloomberg collects web content by identifying sites that contain ﬁ nancially relevant infor-                mation. While this category makes up the majority of FinPile, its classiﬁcations are rough. The rest of the documents are private and available, amon g other sources, through the Bloomberg Terminal. We clean this data to strip oﬀ markup, specialformatting, and templates. The News category includes all news sources excluding news articles written by Bloomberg journalists. The Company Filings category includes financial statements prepared by (public) companies and made avail-able to the general public. The web crawl is focused on high-quality websites that have ﬁnancially relevant i nformation, as opposed to a general-purpose crawl of the web. The content in this dataset comes from reputable sources of news that are relevant to the financial community. summarize:  (public) companie s and made avail-                able to the general public. In some countries, like the US, public com panies are mandated.Date Bloomberg Filings News Press Web Total 2007 [03-] 276 73 892 523 2,667 4,431 2008 351 91 1,621 628 9 \n",
            "\n",
            "Summary: Bloomberg collects web content by identifying sites that contain ﬁ nancially relevant infor-                mation. The News category includes all news sources excluding news articles written by Bloomberg journalists. The Company Filings category includes financial statements prepared by (public) companies and made avail-able to the general public. The web crawl is focused on high-quality websites that have ﬃnancially relevant i nformation.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.7:\n",
            "Date Bloomberg Filings News Press Web Total 2007 [03-] 276 73 892 523 2,667 4,431 2008 351 91 1,621 628 9,003 11,695. 1.1 Press (9B tokens – 1.21% of training)1.2 Bloomberg (5B tokens - 0.70% ofTraining)1:3 SEC (1.90% of total) and 1:4 Press (0.70%) The Press category contains press releases typically issued by companies that are ﬁnanciallyrelevant. The Bloomberg category comprises Bloomberg authored news and other documents. This category comprises Bloomberg authored news and other documents s uch as opinions and analyses. The largest sources are “Bloomberg News’ (0.44% of total) and “Bloom berg berg First Word’s’ 0.13%. We use three widely known and available public datasets in our traini ng corpus. The Pile (184B tokens – 25.9% of training) is the dataset used in GPT-Neo (Black et al., 2021) The Pile and C4 include out-of-date copies of Wikipedia. The Colossal Clean Crawled Corpus (C4) is a common dataset used to train LLMs. We feel that incl uding C4 in addition to The Pile can add value more than duplicated documents would. For example, domains such asFreeLaw and GitHub are useful to teams at Bloomberg that work on legal docum ents and software development. We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer. The dataset is toke n \n",
            "\n",
            "Summary: We use three widely known and available public datasets in our traini ng corpus. We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer. The Pile (184B tokens – 25.9% of training) is the dataset used in GPT-Neo. The Colossal Clean Crawled Corpus (C4) is a common dataset used to train LLMs.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.8:\n",
            ". We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer. The dataset is toke nized quite pleasantly (3.06 characters per token), indicating an above-average amount of markup. The Pile and C4 include out-of-date copies of Wikipedia. We include a dump of English Wikipedia from July 1, 2022. The tokenizer uses BLOOM, NeoX, OPT (GPT 2), and BloombergGPT tokenizers. The Pile dataset is used to train the tokenizer. The Unigram tokenizer implementation is too ineﬃcient to process the entire Pile datasets at once. The pretokenization is based on GPT-2, but includes spaces in the alphabetic chunks, which allows multi-word tokens to be learned. The Unigram tokenizer implementation is too ineﬃcient to process the entire Pile dataset at once. We spliteach of the 22 domains in the Pile into 256 chunks of roughly equal size. To ensure we do not need an out-of-vocabular y token, we also add as tokens the 36 (of 256 possible) bytes that do not occur in The Pile. We choose a vocabulary size of 217tokens based on experiments with vocabulary ranging from 25,000 to 550,000. We choose the vocabulary size that leads to the smallest encoded representation of C4. Our tokenizer is large, relative to the standard vocabulary s ize of approximately50,000 tokens. For an analysis of tokenization eﬃciency, see Table 3. The model is a decoder-only causal language \n",
            "\n",
            "Summary: We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer. The dataset is toke nized quite pleasantly (3.06 characters per token), indicating an above-average amount of markup. We choose a vocabulary size of 217tokens based on experiments with vocabulary ranging from 25,000 to 550,000.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.9:\n",
            "ize of approximately50,000 tokens. For an analysis of tokenization eﬃciency, see Table 3. The model is a decoder-only causal language model based on BLOOM (Scao et al., 2022). We present an overview of the architecture, with full details in Ap pendix A. The model contains 70 layers of transformer decoder blocks deﬁned as follows. The inp ut token embeddings are tied to the linear mapping before the softmax. The model has an additional layer of normalization after token embeddings, formally:¯¯h1= LNem(h0) + SA(LN(Lnem(H0) - LNemis(H1) H0 is the initial token embedding and LN Artemis is the new component of embedding layer-normalization. Notice that the second term includes two consecutive l ayer-normalizations. The size of our model is based on Chinchilla scaling laws (Hoﬀmann et al., 2022), in particular their Approach 1 and Approach 2. We start with a total compute budge t of 1.3M GPUs. We adopt activation checkpointing to reduc e ourmemory footprint. This costs us an additional 0.33x TFLOPs per iteration du e to repeatedforward passes. The dataset of /tildelow700B tokens is too small for a “Chinchillaoptimal’ conﬁguration given our compute budget. The scaling law derived by Chinchilla is tokenizer-speci ﬁc. Our tokenizer can encode the same document more compactly due to the support of multi-word expressions \n",
            "\n",
            "Summary: The model is a decoder-only causal language model based on BLOOM. It contains 70 layers of transformer decoder blocks deﬁned as follows. The size of our model is based on Chinchilla scaling laws. We adopt activation checkpointing to reduc e ourmemory footprint. This costs us an additional 0.33x TFLOPs per iteration du e to repeatedforward passes.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.10:\n",
            " by Chinchilla is tokenizer-speci ﬁc. Our tokenizer can encode the same document more compactly due to the support of multi-word expressions and t he larger vocabulary size. We choose the largest model that we can to ensure we can train on all our tokens and still leave /tildelow30% of the total compute budget as a buﬀer for unforeseen failures, retries, and restarts. This leads us to a 50B parameter model, which is roughly the Chinchilla optimal size for our compute budget. We want the dimensions t o be multiples of 8 to achieve higher performance in Tensor Core operations. BloombergGPT is a PyTorch model trained with a standard left-to-right language modeling objective. We use the AdamW optimizer (Loshchilov and Hutter, 2019) for optimization. We set tle on 40 heads, each having a dimension of 192, resulting in a total hidden dimen sion ofD= 7680 and a total of 50.6B parameters. We set dropout to 0.0 in all layers in our initial run, although we add dropou t later. The model parameters are randomly initialized to samples from a normal distribution. We rescale the standard de viationof the second layer in the MLP and the output layer of the attention by 1 /√                2L. We use the technique of querykeylayerscaling (Shoeybi et al., 2019) We use the Amazon SageMaker service provided by AWS to train andevaluate BloombergGPT. We use the latest version available at the time of training andtrain on a total of 64 \n",
            "\n",
            "Summary: Chinchilla tokenizer can encode the same document more compactly due to the support of multi-word expressions. We choose the largest model that we can to ensure we can train on all our tokens and still leave /tildelow30% of the total compute budget as a buﬀer for unforeseen failures, retries, and restarts. This leads us to a 50B parameter model, which is roughly the Chinchilla optimal size for our compute budget.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.11:\n",
            " use the Amazon SageMaker service provided by AWS to train andevaluate BloombergGPT. We use the latest version available at the time of training andtrain on a total of 64 p4d.24xlarge instances. We train on a single instance of Bloomberg GPT, which we use to evaluate the model. We also train on the same instance of GPT that we evaluate the Bloomberg model. We train BloombergGPT on a total of 64 p4d.24xlarge instances. Each instance has 8 NVIDIA40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) and NVIDIAGPUDirect using AWS Elastic Fabric Adapter (EFA) inter-node connec tions (400 Gb/s). We achieve                102 TFLOPs on average and each training step takes 32.5 seconds. We shard a model acros s 128 GPUs and have 4 copies of the model during training. MiCS includes such feat ures as hierarchical commu-nication, 2-hop gradient update, scale-aware model partitioning. We apply activation checkpointing to each transformer layer to reduce training memory consumption. We have 4 versions of MiCS available for training. We shard the model across a group of GPUs. To reduce the memory requirements, forward and backward passes are done in BF16, while parameters are stored and updated in full pr ecision (FP32) We al so use FP32to calculate fused softmax in the Attention block and store its resul ts inBF16. Another possibility for optimization is combining composition of several operations into a single GPU operation. This can both reduce peak memory u sage by avoiding storage of intermediate results in the computation graph, \n",
            "\n",
            "Summary: We train BloombergGPT on a total of 64 p4d.24xlarge instances. Each instance has 8 NVIDIA40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) We achieve                102 TFLOPs on average and each training step takes 32.5 seconds. We shard a model acros s 128 GPUs and have 4 copies of the model during training.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.12:\n",
            " Another possibility for optimization is combining composition of several operations into a single GPU operation. This can both reduce peak memory u sage by avoiding storage of intermediate results in the computation graph, as wel l as help improve speed. Figure 2 shows the learning curves for both training and validation sets. The solid lines show (smoothed) tr aining loss and thedotted lines show loss on the held-out validation set. Changes in the col or of the lines15indicate changes to the optimization hyperparameter conﬁgurations, eit her as scheduled, or in response to increasing or stagnating validation loss. We trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after /tILDelow80% of one epoch. We ended training early because the loss on our held -out development set was no longer improving. It is possible that substantially l onger training may have yielded further improvements. We then applied the following correc tive modiﬁcations in the training sequence. We evaluated the performance of BloombergGPT on two broad categories of tasks: ﬁnance-speciﬁc and general purpose. The general purpose tasks investigate whether the. performance of our model is directly comparable to previously published results. For ﬃnancial tasks, we assembled pub-licly available NLP datasets that include a range of NLP tasks. summarize: Reading Comprehension, and Linguistic Tasks. The number of tasks per t ype and the                deﬁnitions of the groups are presented in Table 5. We evaluate BloombergGPT on a high-coverage set of \n",
            "\n",
            "Summary: Figure 2 shows the learning curves for both training and validation sets. The solid lines show (smoothed) tr aining loss and thedotted lines show loss on the held-out validation set. Changes in the col or of the lines15indicate changes to the optimization hyperparameter conﬁgurations. We trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after /tILDelow80% of one epoch.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.13:\n",
            " and the                deﬁnitions of the groups are presented in Table 5. We evaluate BloombergGPT on a high-coverage set of standard benchmarks that assess downstream performance, taken from HE LM, SuperGLUE, MMLU, and GPT-3. We compare BloombergGPT to the three closest models described in §7 based on model size, type of training data, overall performance, and most import antly, access. An overview of the model sizes and compute is provided in Table 6. We report GPT-3 res ultswhenever available but did not run it ourselves due to lack of availabi lity. We additionally report results from the original GPT. summarize:  of the same general-purpose datasets we use in our training cor-                pus. We additionally report results from the original GPT-3 (Brown et al., 2020) whenever they are externally available. For each group of results, we further present a win rate similar to Liang et al. (2022) th at represents the percentage of “wins” in side-by-side comparisons over individual tasks. We consider three methods for classiﬁcat ion: regular, cali-bration, and normalization. We report the performance of the best method for each model and task. For other tasks, we perform generation via greedy decoding. We evaluate the bits per byte of the diﬀere nt data. We use the oﬃcial split and report performance on the test set wheneve r possible. We evaluate the bits per byte of the diﬀere nt models on a heldout. \n",
            "\n",
            "Summary: We evaluate BloombergGPT on a high-coverage set of standard benchmarks that assess downstream performance, taken from HE LM, SuperGLUE, MMLU, and GPT-3. We additionally report results from the original GPT. We consider three methods for classiﬁcat ion: regular, cali-bration, and normalization. We report the performance of the best method for each model and task.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.14:\n",
            "cial split and report performance on the test set wheneve r possible. We evaluate the bits per byte of the diﬀere nt models on a heldout.dataset that contains examples from all sections of FinPile. The set of documents is held out in time and deduplicat ed with the training set. During evaluation, for docume nts that are longer than 2,048 tokens, we use a sliding window approach. BloombergGPT consistently outperforms other models. The gap to BloombergGPT is most signiﬁcant in the Filings category. We report the loss breakdown by the type of document in FinPile. We use a combination of p ublic and internal benchmarks to assess the performance of BloombergG PT, BLOOM 176B, GPT-Ne and BLOom 176B. Summarize:  of p ublic and internalbenchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, andOPT66B. All task types considered and their corresponding prompt templates are shown. Table 7: Template for the diﬀerent tasks we evaluate in the ﬁnancial domai n. The Financial Phrasebank Dataset includes a sentimen t                classiﬁcation task on sentences from ﬁnancial news. We create our own own ownsplits and report F1 score weighted by support in a 5-shot setup. While non-LLM numbers of custom m odels are available, we omit reporting them here due to diﬀere nces in the evaluation setup. Summarize:  \n",
            "\n",
            "Summary: summarize: cial split and report performance on the test set wheneve r possible. We evaluate the bits per byte of the diﬀere nt models on a heldout.dataset that contains examples from all sections of FinPile. The Financial Phrasebank Dataset includes a sentimen t                classiﬁcation task on sentences from ﬁnancial news.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.15:\n",
            "LLM numbers of custom m odels are available, we omit reporting them here due to diﬀere nces in the evaluation setup. Summarize:  we systematicallydiscretize the data into a classiﬁcation setup with negative, neut ral, and positive Classes. Like with FPB, we create our own splits including microbl ogs and news, and use a 5-shot setup, reporting weighted F1. Table 8: Results on ﬁnancial domain tasks. The results are based on a dataset of English news headlines about “gold” Summarize each tag into a question using the oﬃcial docu mentation and report the average weighted F1 score across all categories. All the models required more shots to perform well and we thus selected 20 shots and reported the entity-level F1 scores. The annotated entity types follow the standard CoNLL format and are annotated with PER, LOC, ORG, and MISC. BloombergGPT performs best of all models for four of the ﬁve tasks (ConvFinQA, FiQA SA, FPB, and Headline) and comes in second in NER (Table 8). Consequen tly, BloombergGPT also has the highest win rate among all the models that we tested. The gap to equally-sized models is especially pronounced for Conv FinQA w hich is challenging due to the requirement to use conversational input to reason over tab les and generate ananswer. For the Bloomberg-internal tasks, we consider aspect-speciﬁc sent iment analysis. All of the datasets we use are in Engli sh. \n",
            "\n",
            "Summary: BloombergGPT performs best of all models for four of the ﬁve tasks (ConvFinQA, FiQA SA, FPB, and Headline) BloombergGPT also has the highest win rate among all the models that we tested. The gap to equally-sized models is especially pronounced for Conv FinQA w hich is challenging due to the requirement to use conversational input to reason over tab les.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.16:\n",
            ". For the Bloomberg-internal tasks, we consider aspect-speciﬁc sent iment analysis. All of the datasets we use are in Engli sh. The datasets in this section were annotated by 2 annotat ors with a third annotator breaking any ties. We measure the performance of LLMs for the internal datasets using a ﬁve- shot evaluation, similar to the external datasets. The dataset consists of Engl ish news stories from Bloomberg, premium, and web content. The task is to predict the aspect-speciﬁc sentiment ex-pressed in the news story toward a company. Annotations of “positiv e”, “neg-reprehensible” or “neutral” indicate that the story is likely to increase or decrease the long-term investor con-dence in the company. BloombergGPT GPT-NeoX OPT 66B BLOOM 176BEquity News 79.63 14.17 20.98 19.96Equity Social Media 72.40 66.48 71.36 68.04Equity Transcript 65.06 25.08 37.58 34.82ES News 46.12 26.99 31.44 28.07Country News 49.14 13.45 17.41 16.06 BloombergGPT performs better than all the other tested models, by a wide margin. The only task in which the models perform similarly is the social media sentiment task. We report average number of LOCation, ORGanization, PERson, and PERson per example.summarize: ow12 500 0.4 1.4 0.2 0.1 0.0 0. Bloomberg \n",
            "\n",
            "Summary: BloombergGPT performs better than all the other tested models, by a wide margin. The only task in which the models perform similarly is the social media sentiment task. We report average number of LOCation, ORGanization, PERson, and PERson per example. We measure the performance of LLMs for the internal datasets using a ﬁve- shot evaluation, similar to the external datasets.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.17:\n",
            ", PERson, and PERson per example.summarize: ow12 500 0.4 1.4 0.2 0.1 0.0 0. BloombergGPT outperforms the other models by at least 25 and up to over 60 points in the other three. T he only task in which the models perform similarly is the social media sentiment task. NER is an information extraction task, and a better for encoder-decod er or encoder onlyarchitectures. The generative nature of LLMs does not confer an advantage for NER. For example, consider the (fabricated) headline “Bloomberg: Mr. Musk adds new fea-                tures to Twitter and comments on China”. Depending on our annotation guide lines and downstream task needs, “China’s” can be tagged ORG or LOC, though the right tag is likely ORG. Without adding e xtensive Guidelines in the prompt, the LLM does not know the intended tagging behavior. Summarize:  all documents that contain no entities (i.e., all “O”’s ). Both of these modi-ﬁcations are intended to increase the usefulness of the examples se en in few-shot prompting. We expect that further work on prompt engineering for NER could produc e better results. We consider seven Bloomberg internal NER datasets from diﬀerent domai ns. The dataset contains transcripts from 2019. The goal of this task is to identify entities that occur in transcrip tsof company press conferences.summarize:                  Transcripts NER : The goal to identify. entities \n",
            "\n",
            "Summary: BloombergGPT outperforms the other models by at least 25 and up to over 60 points in the other three. The only task in which the models perform similarly is the social media sentiment task. NER is an information extraction task, and a better for encoder-decod er or encoder onlyarchitectures. The generative nature of LLMs does not confer an advantage for NER.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.18:\n",
            " is to identify entities that occur in transcrip tsof company press conferences.summarize:                  Transcripts NER : The goal to identify. entities that occurred in transcrips of company press. conferences. The dataset contains. transcripts from2019.BloombergGPT GPT-NeoX OPT 66B BLOOM 176BNER. We randomly sample 4,000 training and 500 test ing ex-amples from each internal dataset. We utilize 20-shot prompts an d evaluate using F1. The results from the internal NER tasks are mixed (Table 12). The muc h larger                BLOOM 176Bwins most of the N ER tasks. On NER+NED, BloombergGPT outperforms all other models by a large margin. We test the ability of an LLM to complete this task by evaluatin g a jointNER+NED task: identify the stock tickers of companies mentioned in a document. The task requires the model to identify company mentions and then gene rate the corresponding stock ticker. While NER evaluation requires exact matches, ticker s may be successfully produced without ﬁrst identifying spans. Table 12 shows that BloombergGPT outperforms all other models by a large margin, except on social media data where it comes in second behind BLOOM 176B. While the focus of our model is on financial tasks, our inclusion of general-purp ose training data may help improve not only the ﬁnancial tasks, but also allow our model t o perform well on more standard NLP datasets. BloombergGPT falls behind PaLM 540B and BLOOM 176B, but is the best-performing \n",
            "\n",
            "Summary: BloombergGPT GPT-NeoX OPT 66B BLOOM 176BNER. We randomly sample 4,000 training and 500 test ing ex-amples from each internal dataset. The results from the internal NER tasks are mixed (Table 12). The muc h larger                BLOOM176Bwins most of the N ER tasks. On NER+NED, BloombergGPT outperforms all other models by a large margin.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.19:\n",
            " our model t o perform well on more standard NLP datasets. BloombergGPT falls behind PaLM 540B and BLOOM 176B, but is the best-performing among similarly sized models. It achieves the best perfor-                mance of all models in date understanding, hyperbaton (ordering of adjec tives), and tracking grotesqueshuﬄed objects. We next assess knowledge, which we deﬁne as the ability to recall in formation seen during model training. summarize:  3rd to 9th gradescience exams, includes easy and challenging splits. 26BIG-bench Hard Task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B PaLM 540BBoolean Expressionsλ62.40 71.20 48.40 69.20 83.2. Causal Judgement 49.73 52.41 51.87 51.80 53.6. Summarize:  Penguins in a Table 37.67 33.56 28.08 40.41 44.5. Reasoning about Colored Objects 34.80 26.00 31.20 36.80 38.0Ruin Names 56.00 54.00 52.80 54.80 76.0.Salient Translation Error Detection 20.00 20.40 16.40 23.60 48.8. Snarks 69.36 69.66 72.47 78.1. BloombergGPT consistently outperforms models of similar size while almost being on par with themuch larger models. The Massive Multitask Language Understanding (MMLU) covers 57 diﬀerent subjects and thus has a much wider cove rage than \n",
            "\n",
            "Summary: BloombergGPT falls behind PaLM 540B and BLOOM 176B, but is the best-performing among similarly sized models. It achieves the best perfor-                mance of all models in date understanding, hyperbaton (ordering of adjec tives), and tracking grotesqueshuﬄed objects. BloombergGPT consistently outperforms models of similar size while almost being on par with themuch larger models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.20:\n",
            " with themuch larger models. The Massive Multitask Language Understanding (MMLU) covers 57 diﬀerent subjects and thus has a much wider cove rage than the tasks described above. BloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, andOPT66B. BloombergGPT achieves the highest win rate among the models we ran ourselves. GPT-3 performs second best on average. BloombergGPT outperformsOPT66B, which in turn outperforms GPT -NeoX, while G PT-3 is the best. The baseline numbers from GP T-3 are taken from Brown et. al. (2020) Among all models, BloombergG PT achieves thehighest win rate. BloombergGPT lacks behind BLOOM 176Bon three of the categories, but its aver-ishlyage is the highest among all models. The gap to GPT-3 is closest in the STEM and ‘Other’ domains. BloombergGPT also outperforms BLOom 176Bin this category, although by a slim margin. It falls behind the reported performance of GP T-3, especially in the social science category. BloombergGPT far outclasses the models weevaluated ourselves, and is slightly behind GPT-3. Table 16: Reading Comprehension Results (1-shot). The baseline numbe rs are taken from Brown et al. (2020) BloombergGPT-NeoX OPT 66B BLOOM 176B GPT -3 BoolQ 74.59 46.36 57.46 52.94 76.7 OpenBookQA 51.60 44.20 58.00 47.2058.8 R \n",
            "\n",
            "Summary: The Massive Multitask Language Understanding (MMLU) covers 57 diﬀerent subjects. BloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, andOPT66B. G PT-3 performs second best on average, while OpenBookQA is the best. The gap to GPT -3 is closest in the STEM and 'Other' domains.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.21:\n",
            " 74.59 46.36 57.46 52.94 76.7 OpenBookQA 51.60 44.20 58.00 47.2058.8 RACE (middle) 54.32 41.23 47.42 52.30 57.4RACE (high) 41.74 34.33 37.02 39.14 45.9ReCoRD 82.79 67.86 82. Table 16 reﬁne as linguistic tasks those scenarios that are not directly conne cted to user-facing applications. These include tasks that evaluate disambiguation, grammar, or entailment. The list of t asks is as follows:Recognizing Textual Entailment, Multi-Sentence Reading Comprehension, Shortparagraphs and multi-sentence questions. Summarize: Given two text fragments, i dentify whether the meaning of one text is entailed. AnLI: Adversarially constructed entailmentdetection. CB: Naturally occurring discourses whose sentence contains a clause-embedding predicate. WIC: Determine if a word is being used with the same meaning in two sentences. Bloomberg: Linguistic Scenario Bloomberg. BloombergGPT consistently scores highestamong the models we evaluate, achieving an 85% win rate. Table 17: Results on the Linguistic Scenarios (1-shot). The baseline num bers from GPT- autoimmune3 are taken from Brown et al. (2020) Win rates and averages are computed only based on accuracy numbers. BloombergGPT is the most accurate of the models. The results (Table 17) for linguistic tasks follow a similar trend to t he knowledge category. BloombergGPT falls slightly behind GPT \n",
            "\n",
            "Summary: Summarize:  74.59 46.36 57.46 52.94 76.7 OpenBookQA 51.60 44.20 58.00 47.2058.8 RACE (middle) 54.32 41.23 47.42 52.30 57.4RACE (high) 41.74 34.33 37.02 39.14 45.9ReCoRD 82.79 67.86 82.9 82.7 BloombergGPT is the most accurate of the models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.22:\n",
            " is the most accurate of the models. The results (Table 17) for linguistic tasks follow a similar trend to t he knowledge category. BloombergGPT falls slightly behind GPT-3 and outperforms the other models. Simi larto the reading comprehension category, BLOOM 176B falls behind BloombergG PT. The model has still attained abilities on general-purpose data that exceed similarly sized models. In some cases it is competitive or exceeds the performance of much larger models. The model can utilize its knowledge about stock tickers and ﬁnancial terms to compose valid queries to ret rieve the data, given a request in natural language. One use case for BloombergGPT is to make interactions with financial data more natural. In each case, the model is given 3 examples (not shown) followed by the ‘Input’ and a prompt of “Output: ”. BloombergGPT can be utilized to make interactions with financial data more natural. It can be used for many news applications and to assist journalists in their day-to-day work. BloombergGPT-NeoX does not, and FLAN-T5-XXL completely fails, consistently ignoring the US housing market. The Bloomberg Query Language (BQL) is an incredibly powerful but complex tool. The US housing market shrank in value by $2.3 trillion, or 4.9%, in the secondhalf of 2022. That’s the largest drop in percentage terms since the 2008 housing crisis. Google was sued by the US and eight states seeking the breakup of its digital advertising business for allegedly monopolizing the digital advertising m arket. The DOJ has sought to cleave up a major company for the first time since \n",
            "\n",
            "Summary: BloombergGPT can be utilized to make interactions with financial data more natural. It can be used for many news applications and to assist journalists in their day-to-day work. The US housing market shrank in value by $2.3 trillion, or 4.9%, in the secondhalf of 2022. That’s the largest drop in percentage terms since the 2008 housing crisis.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.23:\n",
            " seeking the breakup of its digital advertising business for allegedly monopolizing the digital advertising m arket. The DOJ has sought to cleave up a major company for the first time since 1982. BloombergGPT could help with the editing process by suggesting initial headlines from the text. It could also predict the CEO at Cirrus Logic who was included in the prompt. We were not able to find any example where the other models solved the task whil e BloombergGPT did not.7 Related Work                Language Models. Language modeling has a long history in the NLP community.  models that could better approximate the distribution of                language over large corpora led to the discovery that the representation s these modelsproduce are useful starting points for many downstream tasks. This w as demonstrated by Radford et al. (2018) and Howard and Ruder ( 2018) who showed that generative pretr aining can be used to predict the performance of large companies. The study was published in the open-source journal, Theoretical Computer Science. BloombergGPT, GPT-NeoX, and FLAN-T5-XXL are tested. Each model is run in a 10-shot setting. We sample up to three answers and present all of them if they are incorr ect. Michael Corbat was CEO of Citigroup until 2021, highlighting the importance of an up-to-date model. Since the rel ease of G PT-3 by Brownet al. (2020), many other researchers built large language models to study dat a quantity. The value of domain-speciﬁc training for language models is well established. Commonly ac cepted approaches are to train \n",
            "\n",
            "Summary: BloombergGPT could help with the editing process by suggesting initial headlines from the text. It could also predict the CEO at Cirrus Logic who was included in the prompt. We were not able to find any example where the other models solved the task whil e BloombergGPT did not. The study was published in the open-source journal, Theoretical Computer Science. The value of domain-speciﬁc training for language models is well established.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.24:\n",
            " to study dat a quantity. The value of domain-speciﬁc training for language models is well established. Commonly ac cepted approaches are to train BERT models from scratch or to continue pretraining an existing model on new domain- Speci ﬁC data. Following these strategies, BioBERT (Lee et al., 2020) adapts BERT to th e biomed-                ical domain and SciBERT is trained on scientiﬀc publications. In-domain training allows model s to outperform state-of-the-art models in biomedical text mining tas ks. The training of auto-regressive—decoder-only—language models of m ore than 10Bparameters is signiﬁcantly more costly than training masked LMs under 1B parameters. One popular domain is protein sequences s ince they can berepresented using language-like sequences. Galactica is trained exclusively on a large collection of scientiﬁc datasets. It includes special proces sing to handle Scientology-relatednotations. Galactica also surprisingly also performs well on more standard NLP tasks. We are inspired by the general capabilit ies of GalActica, we want to augment our private data with public data with the goal of making NLP more accessible. Large corpora of raw text data are critical for training LLMs. There are now several corpora available that cover a wide range of sources. Various eﬀorts aim to clean datasets, especially web data, by removing harmful text. These datasets are built on or inc ludely augmented with an array of data from high-qu ality sources \n",
            "\n",
            "Summary: In-domain training allows model s to outperform state-of-the-art models in biomedical text mining tas ks. Large corpora of raw text data are critical for training LLMs. Galactica is trained exclusively on a large collection of scientiﬁc datasets. We want to augment our private data with public data with the goal of making NLP more accessible.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.25:\n",
            " clean datasets, especially web data, by removing harmful text. These datasets are built on or inc ludely augmented with an array of data from high-qu ality sources. The aim is to investigate whether a model can gain in-domain capabilities without sacriﬁcing general-domain perfor mance. The tasks addressed by language models have vastly increased and requi re a very diﬀerent evaluation process from traditional task-speciﬁcsys tems. Bloomberg’s core business curates and provides access to datasets, which we use to construct a high-quality dataset to trainBloombergGPT, resulting in best-in-class ﬁnancial performance. There have been two strategies for LLM evaluation. The first is to evaluate a model in many diﬀere nt scenarios via automatic evaluation (Liang et al., 2022; Srivastava et al. 2022) and the second is to integrate models into user work. In our case, we combine multiple general-purpose evaluations from multiple existing benc hmarks that have di ﬀerent goals. There exists no standard benchmark for the ﬁnancial NLP domain. Large language model training remains expensive. Determ ining theoptimal amount of training data and model shape and size for the best utili zation of re-estylesources is important. We developed a few-shot strategy for FLUE, but also decided to augment the publicly a vailable evaluation with company-internal benchmarks. Kaplan et al. (2020) studied the dependence of language model performan ce onarchitecture, parameter size, compute power, and dataset size. A similar investigation by Hernandez et. \n",
            "\n",
            "Summary: The aim is to investigate whether a model can gain in-domain capabilities without sacriﬁcing general-domain perfor mance. Bloomberg’s core business curates and provides access to datasets, which we use to construct a high-quality dataset to trainBloombergGPT. There exists no standard benchmark for the ﬁnancial NLP domain. Large language model training remains expensive.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.26:\n",
            ". (2020) studied the dependence of language model performan ce onarchitecture, parameter size, compute power, and dataset size. A similar investigation by Hernandez et.al. (2021) into data transfer f or diﬀering dis-                tributions found that this also follows a power law. Tay and Rae (2022a) further demonstrated the importance of architecturechoice when scaling. The study of Hoﬀmann et al. (2022) posited that existing large l anguage models were undertrained. They demonstrated this hypothesis through Chinchilla, a model sign iﬁcantly smaller, yet higher performing, than most of the largest LLMs. These findings opened the door for “Chinchilla optimal” training of smaller models. The Unigram tokenizer learns a top-down vocabulary by initializing a large vocabulary and repeatedl y discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. Beltagy et al. (2019) constr ucted a biomedical vo-                cabulary to scientiﬁc text. We train our own unigram tok enizer instead of relying on existing public ones. Transformer-based models rely on positional embeddings to code position and location information of words in a text. A dedicated biomedical vo-                cabulary improved performance on sequence labeling tasks consisten tly. The se ﬁndings demonstrate the importance of selecting a tokenizer and accompanying vocabulary. The rapid development and adoption of large language models have been accompanied by a \n",
            "\n",
            "Summary: The rapid development and adoption of large language models has led to the development of new models. The Unigram tokenizer learns a top-down vocabulary by initializing a large vocabulary and repeatedl y discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. A dedicated biomedical vo-                cabulary improved performance on sequence labeling tasks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.27:\n",
            "ly. The se ﬁndings demonstrate the importance of selecting a tokenizer and accompanying vocabulary. The rapid development and adoption of large language models have been accompanied by a rigorous conversation about the ethics, uses, and limitations of these m odels. We discuss issues that are directly relevant to the development of BloombergGPT. We conduct our resear ch, development, and deployment of NLP and AI systems in accordance with ethical guidelines. We conclude by discussing the implications of our findings for the financial industry. We conduct our resear ch, development, and deployment of NLP and AI systems in accordance with all applicable regul ations. We are particular ly interested in studying whether FinPile, which is cleaner and contains fewer examples of overtly biased language (e.g., Press Releases), reduces the proclivity of t he model to generate inappropriate content. As we move to develop products built on this t echnology, we will apply existing testing procedures, as well as risk and compliance c ontrols, to ensure safe use. One strategy is to freely and openly share trained models (Scao e t al., 2022), andrely on a license that dictates how the model should and should not be u sed. Anotherrequires individuals to apply for access to the trained model parame ters. A more restrictive approach is to provide API access to models, but no access to underlying model parameters or detail ed information on the data the model was trained on. We have presented BloombergGPT, a best-in-class LLM for ﬁnancial NLP. Our training strategy of mixing domain-speciﬁc and general-p urpose data results in a model that balances performance in both domains. We hope \n",
            "\n",
            "Summary: The rapid development and adoption of large language models have been accompanied by a rigorous conversation about the ethics, uses, and limitations of these m odels. We discuss issues that are directly relevant to the development of BloombergGPT. We conduct our resear ch, development, and deployment of NLP and AI systems in accordance with ethical guidelines. We conclude by discussing the implications of our findings for the financial industry.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.28:\n",
            "LP. Our training strategy of mixing domain-speciﬁc and general-p urpose data results in a model that balances performance in both domains. We hope that our model's training logs will provide a guide for those training their own LLMs. We have several interesting directions to pursue. summarize:, task ﬁ ne-tuning has yielded                signiﬁcant improvements in LLMs, and we plan to consider what unique op portunities exist                38. Summarize., task  ne-tuned has yieldedipientsigni ﬉cant improvements in LLMs. summarize , task ne tuning has yielded                Signiשּ improvements in LLMs. \n",
            "\n",
            "Summary: Our training strategy of mixing domain-speciﬁc and general-p urpose data results in a model that balances performance in both domains. We hope that our model's training logs will provide a guide for those training their own LLMs. We plan to consider what unique op portunities exist in the LLMs we are training. We have several interesting directions to pursue.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Recursive\n",
            "Bart long recurs. level: 3\n",
            "\n",
            "Number of chunks: 8\n",
            "Chunk no.1:\n",
            "We present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of data. We construct a 363 billion token dataset based on Bloombe rg’sextensive data sources. Our mixed dataset train ing leads to a model that outperforms existing models on a variety of tasks. We detail our training process and Evaluation methodology. The training process is described in the appendix. summarize:  External Financial T tasks........................ 20 grotesque5,.3.2 Heldout Loss................................... 18 progressively5.4. Internal Task: Sentiment Analysis........................ 22 encompasses. Exploratory Task: NER........................ 23 encompasses. Big-bench Hard................................. 26 encompasses. Linguistic Tasks.................................. 29 encompasses. Qualitative Samples 31 encompasses. Ethics, Limitations, and Implications 37 encompasses. Ethical Use. Recent NLP models have almost exclusively focused on general LLMs, trained on datasets that cover a broad range of topics and domains. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain. This paper describes an alternative approach to training language models on web-scraped data. The resu lting model does very well on domain-speciﬁc tasks, but also maintains strong performance on ge neral-purposebenchmarks. The paper provides evidence that further develops the community’s understanding of several open questions in the literature. We hope to contribute to the research community. summarize:  We train BloombergGPT from \n",
            "\n",
            "Summary: BloombergGPT is a 50 billion parameter language model that is trained on a wide range of data. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.2:\n",
            " that further develops the community’s understanding of several open questions in the literature. We hope to contribute to the research community. summarize:  We train BloombergGPT from a comprehensive dataset of news, press releases, web-scraped documents, and social media drawn from the Bloomberg archiv es. To improve data quality, we de-duplicate each dataset according to Lee et al. (2022a) to improve quality. We address each of the above topics in detail to support future training eﬀorts. Bloomberg collects web content by identifying sites that contain ﬁ nancially relevant infor-                mation. The News category includes all news sources excluding news articles written by Bloomberg journalists. The Company Filings category includes financial statements prepared by (public) companies and made avail-able to the general public. The web crawl is focused on high-quality websites that have ﬃnancially relevant i nformation. We use three widely known and available public datasets in our traini ng corpus. We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer. The Pile (184B tokens – 25.9% of training) is the dataset used in GPT-Neo. The Colossal Clean Crawled Corpus (C4) is a common dataset used to train LLMs. We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer. The dataset is toke nized quite pleasantly (3.06 characters per token), indicating an above-average amount of markup. We choose a vocabulary size of 217tokens \n",
            "\n",
            "Summary: We train BloombergGPT from a comprehensive dataset of news, press releases, web-scraped documents, and social media drawn from the Bloomberg archiv es. To improve data quality, we de-duplicate each dataset according to Lee et al. (2022a) to improve quality. We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.3:\n",
            " dataset is toke nized quite pleasantly (3.06 characters per token), indicating an above-average amount of markup. We choose a vocabulary size of 217tokens based on experiments with vocabulary ranging from 25,000 to 550,000. The model is a decoder-only causal language model based on BLOOM. It contains 70 layers of transformer decoder blocks deﬁned as follows. The size of our model is based on Chinchilla scaling laws. We adopt activation checkpointing to reduc e ourmemory footprint. This costs us an additional 0.33x TFLOPs per iteration du e to repeatedforward passes. Chinchilla tokenizer can encode the same document more compactly due to the support of multi-word expressions. We choose the largest model that we can to ensure we can train on all our tokens and still leave /tildelow30% of the total compute budget as a buﬀer for unforeseen failures, retries, and restarts. This leads us to a 50B parameter model, which is roughly the Chinchilla optimal size for our compute budget. We train BloombergGPT on a total of 64 p4d.24xlarge instances. Each instance has 8 NVIDIA40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) We achieve                102 TFLOPs on average and each training step takes 32.5 seconds. We shard a model acros s 128 GPUs and have 4 copies of the model during training. Figure 2 shows the learning curves for both training and validation sets. The solid lines show (smoothed) tr aining loss and thedotted lines show loss \n",
            "\n",
            "Summary: We choose a vocabulary size of 217tokens based on experiments with vocabulary ranging from 25,000 to 550,000. We train BloombergGPT on a total of 64 p4d.24xlarge instances. Each instance has 8 NVIDIA40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) We achieve                102 TFLOPs on average and each training step takes 32.5 seconds.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.4:\n",
            " training. Figure 2 shows the learning curves for both training and validation sets. The solid lines show (smoothed) tr aining loss and thedotted lines show loss on the held-out validation set. Changes in the col or of the lines15indicate changes to the optimization hyperparameter conﬁgurations. We trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after /tILDelow80% of one epoch. We evaluate BloombergGPT on a high-coverage set of standard benchmarks that assess downstream performance, taken from HE LM, SuperGLUE, MMLU, and GPT-3. We additionally report results from the original GPT. We consider three methods for classiﬁcat ion: regular, cali-bration, and normalization. We report the performance of the best method for each model and task. summarize: cial split and report performance on the test set wheneve r possible. We evaluate the bits per byte of the diﬀere nt models on a heldout.dataset that contains examples from all sections of FinPile. The Financial Phrasebank Dataset includes a sentimen t                classiﬁcation task on sentences from ﬁnancial news. BloombergGPT performs best of all models for four of the ﬁve tasks (ConvFinQA, FiQA SA, FPB, and Headline) BloombergGPT also has the highest win rate among all the models that we tested. The gap to equally-sized models is especially pronounced for Conv FinQA w hich is \n",
            "\n",
            "Summary: We trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after /tILDelow80% of one epoch. BloombergGPT performs best of all models for four of the ﬁve tasks (ConvFinQA, FiQA SA, FPB, and Headline) Figure 2 shows the learning curves for both training and validation sets.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.5:\n",
            "GPT also has the highest win rate among all the models that we tested. The gap to equally-sized models is especially pronounced for Conv FinQA w hich is challenging due to the requirement to use conversational input to reason over tab les. BloombergGPT performs better than all the other tested models, by a wide margin. The only task in which the models perform similarly is the social media sentiment task. We report average number of LOCation, ORGanization, PERson, and PERson per example. We measure the performance of LLMs for the internal datasets using a ﬁve- shot evaluation, similar to the external datasets. BloombergGPT outperforms the other models by at least 25 and up to over 60 points in the other three. The only task in which the models perform similarly is the social media sentiment task. NER is an information extraction task, and a better for encoder-decod er or encoder onlyarchitectures. The generative nature of LLMs does not confer an advantage for NER. BloombergGPT GPT-NeoX OPT 66B BLOOM 176BNER. We randomly sample 4,000 training and 500 test ing ex-amples from each internal dataset. The results from the internal NER tasks are mixed (Table 12). The muc h larger                BLOOM176Bwins most of the N ER tasks. On NER+NED, BloombergGPT outperforms all other models by a large margin. BloombergGPT falls behind PaLM 540B and BLOOM 176B, but is the best-performing among similarly sized models. It achieves the best perfor-                mance \n",
            "\n",
            "Summary: BloombergGPT performs better than all the other tested models, by a wide margin. The only task in which the models perform similarly is the social media sentiment task. On NER+NED, BloombergGPT outperforms all other models by a large margin. NER is an information extraction task, and a better for encoder-decod er or encoder onlyarchitectures.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.6:\n",
            "LM 540B and BLOOM 176B, but is the best-performing among similarly sized models. It achieves the best perfor-                mance of all models in date understanding, hyperbaton (ordering of adjec tives), and tracking grotesqueshuﬄed objects. BloombergGPT consistently outperforms models of similar size while almost being on par with themuch larger models. The Massive Multitask Language Understanding (MMLU) covers 57 diﬀerent subjects. BloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, andOPT66B. G PT-3 performs second best on average, while OpenBookQA is the best. The gap to GPT -3 is closest in the STEM and 'Other' domains. Summarize:  74.59 46.36 57.46 52.94 76.7 OpenBookQA 51.60 44.20 58.00 47.2058.8 RACE (middle) 54.32 41.23 47.42 52.30 57.4RACE (high) 41.74 34.33 37.02 39.14 45.9ReCoRD 82.79 67.86 82.9 82.7 BloombergGPT is the most accurate of the models. BloombergGPT can be utilized to make interactions with financial data more natural. It can be used for many news applications and to assist journalists in their day-to-day work. The US housing market shrank in value by $2.3 trillion, or 4.9%, in the secondhalf of 2022. That’s the largest drop in percentage terms \n",
            "\n",
            "Summary: BloombergGPT consistently outperforms models of similar size while almost being on par with themuch larger models. It achieves the best perfor-                mance of all models in date understanding, hyperbaton (ordering of adjec tives), and tracking grotesqueshuﬄed objects. G PT-3 performs second best on average, while OpenBookQA is the best.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.7:\n",
            " housing market shrank in value by $2.3 trillion, or 4.9%, in the secondhalf of 2022. That’s the largest drop in percentage terms since the 2008 housing crisis. BloombergGPT could help with the editing process by suggesting initial headlines from the text. It could also predict the CEO at Cirrus Logic who was included in the prompt. We were not able to find any example where the other models solved the task whil e BloombergGPT did not. The study was published in the open-source journal, Theoretical Computer Science. The value of domain-speciﬁc training for language models is well established. In-domain training allows model s to outperform state-of-the-art models in biomedical text mining tas ks. Large corpora of raw text data are critical for training LLMs. Galactica is trained exclusively on a large collection of scientiﬁc datasets. We want to augment our private data with public data with the goal of making NLP more accessible. The aim is to investigate whether a model can gain in-domain capabilities without sacriﬁcing general-domain perfor mance. Bloomberg’s core business curates and provides access to datasets, which we use to construct a high-quality dataset to trainBloombergGPT. There exists no standard benchmark for the ﬁnancial NLP domain. Large language model training remains expensive. The rapid development and adoption of large language models has led to the development of new models. The Unigram tokenizer learns a top-down vocabulary by initializing a large vocabulary and repeatedl y discarding those items that increase loss the least. SentenceP \n",
            "\n",
            "Summary: BloombergGPT could help with the editing process by suggesting initial headlines from the text. It could also predict the CEO at Cirrus Logic who was included in the prompt. We were not able to find any example where the other models solved the task whil e BloombergGPT did not. The study was published in the open-source journal, Theoretical Computer Science. The value of domain-speciﬁc training for language models is well established.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.8:\n",
            " The Unigram tokenizer learns a top-down vocabulary by initializing a large vocabulary and repeatedl y discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. A dedicated biomedical vo-                cabulary improved performance on sequence labeling tasks. The rapid development and adoption of large language models have been accompanied by a rigorous conversation about the ethics, uses, and limitations of these m odels. We discuss issues that are directly relevant to the development of BloombergGPT. We conduct our resear ch, development, and deployment of NLP and AI systems in accordance with ethical guidelines. We conclude by discussing the implications of our findings for the financial industry. Our training strategy of mixing domain-speciﬁc and general-p urpose data results in a model that balances performance in both domains. We hope that our model's training logs will provide a guide for those training their own LLMs. We plan to consider what unique op portunities exist in the LLMs we are training. We have several interesting directions to pursue. \n",
            "\n",
            "Summary: The Unigram tokenizer learns a top-down vocabulary by initializing a large vocabulary and repeatedl y discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. A dedicated biomedical vo-                cabulary improved performance on sequence labeling tasks. We conclude by discussing the implications of our findings for the financial industry.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Recursive\n",
            "Bart long recurs. level: 4\n",
            "\n",
            "Number of chunks: 3\n",
            "Chunk no.1:\n",
            "BloombergGPT is a 50 billion parameter language model that is trained on a wide range of data. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain. We train BloombergGPT from a comprehensive dataset of news, press releases, web-scraped documents, and social media drawn from the Bloomberg archiv es. To improve data quality, we de-duplicate each dataset according to Lee et al. (2022a) to improve quality. We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer. We choose a vocabulary size of 217tokens based on experiments with vocabulary ranging from 25,000 to 550,000. We train BloombergGPT on a total of 64 p4d.24xlarge instances. Each instance has 8 NVIDIA40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) We achieve                102 TFLOPs on average and each training step takes 32.5 seconds. We trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after /tILDelow80% of one epoch. BloombergGPT performs best of all models for four of the ﬁve tasks (ConvFinQA, FiQA SA, FPB, and Headline) Figure 2 shows the learning curves for both training and validation sets. \n",
            "\n",
            "Summary: BloombergGPT is a 50 billion parameter language model that is trained on a wide range of data. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.2:\n",
            "�ve tasks (ConvFinQA, FiQA SA, FPB, and Headline) Figure 2 shows the learning curves for both training and validation sets. BloombergGPT performs better than all the other tested models, by a wide margin. The only task in which the models perform similarly is the social media sentiment task. On NER+NED, BloombergGPT outperforms all other models by a large margin. NER is an information extraction task, and a better for encoder-decod er or encoder onlyarchitectures. BloombergGPT consistently outperforms models of similar size while almost being on par with themuch larger models. It achieves the best perfor-                mance of all models in date understanding, hyperbaton (ordering of adjec tives), and tracking grotesqueshuﬄed objects. G PT-3 performs second best on average, while OpenBookQA is the best. BloombergGPT could help with the editing process by suggesting initial headlines from the text. It could also predict the CEO at Cirrus Logic who was included in the prompt. We were not able to find any example where the other models solved the task whil e BloombergGPT did not. The study was published in the open-source journal, Theoretical Computer Science. The value of domain-speciﬁc training for language models is well established. The Unigram tokenizer learns a top-down vocabulary by initializing a large vocabulary and repeatedl y discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. A dedicated biomedical vo-         \n",
            "\n",
            "Summary: BloombergGPT performs better than all the other tested models, by a wide margin. The only task in which the models perform similarly is the social media sentiment task. G PT-3 performs second best on average, while OpenBookQA is the best. BloombergGPT could help with the editing process by suggesting initial headlines from the text. It could also predict the CEO at Cirrus Logic who was included in the prompt.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.3:\n",
            " discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. A dedicated biomedical vo-                cabulary improved performance on sequence labeling tasks. We conclude by discussing the implications of our findings for the financial industry. \n",
            "\n",
            "Summary: summarize:  discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. A dedicated biomedical vo-                cabulary improved performance on sequence labeling tasks. We conclude by discussing the implications of our findings for the financial industry. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Recursive\n",
            "Bart long recurs. level: 5\n",
            "\n",
            "Number of chunks: 1\n",
            "Chunk no.1:\n",
            "BloombergGPT is a 50 billion parameter language model that is trained on a wide range of data. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain. BloombergGPT performs better than all the other tested models, by a wide margin. The only task in which the models perform similarly is the social media sentiment task. G PT-3 performs second best on average, while OpenBookQA is the best. BloombergGPT could help with the editing process by suggesting initial headlines from the text. It could also predict the CEO at Cirrus Logic who was included in the prompt. summarize:  discarding those items that increase loss the least. SentencePiece adapts the schemes mentioned above to handle languages that are not space separated. A dedicated biomedical vo-                cabulary improved performance on sequence labeling tasks. We conclude by discussing the implications of our findings for the financial industry. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or see www.samaritans.org. \n",
            "\n",
            "Summary: BloombergGPT is a 50 billion parameter language model that is trained on a wide range of data. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "T5 recurs. level: 1\n",
            "\n",
            "Number of chunks: 101\n",
            "Chunk no.1:\n",
            "BloombergGPT: A Large Language Model for Finance Shijie Wu1,, Ozan Irsoy1,, Steven Lu1,, Vadim Dabravolski1, Mark Dredze1,3, Sebastian Gehrmann1, Prabhanjan Kambadur1, David Rosenberg2, Gideon Mann1 1Bloomberg, New York, NY USA 2Bloomberg, Toronto, ON Canada 3Computer Science, Johns Hopkins University, Baltimore, MD USA Abstract The use of NLP in the realm of financial technology is broad and complex, with app lications ranging from sentiment analysis and named entity recognition to questi on answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks ; however, no LLM specialized for the financial domain has been reported in literature. In this work, we presentBloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloombe rg’s extensive data sources, perhaps the largest domain-specific dataset y et, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on stan- dard LLM benchmarks, open financial benchmarks, and a suite of internal be nchmarks that most accurately reflect our intended usage. Our mixed dataset train ing leads to a model that outperforms existing models on financial tasks by significan t margins without sacrificing performance on general LLM benchmarks. Additionally, we expl a \n",
            "\n",
            "Summary: BloombergGPT is a large language model trained on a wide range of financial data. it outperforms existing models on financial tasks by significan t margins. the model can also be used to answer financial questions. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set based on a wide range of financial data.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.2:\n",
            "outperforms existing models on financial tasks by significan t margins without sacrificing performance on general LLM benchmarks. Additionally, we expl ain our model- ing choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT. Contents 1 Introduction 3 1.1BloombergGPT................................ 3 1.2 Broader Contributions.............................. 4 2 Dataset 5 2.1 Financial Datasets (363B tokens – 51.27% of training)............ 7 2.1.1 Web (298B tokens – 42.01% of training)................ 7 2.1.2 News (38B tokens – 5.31% of training)........... \n",
            "\n",
            "Summary: outperforms existing models on financial tasks by significan t margins. outperforms performance on general LLM benchmarks without sacrificing performance. outperforms existing models on financial tasks by significan t margins. outperforms existing models on financial tasks by significan t margins. a new version of BloombergGPT is available.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.3:\n",
            "(38B tokens – 5.31% of training)................. 7 2.1.3 Filings (14B tokens – 2.04% of training)................ 7 2.1.4 Press (9B tokens – 1.21% of training)................. 8 2.1.5 Bloomberg (5B tokens – 0.70% of training).............. 8 2.2 Public Datasets (345B tokens – 48.73% of training).............. 9 2.2.1 The Pile (184B tokens – 25.9% of training).............. 9 2.2.2 C4 (138B tokens – 19.48% of training)................. 9 2.2.3 Wikipedia (24B tokens – \n",
            "\n",
            "Summary: tokens have a value of 38B tokens – 5.31% of training. public datasets has a value of 345B tokens – 48.73% of training. wikipedia has a value of 24B tokens – 19.48% of training. twitter has a value of 2B tokens – 1.21% of training.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.4:\n",
            "............ 9 2.2.3 Wikipedia (24B tokens – 3.35% of training).............. 9 2.3 Tokenization................................... 9. Co-first authors. Corresponding author email: gmann16@bloomberg.net 1arXiv:2303.17564v2 [cs.LG] 9 May 20233 Model 11 3.1 Architecture.................................... 11 3.2 Model Scaling................................... 12 3.3 Training Configur \n",
            "\n",
            "Summary: co-first authors: gmann16@bloomberg.net 1arXiv:2303.17564v2 [cs.LG] 9 2.2.3 Wikipedia (24B tokens – 3.35% of training)........................................ 9 2.3 Tokenization................................................................................................................................................ 9 3.3 Training Configuration................................................................................\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.5:\n",
            "............... 12 3.3 Training Configuration.............................. 13 3.4 Large-scale Optimization............................. 14 4 Training Run 15 5 Evaluation 16 5.1 Few-shot Methodology.............................. 18 5.2 Heldout Loss................................... 18 5.3 Financial Tasks............... \n",
            "\n",
            "Summary: training configuration...................................................................................................... 13 3.4 large-scale optimization.............................. 14 4 training run 15 5 evaluation 16 5.1 few-shot methodology.............................................................. 18 5.2 heldout loss................................................................... 18 5.3 financial tasks.........................................................................................................................................................................................................................................................................................................................................................................\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.6:\n",
            "5.3 Financial Tasks.................................. 19 5.3.1 External Financial Tasks........................ 20 5.3.2 Internal Task: Sentiment Analysis................... 22 5.3.3 Exploratory Task: NER......................... 23 5.4 BIG-bench Hard................................. 26 5.5 Knowledge Assessments................. \n",
            "\n",
            "Summary: 5.3 Financial Tasks.........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.7:\n",
            "............................. 26 5.6 Reading Comprehension............................. 28 5.7 Linguistic Tasks.................................. 29 5.8 Summary..................................... 30 6 Qualitative Samples 31 7 Related Work 32 8 Ethics, Limitations, and Implications 37 8.1 Ethical Use...................... \n",
            "\n",
            "Summary: 26 5.6 Reading Comprehension.............................................. 28 5.7 Linguistic Tasks................................................ 29 5.8 Summary...................................... 30 6 Qualitative Samples 31 7 Related Work 32 8 Ethics, Limitations, and Implications......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.8:\n",
            "................................ 37 8.2 Openness..................................... 38 9 Conclusion 38 A Architecture 61 A.0 Notation...................................... 61 A.1 Full Architecture................................. 61 A.2 SelfAttention with ALiBi (SA).............. \n",
            "\n",
            "Summary: architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.9:\n",
            "iBi (SA)......................... 62 A.3 LayerNorm (LN)................................. 63 A.4 FeedForwardNetwork (FFN)........................... 63 A.5 List of All Trainable Parameters........................ 64 B Details on external financial tasks 65 C Training Chronicles 67 C.0 Still.................................. \n",
            "\n",
            "Summary: 62 a.3 layer norm (ln) 63 a.4 feedforwardnetwork (ffn) 63 a.5 list of all trainable parameters................................................ 64 b details on external financial tasks 65 c training Chronicles 67 c.0 still.................................................................................................................. 67 c.0 still..................................................\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.10:\n",
            "..................... 67 C.1 Elbow..................................... 68 C.2 Slide...................................... 71 C.3 Suspense................................... 72 21 Introduction The release of GPT-3 in 2020 (Brown et al., 2020) demonstrated the powerful benefits of training very large auto-regressive language models (LLMs). GPT-3 had 175 b illion parameters, a hundredfold increase over the previous GPT-2 \n",
            "\n",
            "Summary: the release of GPT-3 in 2020 demonstrated the powerful benefits of training very large auto-regressive language models (LLMs) GPT-3 had 175 b illion parameters, a hundredfold increase over the previous GPT-2 model. a large number of parameters can be used to train very large auto-regressive language models (LLMs)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.11:\n",
            "auto-regressive language models (LLMs). GPT-3 had 175 b illion parameters, a hundredfold increase over the previous GPT-2 model, and did remarkably well across a wide range of now popular LLM tasks, including reading compre hension, open-ended question answering, and code generation. This performance has been replicated across several other models (Chowdhery et al., 2022; Scao et al., 2022; Zhang et al., 2022a). Furthermore, evidence suggests that large models exhibit emergent behaviors; growth allows them to acquire abilities not present in smaller models (Wei et al., 2022a). A notable example of emergent behavior is the ability to perform tasks via few- shot prompting, where a model can learn a task from just a few examples. This ability improve s well-above random as we increase the size of language models. Broadly speaking, few-shot promp ting dramatically expands the range of tasks supported by models and lowers the barrier t o entry for users seeking automation for new language tasks. After GPT-3, models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 b il- lion (PaLM, Chowdhery et al., 2022), and 1 trillion parameters (Megatron, Kort hikanti et al., 2022). Work also explored other important aspects of achieving a high- performing \n",
            "\n",
            "Summary: GPT-3 had 175 b illion parameters, a hundredfold increase over the previous model. evidence suggests that large models exhibit emergent behaviors. few-shot prompting dramatically expands the range of tasks supported by models. models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 billion (PaLM, 2022), and 1 trillion parameters.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.12:\n",
            "trillion parameters (Megatron, Kort hikanti et al., 2022). Work also explored other important aspects of achieving a high- performing LLM, such as different training objectives (Tay et al., 2022b), multilin gual models (Scao et al., 2022), more efficient and smaller models (Black et al., 2022), and finding data and parameter-efficient training sizes (Hoffmann et al., 2022). These efforts have almost exclusively focused on general LLMs, trained on datasets that cover a broad range of topics and domains. While these have included some datasets for specialized domains (e.g., code (Chen et al., 2021a) or biomedical articl es (Gao et al., 2021)) the focus has been on building LLMs with broad capabilities. Recent efforts training models using only domain-specific data have yielded models that, w hile much smaller, beat general purpose LLMs on tasks within those domains, such as science (Tayl or et al., 2022) and medicine (Bolton et al., 2023; Luo et al., 2022; Lehman et al., 2023). These findings motivate further development of models focused on specific domains. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role (Xing \n",
            "\n",
            "Summary: financial technology (FinTech) is a large and growing area with NLP technologies having an increasingly important role. these efforts have almost exclusively focused on general LLMs, trained on datasets that cover a broad range of topics and domains. recent efforts training models using only domain-specific data have yielded models that, while much smaller, beat general purpose LLMs on tasks within those domains.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.13:\n",
            "specific domains. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role (Xing et al., 2018; Fisher et al., 2016; Dr edze et al., 2016). Financial NLP tasks (Shah et al., 2022) include sentiment analysis (Araci, 2019), named entity recognition (Salinas Alvarado et al., 2015), news classification ( Sinha and Khandait, 2020), and question answering (Chen et al., 2021b, 2022). While the range of tasks is similar to those found in general NLP benchmarks, the complexit y and terminology of the financial domain warrant a domain-specific system. For all of the reason s generative LLMs are attractive in general – few-shot learning, text generation, conver sational systems, etc. – it would be valuable to have a LLM focused on the financial domain. Wh ile there are masked language models tuned for the financial domain (Araci, 2019), no LLM has been tuned for or evaluated on tasks for this domain. 1.1 BloombergGPT We train BloombergGPT, a 50 billion parameter language model that supports a wide range of tasks within the financial industry. Rather than building a gen eral-purpose LLM, or a small LLM exclusively on domain-specific data, we take a mixed approach. General 3models cover many domains, are \n",
            "\n",
            "Summary: financial technology (FinTech) is a large and growing area with NLP technologies having an increasingly important role. the complexit y and terminology of the financial domain warrant a domain-specific system. no LLM has been tuned for or evaluated on tasks for the financial domain. 1.1 BloombergGPT We train BloombergGPT, a 50 billion parameter language model that supports a wide range of tasks within the financial industry.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.14:\n",
            "purpose LLM, or a small LLM exclusively on domain-specific data, we take a mixed approach. General 3models cover many domains, are able to perform at a high level across a wid e variety of tasks, and obviate the need for specialization during training time. However, results from existing domain-specific models show that general models cannot replace them. At Bloomberg, we support a very large and diverse set of tasks, well served by a general m odel, but the vast majority of our applications are within the financial domain, better serv ed by a specific model. For that reason, we set out to build a model that achieves best- in-class results on financial benchmarks, while also maintaining competitive performanc e on general-purpose LLM benchmarks. We achieve this goal by constructing the largest domain-specific dataset yet, drawing on existing data creation, collection, and curation resources at Bloomber g. As Bloomberg is primarily a financial data company, our data analysts have collected and cu rated financial language documents over the span of forty years. We have extensive arch ives of financial data that cover a range of topics, with careful tracking of data sources and usage rights. We add this data to public datasets to create a large training corpus with o ver 700 billion tokens. Using a portion of this training corpus, we train a BLOOM-style, 50 bill ion parameter model designed based on guidelines from Hoffmann et al. (2022) and Le Scao e \n",
            "\n",
            "Summary: at Bloomberg, we support a large and diverse set of tasks, well served by a general model. but the vast majority of our applications are within the financial domain, better served by a specific model. we achieve this goal by constructing the largest domain-specific dataset yet. we train a BLOOM-style, 50 bill ion parameter model based on guidelines from Hoffmann et al.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.15:\n",
            "LOOM-style, 50 bill ion parameter model designed based on guidelines from Hoffmann et al. (2022) and Le Scao et al. (2022). We validate the model on standard LLM benchmarks, open financial benchmark s, and a suite of Bloomberg-internal benchmarks that most accurately reflect ou r intended use cases. Our results demonstrate that our mixed training approach leads to a mod el that vastly outperforms existing models on in-domain financial tasks while being on par or better on general NLP benchmarks. 1.2 Broader Contributions Beyond the construction of a LLM for financial data, our goal is to contribute to t he broader research community. Specifically, our experience documen ted in this paper provides evidence that further develops the community’s understanding of several open questions in the literature. Domain-specific LLMs. The few existing domain-specific LLMs are trained exclusively on domain-specific data sources (Luo et al., 2022; Bolton et al., 2023; Taylor et al., 2022), or adapt a very large general purpose model to domain-specific tasks (Singh al et al., 2022; Lewkowycz et al., 2022). Our alternative approach – training an LLM on both domain- specific and general data sources – has not been studied so far. The resu lting model does very well on domain-specific tasks, but also maintains strong performance on ge neral \n",
            "\n",
            "Summary: a LOOM-style, 50 bill ion parameter model is designed based on guidelines from Hoffmann et al. (2022) and le Scao et al. (2022) the model vastly outperforms existing models on in-domain financial tasks. our goal is to contribute to t he broader research community.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.16:\n",
            "not been studied so far. The resu lting model does very well on domain-specific tasks, but also maintains strong performance on ge neral-purpose benchmarks. Training data. Nearly all language models rely in large part on web-scraped data, such as C4 (Raffel et al., 2020) and The Pile (Gao et al., 2021) (which includes OpenW ebText2). This data may be cleaned or subsetted in various ways before use (Touv ron et al., 2023; Rae et al., 2020; Scao et al., 2022; Jernite et al., 2022), but issues of data duplication (Carlini et al., 2020) and toxic language remain (Welbl et al., 2021). Our training data is u nusual for LLM training in that it includes a significant amount of curated and prep ared data from reliable sources. Evaluation. LLM evaluation remains a challenging and evolving problem (Gehrmann et al., 2022; Goyal et al., 2022), with new benchmarks trying to standardize e valuation 4across models (Liang et al., 2022; Srivastava et al., 2022). However, for domain-s pecific tasks, there remains a mismatch between evaluation and actual use cases. Evaluations are built on available datasets and \n",
            "\n",
            "Summary: the resu lting model does very well on domain-specific tasks, but also maintains strong performance on ge neral-purpose benchmarks. training data includes a significant amount of curated and prep ared data from reliable sources. evaluations are built on available datasets, but for domain-specific tasks, there remains a mismatch between evaluation and actual use cases.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.17:\n",
            "2). However, for domain-s pecific tasks, there remains a mismatch between evaluation and actual use cases. Evaluations are built on available datasets and not necessarily on how the model will be used in practice. We provide results on both public financial NLP benchmarks (Shah et al., 2022; Chen et al., 2021b) as well as a selection of internal Bloomberg tasks, which are better aligned with our intended use cases and directly evaluate our model’s ability to per form tasks of interest. Model Size. Early LLMs made a single training pass over a corpus of 200-400 billion to- kens (Brown et al., 2020) and Hoffmann et al. (2022) posited that models were unde rtrained, instead focusing on training smaller models with more data, a strategy most recently em- ployed by Touvron et al. (2023). We select a model size motivated by Hoffmann et al. (2022) and train a 50 billion parameter model on 569 billion tokens from our corpus of o ver 700 billion tokens to produce a model that is competitive with larger mo dels. Tokenizer. After assembling training data, the critical step of tokenization trans forms the text into a format suitable for the language model. The importance of t his step is often overlooked (Mielke et al., 2021), and many older LLMs use the same token izer and vocabulary, meaning that we \n",
            "\n",
            "Summary: evaluations are built on available datasets and not necessarily on how the model will be used in practice. we train a 50 billion parameter model on 569 billion tokens from our corpus of o ver 700 billion tokens. the critical step of tokenization trans forms the text into a format suitable for the language model. our model produces a model that is competitive with larger models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.18:\n",
            "often overlooked (Mielke et al., 2021), and many older LLMs use the same token izer and vocabulary, meaning that we have little evidence to support other t okenizers. We take a different approach and use a Unigram model instead of greedy merge-based s ub-word tokenizers since it saves probabilities allowing for smarter token ization at inference time (Kudo, 2018). Model Building Challenges. GPT-3 and subsequent models were the work of large teams and required an enormous amount of computation. Initial work to repro duce these results, such as OPT (Zhang et al., 2022a), did not match the performance of t he original model. With the release of each subsequent model, the community’ s understanding, ex- perience, and software tools increase. In developing BloombergGPT, we benefited from existing code developed as part of the BLOOM effort (Scao et al., 2022), sho wing that a moderately sized team can produce a competitive model on domain-spe cific data. We de- scribe our experiences training BloombergGPT in detail to support future training efforts and address each of the above topics. 2 Dataset To train BloombergGPT, we construct “ FinPile ”, a comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped fi- nancial documents, and social media drawn from the Bloomberg archiv es. \n",
            "\n",
            "Summary: we use a Unigram model instead of greedy merge-based s ub-word tokenizers. we train BloombergGPT on a range of financial documents drawn from the Bloomberg archiv es. the data is a comprehensive dataset consisting of news, filings, web-scraped fi- nancial documents, and social media.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.19:\n",
            "including news, filings, press releases, web-scraped fi- nancial documents, and social media drawn from the Bloomberg archiv es. These documents have been acquired through our business process over the past two d ecades. We augment FinPile with public data widely used to train LLMs. The result is a training corpus that is roughly half domain-specific text and half general-purpose text. For a breakdown of the full training set, see Table 1. To improve data quality, we de-dupl icate each dataset (The Pile, C4, Wikipedia, FinPile ) according to Lee et al. (2022a); as a side-effect, the statistics reported in Table 1 might be different from those reported in other pape rs. 5DatasetDocs 1e4C/DChars 1e8C/TToks 1e8T% FinPile 175,886 1,017 17,883 4.92 3,635 51.27% Web 158,250 933 14,768 4.96 2,978 42.01% News 10,040 1,665 1,672 4.44 376 5.31% Filings 3,335 2,340 780 5.39 145 2.04% Press 1,265 3,443 435 5.06 86 1.21% Bloomberg 2,996 758 227 4.60 49 0.70% PUBLIC 50,744 3,314 16,818 4.87 3,454 48.73% C4 34,832 2,206 7,683 5.56 1,381 19.48% Pile-CC \n",
            "\n",
            "Summary: Bloomberg archiv es news, filings, press releases, web-scraped financial documents. we augment FinPile with public data widely used to train LLMs. the result is a training corpus that is roughly half domain-specific text and half general-purpose text. we de-duplicate each dataset according to Lee et al. (2022a)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.20:\n",
            "16,818 4.87 3,454 48.73% C4 34,832 2,206 7,683 5.56 1,381 19.48% Pile-CC 5,255 4,401 2,312 5.42 427 6.02% GitHub 1,428 5,364 766 3.38 227 3.20% Books3 19 552,398 1,064 4.97 214 3.02% PubMed Central 294 32,181 947 4.51 210 2.96% ArXiv 124 47,819 591 3.56 166 2.35% OpenWebText2 1,684 3,850 648 5.07 128 1.80% FreeLaw 349 15,381 537 4.99 108 1.52% StackExchange 1,538 2,201 339 4.17 81 1.15% DM Mathematics 100 8,193 82 1.92 43 0.60% Wikipedia (en) 590 2,988 176 4.65 38 0.53% USPTO Backgrounds 517 4,339 224 6.18 36 0.51% PubMed Abstracts 1,527 1,333 204 5.77 35 0.50% OpenSubtitles 38 31,055 119 4.90 24 0.34% Gutenberg (PG-19) 3 399,351 112 4.89 23 0.32% Ubuntu IRC 1 539,222 56 3.16 18 0.25% EuroParl 7 65,053 45 2.93 15 0.21% YouTubeSubtitles 17 19,831 33 2.54 13 0.19% BookCorpus2 2 370,384 65 5.36 12 0.17% HackerNews 82 5,009 41 4.87 8 0.12% \n",
            "\n",
            "Summary: GitHub 1,428 5,364 766 3.38 227 3.20% books3 19 552,398 1,064 4.97 214 3.02% arxiv 124 47,819 591 3.56 166 2.35% openwebtext2 1,684 3,850 648 5.07 128 1.80% freelaw 349 15,381 537 4.99 108 1.52% StackExchange 1,538 2,201 339 4.17 81 1.15% DM mathematics 100 8,193 82 1.92\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.21:\n",
            "0.19% BookCorpus2 2 370,384 65 5.36 12 0.17% HackerNews 82 5,009 41 4.87 8 0.12% PhilPapers 3 74,827 23 4.21 6 0.08% NIH ExPorter 92 2,165 20 6.65 3 0.04% Enron Emails 24 1,882 5 3.90 1 0.02% Wikipedia (7/1/22) 2,218 3,271 726 3.06 237 3.35% TOTAL 226,631 1,531 34,701 4.89 7,089 100.00% Table 1: Breakdown of the full training set used to train BloombergGPT. The statistics provided are the average number of characters per document (“C/D”), th e average number of characters per token (“C/T”), and the percentage of the overall t okens (“T%”). Units for each column are denoted in the header. 62.1 Financial Datasets (363B tokens – 51.27% of training) The Bloomberg Terminal has provided access to a comprehensive set of diverse structured and unstructured financial data and analytics for the past four decades. I n serving this mission, Bloomberg analysts have curated a set of financial documents that were either created internally or acquired from external sources. We utilize th is extensive collection of curated and maintained documents to create FinPile, which consists of company filings, financial news, and other data relevant to the financial markets. Some documents included in the FinPile, such as company filings, are available to the general public, although collecting these documents and pre- \n",
            "\n",
            "Summary: the full training set used to train BloombergGPT. the statistics provided are the average number of characters per document (“C/D”), th e average number of characters per token (“C/T”) and the percentage of the overall t okens (“T%”) Bloomberg analysts have curated a set of financial documents to create the FinPile.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.22:\n",
            "to the financial markets. Some documents included in the FinPile, such as company filings, are available to the general public, although collecting these documents and pre-pr ocessing them for LLM training is a non-trivial task. Other documents, such as (a subset of) Bloomberg news, must be purchased. The rest of the documents are private and available, amon g other sources, through the Bloomberg Terminal. Finally, we clean this data to strip off markup, special formatting, and templates. Note that each document in FinPile is time-stamped, with dates ranging from 2007- 03-01 to 2022-07-31; the quality and quantity of documents increase over this t ime range. While we do not utilize date information in this work, we plan to use it in the future, such as for evaluation of what the model learns about different time perio ds. While we cannot release FinPile, our experience training on a large, carefully curated, and clean domain-specific dataset may provide helpful insights to the commun ity on the advantages and challenges of building a financial LLM in particular, and a domain-speci fic model in general. We provide a breakdown and analysis of FinPile in Table 2 and a brief description of the types of data included below. 2.1.1 Web (298B tokens – 42.01% of training) Bloomberg collects web content by identifying sites that contain fi nancially relevant infor- mation. While this category makes up the majority of FinPile, its classifications are rough \n",
            "\n",
            "Summary: Bloomberg collects web content by identifying sites that contain fi nancially relevant infor- mation. some documents, such as company filings, are available to the general public. the rest of the documents are private and available, amon g other sources, through the Bloomberg Terminal. each document in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.23:\n",
            "identifying sites that contain fi nancially relevant infor- mation. While this category makes up the majority of FinPile, its classifications are rough, with content classified mainly by the location of the web domain. Withi n these location- specific sources, e.g. “US” (15.95% of total), “Asia-Pac” (4.72% of total), and “UK” (1.98% of total), document types are highly varied as would be expected from a w eb crawl. While web sources are common in existing public LLM training datasets, Bloomb erg’s web crawl is focused on high-quality websites that have financially relevant i nformation, as opposed to a general-purpose crawl of the web. 2.1.2 News (38B tokens – 5.31% of training) The News category includes all news sources excluding news articles written by Bloomberg journalists. Overall, there are hundreds of English news sources in FinPile including “Bloomberg Transcripts” (0.41% of total), which are transcripts of Bloomb erg TV news. Generally, the content in this dataset comes from reputable sources of news that are relevant to the financial community so as to maintain factuality and reduce bias. 2.1.3 Filings (14B tokens – 2.04% of training) Company Filings are financial statements prepared by (public) companie s and made avail- able to the general public. In some countries, like the US, public com panies are mandated 7Date Bloomberg Filings News Press Web Total 2007 [ \n",
            "\n",
            "Summary: identifying sites that contain fi nancially relevant infor- mation. Bloomb erg’s web crawl is focused on high-quality websites. the News category includes all news sources excluding news articles written by Bloomberg journalists. the content in this category comes from reputable sources of news that are relevant to the financial community so as to maintain factuality and reduce bias.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.24:\n",
            "able to the general public. In some countries, like the US, public com panies are mandated 7Date Bloomberg Filings News Press Web Total 2007 [03-] 276 73 892 523 2,667 4,431 2008 351 91 1,621 628 9,003 11,695 2009 293 93 1,791 528 9,179 11,883 2010 292 111 1,917 527 11,388 14,236 2011 335 117 2,264 548 13,643 16,907 2012 403 105 2,502 529 15,015 18,554 2013 415 87 2,437 441 17,230 20,610 2014 396 251 2,458 437 18,510 22,052 2015 358 1,639 2,371 427 20,782 25,576 2016 324 1,891 2,509 418 24,337 29,478 2017 294 2,294 2,567 398 25,283 30,837 2018 275 1,791 2,702 420 26,027 31,214 2019 263 1,662 3,102 504 27,195 32,726 2020 277 1,632 2,794 805 30,928 36,435 2021 247 1,767 3,515 938 29,749 36,215 2022 [-07] 140 882 2,206 531 16,872 20,631 4,939 14,486 37,647 8,602 297,807 363,482 Table 2: The number of tokens (in millions) contained within documen ts inFinPile, or- gan \n",
            "\n",
            "Summary: in some countries, like the us, public com panies are mandated. the number of tokens (in millions) contained within documen ts inFinPile, or the number of tokens in a document. the number of tokens in a document. the number of tokens in a document. the number of tokens in a document. the number of tokens in a document.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.25:\n",
            "363,482 Table 2: The number of tokens (in millions) contained within documen ts inFinPile, or- ganized by year (rows) and type (column). Units are millions of tokens. to prepare and submit their financial statements on a regular cadence; e.g., 10-K annual reports and 10-Q quarterly reports. In our dataset, a majority of the filin gs come from EDGAR, which is the SEC’s online database (1.90% of total). Filings are typi cally long PDF documents with tables and charts that are dense in financial inform ation, which are processed and normalized in Bloomberg. Filings are substantially diff erent from the types of documents typically used to train LLMs, but contain critically imp ortant information for financial decision-making. 2.1.4 Press (9B tokens – 1.21% of training) The Press category contains press releases typically issued by compan ies that are financially relevant. Taken together with filings, press releases represent mos t of the public communi- cations of a company. However, unlike filings, press releases are simil ar to news stories in terms of content and style. 2.1.5 Bloomberg (5B tokens – 0.70% of training) This category comprises Bloomberg authored news and other documents s uch as opinions and analyses. The largest sources are “Bloomberg News” (0.44% of total) and “Bloom berg First Word” (0.13% of total), the Bloomberg-authored wire of real- \n",
            "\n",
            "Summary: in our dataset, a majority of the filings come from EDGAR, which is the SEC’s online database. the largest sources are “bloomberg news” (0.44% of total) and “bloomberg first word” (0.13% of total), the Bloomberg-authored wire of real-time news. the largest sources are “press releases” (1.21% of total) and “bloomberg first word” (0.13% of total)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.26:\n",
            "“Bloomberg News” (0.44% of total) and “Bloom berg First Word” (0.13% of total), the Bloomberg-authored wire of real-time new s. While Bloomberg News covers a wide range of topics, it typically focuses on c ontent relevant to the financial community. This dataset contains documents of varying lengths. 82.2 Public Datasets (345B tokens – 48.73% of training) We use three widely known and available public datasets in our traini ng corpus. 2.2.1 The Pile (184B tokens – 25.9% of training) The Pile (Gao et al., 2021) is the dataset used in GPT-Neo (Black et al., 2021), G PT- J (Wang and Komatsuzaki, 2021), and GPT-NeoX (20B) (Black et al., 2022). We include The Pile in our training data for the following reasons. First, it has b een used to successfully train an LLM. Second, it has undergone significant data cleaning and pre-pro cessing. Third, it includes multiple domains and we believe such diverse data wil l aid generalization to new domains and may even support training on financial data. For example, domains such as FreeLaw and GitHub are useful to teams at Bloomberg that work on legal docum ents and software development, respectively. Creators of The Pile have del iberately chosen to include duplicate content, with the duplication \n",
            "\n",
            "Summary: Bloomberg News (0.44% of total) and “Bloomberg First Word” (0.13% of total) are Bloomberg-authored wires of real-time new s. we use three widely known and available public datasets in our traini ng corpus. we believe such diverse data will aid generalization to new domains and may even support training on financial data.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.27:\n",
            "docum ents and software development, respectively. Creators of The Pile have del iberately chosen to include duplicate content, with the duplication factor being proportional to t he perceived quality of the content. However, as we deduplicate each of our datasets, the size of The Pile is significantly reduced. Additionally, note that our tokenizer ( 2.3) is trained on The Pile. 2.2.2 C4 (138B tokens – 19.48% of training) The Colossal Clean Crawled Corpus (C4) is a common dataset used to train LLMs, and was introduced to support training T5 (Raffel et al., 2020). Although it overl aps with Pile-CC, C4 is cleaned and processed differently; hence, we feel that incl uding C4 in addition to The Pile can add value more than duplicated documents would. We find th at C4 contains high-quality natural language documents due to the layers of cleaning, t hough others have noted that the distribution across web domains is unusual, with a high f raction of data stemming from patents (Dodge et al., 2021). 2.2.3 Wikipedia (24B tokens – 3.35% of training) Both The Pile and C4 include out-of-date copies of Wikipedia, so it coul d be beneficial for the factuality of the model to have up-to-date Wikipedia pages inclu ded. Therefore, we include a dump of English Wikipedia \n",
            "\n",
            "Summary: the creators of The Pile have deliberately chosen to include duplicate content. however, as we deduplicate each of our datasets, the size of The Pile is significantly reduced. we also include a dump of english Wikipedia pages in addition to out-of-date versions of the corpus. this is to ensure that the factuality of the model is not compromised.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.28:\n",
            "d be beneficial for the factuality of the model to have up-to-date Wikipedia pages inclu ded. Therefore, we include a dump of English Wikipedia from July 1, 2022. This dataset is toke nized quite inefficiently (3.06 characters per token), indicating an above-average amount of markup, which suggests that further cleaning might benefit future model tr aining. 2.3 Tokenization We choose the Unigram tokenizer (Kudo, 2018) instead of a greedy merge-based s ub-word tokenizer, such as Byte Pair Encoding (BPE) (Sennrich et al., 2016) or W ordpiece (Schuster and Nakajima, 2012; Wu et al., 2016), based on promising results in Kudo and Richar dson (2018) and Bostrom and Durrett (2020). Following GPT-2 (Radford et al., 2019), we tr eat our data as a sequence of bytes rather than Unicode characters, and we inc lude each of the 256 bytes as tokens. In a pretokenization step, the input byte sequen ce is broken into chunks by greedily matching the following regular expression: [ A-Za-z]+|[0-9]|[A-Za-z0-9]+. This follows GPT-2 in preventing multiple character classes from appearing in a single token. However, we include spaces in the alphabetic chunk \n",
            "\n",
            "Summary: d be beneficial for the factuality of the model to have up-to-date Wikipedia pages inclu ed. we use the Unigram tokenizer instead of a greedy merge-based s ub-word tokenizer. we include spaces in the alphabetic order of the 256 bytes in the input byte sequen ce.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.29:\n",
            "0-9]+. This follows GPT-2 in preventing multiple character classes from appearing in a single token. However, we include spaces in the alphabetic chunks, which allows multi-word tokens to be learned, increasing information density and reducing context lengt hs. The pretokenization 9BLOOM /ours NeoX /ours OPT /ours BloombergGPT FinPile (old) 451 110% 460 112% 456 111% 412 C4 166 121% 170 123% 170 123% 138 The Pile 203 110% 214 116% 239 130% 184 Wikipedia 21 88% 23 99% 24 103% 24 Total 390 113% 408 118% 434 126% 345 Table 3: Number of tokens in each training dataset with BLOOM, NeoX, OPT (GPT 2), andBloombergGPT tokenizers. All token counts are in billions (B). Note that an older version of FinPile was used for this count, so token numbers wi ll not match earlier tables. follows the approach of PaLM (Chowdhery et al., 2022) in placing each digit in i ts own chunk, with the hope that this will lead to better handling of numbe rs. We train our tokenizer on The Pile (Gao et al., 2021) as it draws from diverse domains, in cluding code and academic papers, in proportions that suit our use case. Parallel Tokenizer Training. The \n",
            "\n",
            "Summary: we include spaces in the alphabetic chunks, which allows multi-word tokens to be learned, increasing information density and reducing context lengt hs. we train our tokenizer on The Pile as it draws from diverse domains, in cluding code and academic papers. our tokenizer is based on BLOOM, NeoX, OPT (GPT 2), and BloombergGPT.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.30:\n",
            ") as it draws from diverse domains, in cluding code and academic papers, in proportions that suit our use case. Parallel Tokenizer Training. The Unigram tokenizer implementation is too inefficient to process the entire Pile dataset at once, so we use a split and merge approach. We split each of the 22 domains in the Pile into 256 chunks of roughly equal size. We t hen train a Unigram tokenizer with a vocabulary size of 65,536 (216) on each of the 22 256 (total = 5,632) chunks. We hierarchically merge the individual tokenizers by fir st merging the 256 tokenizers from each domain, and then combining the 22 resulting toke nizers to get the final tokenizer. Unigram tokenizers amount to probability distributions over tokens (i.e. unigram lan- guage models), and we merge tokenizers by taking a weighted average of the probabilities of corresponding tokens, with the weights determined by the relati ve sizes (in bytes) of the data used to train the tokenizers. The result is a tokenizer wi th 7 million tokens. To reduce the size of the vocabulary to 217tokens, we drop the tokens with the smallest prob- abilities and renormalize. To ensure we do not need an out-of-vocabular y token, we also add as tokens the 36 (of 256 possible) bytes that do not occur in The Pile, al \n",
            "\n",
            "Summary: split and merge approach is used to train a Unigram tokenizer on a large dataset. the result is a tokenizer with 7 million tokens. to reduce the size of the vocabulary to 217 tokens, we drop the tokens with the smallest prob- abilities and renormalize. to ensure we do not need an out-of-vocabular y token, we add as tokens the 36 (of 256 possible) bytes that do not occur in the data.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.31:\n",
            "of-vocabular y token, we also add as tokens the 36 (of 256 possible) bytes that do not occur in The Pile, al ong with an |endoftext|> token. There are various considerations in choosing the vocabulary size. One advantage of a large vocabulary for LLMs is that more information can fit into the context win dow. On the other hand, there is overhead with a larger vocabulary: a larger prop ortion of model parameters are required for token embedding. We select our vocabulary size of 217tokens based on experiments with vocabulary ranging from 25,000 to 550,000. For each vocab ulary size, we tokenize the C4 dataset and compute the total size (in bytes) for the dataset, where each token is represented using log2(vocabulary size) bits. Our heuristic is to choose the vocabulary size that leads to the smallest encoded representation of C4. This gives us a vocabulary size of 125,000, which we then round up to the nearest power of 2 ( 217, or 131,072 tokens). Our tokenizer is large, relative to the standard vocabulary s ize of approximately 50,000 tokens. For an analysis of tokenization efficiency, see Table 3. 10Shape Number of Layers 70 Number of Heads 40 Vocabulary Size 131,072 Hidden Dimension 7,680 Total Parameters 50.6B Hyperparameters Max Learning Rate 6e-5 Final Learning Rate 6e-6 Learning Rate schedule cosine decay Gradient Clipping 0.3 Training Tokens 5 \n",
            "\n",
            "Summary: we select our vocabulary size of 217tokens based on experiments with 25,000 to 550,000 words. there are various considerations in choosing the vocabulary size. one advantage of a large vocabulary for LLMs is that more information can fit into the context window. on the other hand, there is overhead with a larger vocabulary: a larger prop ortion of model parameters are required for token embedding.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.32:\n",
            "B Hyperparameters Max Learning Rate 6e-5 Final Learning Rate 6e-6 Learning Rate schedule cosine decay Gradient Clipping 0.3 Training Tokens 569B Hardware 64 8 A100 40GB Throughput 32.5 sec/step avg. TFLOPs 102 total FLOPS 2.36e23 Table 4: A summary of the hyper-parameters and their values for BloombergGPT. 3 Model 3.1 Architecture Our model is a decoder-only causal language model based on BLOOM (Scao et al., 2022). We present an overview of the architecture, with full details in Ap pendix A. The model contains 70 layers of transformer decoder blocks defined as follows: hl=hl1+ SA(LN( hl1)) hl= hl+ FFN(LN( hl)) where SA is multi-head self-attention, LN is layer-normalization, an d FFN is a feed-forward network with 1-hidden layer. Inside FFN, the non-linear function i s GELU (Hendrycks and Gimpel, 2016). ALiBi positional encoding is applied through additive biase s at the self- attention component of the transformer network (Le Scao et al., 2022). The inp ut token embeddings are tied to the linear mapping before the final softmax. Foll owing Le Scao et al. (20 \n",
            "\n",
            "Summary: BloombergGPT is a decoder-only causal language model based on BLOOM. the model contains 70 layers of transformer decoder blocks defined as follows: hl=hl1+ SA(LN( hl1)) hl= hl+ FFN(LN( hl1)) where SA is multi-head self-attention, LN is layer-normalization, an d FFN is a feed-forward network with 1-hidden layer.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.33:\n",
            "p ut token embeddings are tied to the linear mapping before the final softmax. Foll owing Le Scao et al. (2022) and first used in Dettmers et al. (2022), the model has an additional layer normalization after token embeddings, formally: h1= LNem(h0) + SA(LN(LNem(h0))), whereh0is the initial token embedding and LNemis the new component of embedding layer- normalization. Notice that the second term includes two consecutive l ayer-normalizations. 111e22 3.2e22 1e23 3.2e23 1e24 3.2e24 FLOPs10205010020050010002000Parameters (B) NeoX LaMDA GPT-3/Jurassic/OPT OPT GopherMT-NLG BLOOM PaLM PaLM Chinchilla LLaMA LLaMA LLaMA BloombergGPT Optimal # Parameters w.r.t. FLOPs Chinchilla-1 Chinchilla-2 Chinchilla-3 Kaplan 1e22 3.2e22 1e23 3.2e23 1e24 3.2e24 FLOPs100200500100020005000Tokens (B) NeoX LaMDA GPT-3/Jurassic/OPT OPT Gopher MT-NLG BLOOM PaLM PaLM ChinchillaLLaMA LLa \n",
            "\n",
            "Summary: token embeddings are tied to the linear mapping before the final softmax. h1= lnem(h0) + SA(ln(h0)); whereh0is the initial token embedding. 111e22 3.2e22 1e23 3.2e23 1e24 3.2e24 FLOPs102050100200500100020005000Tokens (b) NeoX LaMDA GPT-3/Jurassic/OPT OPT\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.34:\n",
            "Jurassic/OPT OPT Gopher MT-NLG BLOOM PaLM PaLM ChinchillaLLaMA LLaMA LLaMA BloombergGPTOptimal # Tokens w.r.t. FLOPs Chinchilla-1 Chinchilla-2 Chinchilla-3 Kaplan Figure 1: Kaplan et al. (2020) and Chinchilla scaling laws with prior large lan guage model andBloombergGPT parameter and data sizes. We adopt the style from Hoff- mann et al. (2022). 3.2 Model Scaling Size. The size of our model is based on Chinchilla scaling laws (Hoffmann et al., 2022), in particular their Approach 1 and Approach 2. We start with a total compute budge t of 1.3M GPU hours on 40GB A100 GPUs. Since we adopt activation checkpointing to reduc e our memory footprint, this costs us an additional 0.33x TFLOPs per iteration du e to repeated forward passes. To account for this additional cost, we plug in 0.75 1.3M into Chinchilla equations instead of the full amount. From Hoffmann et al. (2022), we use the data reported in Table 3 for Approach 1 and Table A3 for Approach 2, and fit regression lines to their log-scaled versions. This gives us: Approach 1 Parameters = exp10(log10(FLOPs )0.4981.004) = 52.993B Token \n",
            "\n",
            "Summary: the size of our model is based on Chinchilla scaling laws. approach 1 Parameters = exp10(log10(FLOPs)0.4981.004) = 52.993B Tokens = exp10(log10(FLOPs)0.4981.004) = 52.993B Approach 2 Parameters = exp10(log10(FLOPs)0.4981.004) = 52.993B Tokens = exp10(log10(FLOPs)0.4981.004) =\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.35:\n",
            "gives us: Approach 1 Parameters = exp10(log10(FLOPs )0.4981.004) = 52.993B Tokens = exp10(log10(FLOPs )0.502 + 0.229) = 1111.112B Approach 2 Parameters = exp10(log10(FLOPs )0.4900.839) = 49.753B Tokens = exp10(log10(FLOPs )0.510 + 0.062) = 1175.766B These calculations imply that our dataset of /tildelow700B tokens is too small for a “Chinchilla optimal” configuration given our compute budget (assuming just one pass thr ough the data).1While we can increase the amount of general-purpose training data, we are l imited in the amount of domain-specific training data at our disposal. FinPile is already among the largest domain-specific training sets, and we do not want it to repr esent less than half of our total training. 1. The scaling law derived by Chinchilla is tokenizer-specific. Our tokenizer can encode the same document more compactly due to the support of multi-word expressions and t he larger vocabulary size. It’s still an open question how well these scaling laws transfer across token izers, and how vocabulary size impacts token and parameter trade-offs assuming fixed compute. We leave this exploration to future work. 12Since we are data limited, we choose the largest model that we \n",
            "\n",
            "Summary: finPile is already among the largest domain-specific training sets at our disposal. the scaling law derived by Chinchilla is tokenizer-specific. our tokenizer can encode the same document more compactly due to the support of multi-word expressions and t he larger vocabulary size. finPile will be used in a future version of the finpile project.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.36:\n",
            "and parameter trade-offs assuming fixed compute. We leave this exploration to future work. 12Since we are data limited, we choose the largest model that we can, whi le ensuring that we can train on all our tokens and still leave /tildelow30% of the total compute budget as a buffer for unforeseen failures, retries, and restarts. This leads us to a 50B parameter model, which is also roughly the Chinchilla optimal size for our compute budget. Figu re 1 provides a summary of the scaling laws and how BloombergGPT compares to other models. Shape. To determine how to allocate the 50B parameters to different model com ponents (i.e., the “shape” of our model), we follow Levine et al. (2020), who propose that for a total number of self-attention layers L, the optimal hidden dimension Dis obtained by: D= exp(5.039) exp(0.0555L) We sweep Lover a range of integer values and pick the ( L,D) combination that yields a total of /tildelow50B parameters. This leads to the choice of L= 70 and D= 7510 as our target shape parameters. However, we also want to follow the tradition th at the hidden dimension is evenly divisible by the number of attention heads, wi th the quotient giving the attention head dimension. Furthermore, we want the dimensions t o be multiples of 8 to achieve higher performance in Tensor Core operations (NVIDIA, 2023 \n",
            "\n",
            "Summary: BloombergGPT has 50B parameters, which is roughly the Chinchilla optimal size for our compute budget. we want the dimensions t o be multiples of 8 to achieve higher performance in Tensor Core operations. we also want to follow the tradition th at the hidden dimension is evenly divisible by the number of attention heads, wi th the quotient giving the attention head dimension.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.37:\n",
            "head dimension. Furthermore, we want the dimensions t o be multiples of 8 to achieve higher performance in Tensor Core operations (NVIDIA, 2023). We set tle on 40 heads, each having a dimension of 192, resulting in a total hidden dimen sion ofD= 7680 and a total of 50.6B parameters. Table 4 provides a summary of the hyper-param eters used inBloombergGPT. 3.3 Training Configuration Training. BloombergGPT is a PyTorch model trained with a standard left-to-right causal language modeling objective. Following Brown et al. (2020), we want al l our train- ing sequences to be exactly the same length, in our case 2,048 tokens, to maximize GPU utilization. To achieve this, we concatenate all our tokenized trainin g documents with an |endoftext|> token as a document separator. We then break this token sequence into chunks of 2,048 tokens. Note that with this approach, each training sequenc e may contain multiple documents from different domains. Also note that, because w e’re using ALiBi positional encoding, BloombergGPT can be applied to sequences longer than 2,048 at inference time. For optimization efficiency, training sequences ar e grouped together into batches, as described in more detail below. Optimization. We use the AdamW optimizer (Loshchilov and Hutter, 2019). We set 1to 0.9 \n",
            "\n",
            "Summary: BloombergGPT is a PyTorch model trained with a standard left-right causal language modeling objective. we want all our train- ing sequences to be exactly the same length, in our case 2,048 tokens, to maximize GPU utilization. for optimization efficiency, training sequences ar e grouped together into batches, as described in more detail below.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.38:\n",
            "in more detail below. Optimization. We use the AdamW optimizer (Loshchilov and Hutter, 2019). We set 1to 0.9,2to 0.95, and weight decay to 0.1. Following Brown et al. (2020), we set the maximum learning rate to 6e-5 and use the cosine decay learning rate sc heduler with linear warmup. We warm up the learning rate in the first 1800 steps. Following Hoff mann et al. (2022), the final learning rate is 0.1x the max learning rate, i.e. 6e-6. We also employ batch size warmup (Brown et al., 2020): in the first 7,200 steps, we use a batch si ze of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remaind er of training. We set dropout to 0.0 in all layers in our initial run, although we add dropou t later as explained in 4. The model parameters are randomly initialized to samples from a normal distribution with zero mean and standard deviation/radicalbig 1/(3D) = 0.006588 (Smith et al., 2022). Following Megatron-LM (Shoeybi et al., 2019), we rescale the standard de viation of the second layer in the MLP and the output layer of the attention by 1 / 2L. We use 13the technique of querykeylayerscaling (Sh \n",
            "\n",
            "Summary: we set the maximum learning rate to 6e-5 and use the cosine decay learning rate sc heduler with linear warmup. we also employ batch size warmup: in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens) and switch to a batch size of 2,048 (4.2M tokens)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.39:\n",
            "second layer in the MLP and the output layer of the attention by 1 / 2L. We use 13the technique of querykeylayerscaling (Shoeybi et al., 2019), which was proposed to improve numerical stability for FP16 mixed-precision training bu t may also help in BF16. Training Instability. LLMs optimization requires running convex optimization algo- rithms over incredibly complex non-convex loss surfaces. Previou s work has reported vari- ous instabilities while training LLMs. For example, Chowdhery et al. (2022) found that the loss spiked roughly 20 times while training PaLM, despite the fact that gradient clipping was enabled. They mitigated these issues by re-starting training f rom a checkpoint roughly 100 steps before the spike started, and then skip 200–500 data batches. They hypothesized that spikes occur due to the combination of specific data batches with a particular model parameter state. Similarly, during OPT training, Zhang et al. (2022a) notic ed spikes in the gradient and activation norms, or divergences in the training perplexi ty. After these behav- iors, they lowered their learning rate, which stabilized these n orms and allowed training to continue. Interestingly, Scao et al. (2022) report only a single loss spik e, from which the model recovered on its own. Hardware \n",
            "\n",
            "Summary: we use 13the technique of querykeylayerscaling, which was proposed to improve numerical stability for FP16 mixed-precision training bu t may also help in BF16. training Instability. LLMs optimization requires running convex optimization algo- rithms over incredibly complex non-convex loss surfaces. despite the fact that gradient clipping was enabled, Chowdhery et al. (2022) found that the loss spiked roughly 20 times while training pa\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.40:\n",
            ", Scao et al. (2022) report only a single loss spik e, from which the model recovered on its own. Hardware Stack. We use the Amazon SageMaker service provided by AWS to train and evaluate BloombergGPT. We use the latest version available at the time of training and train on a total of 64 p4d.24xlarge instances. Each p4d.24xlarge instance has 8 NVIDIA 40GB A100 GPUs with NVIDIA NVSwitch intra-node connections (600 GB/s) and NVIDIA GPUDirect using AWS Elastic Fabric Adapter (EFA) inter-node connec tions (400 Gb/s). This yields a total of 512 40GB A100 GPUs. For quick data access, we use Amazon FSX f or Lustre, which supports up to 1000 MB/s read and write throughput per TiB s torage unit. 3.4 Large-scale Optimization To train BloombergGPT, which has a larger memory footprint than available GPU mem- ory on cloud instances, we rely on stage 3 of ZeRO optimization (Rajbhandari et al., 2020). We utilize the proprietary SageMaker Model Parallelism (SMP) librar y from AWS, which enables the automatic distribution of large models across multiple GPU devices and in- stances (Karakus et al., 2021). After experimenting with various techni ques, we achieve \n",
            "\n",
            "Summary: we use the amazon SageMaker service provided by aws to train and evaluate BloombergGPT. each p4d.24xlarge instance has 8 NVIDIA 40GB A100 GPUs. for quick data access, we use amazon FSX f or Lustre, which supports up to 1000 MB/s read and write throughput per TiB s torage unit.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.41:\n",
            "GPU devices and in- stances (Karakus et al., 2021). After experimenting with various techni ques, we achieve 102 TFLOPs on average and each training step takes 32.5 seconds. We find the fol lowing setup to be the best performing in our training. ZeRO Optimization (stage 3). ZeRO shards the training state (model parameters, gradients, and optimizer state) across a group of GPUs. We shard a model acros s 128 GPUs, and we have 4 copies of the model during training. MiCS. Zhang et al. (2022b) decrease training communication overhead and memory re - quirements for cloud training clusters. MiCS includes such feat ures as hierarchical commu- nication, 2-hop gradient update, scale-aware model partitioning. Activation Checkpointing. Chen et al. (2016) minimizes training memory consumption by removing activations at the expense of additional computation during backward passes. When a layer has activation checkpointing enabled, only the layer in put and outputs are kept in memory following a forward pass, while any intermediate tens ors are discarded from memory. During the backward pass, these intermediate tensors may b e recomputed. We apply activation checkpointing to each transformer layer. 140 20000 40000 60000 80000 100000 120000 140000 Steps2.002.252.502.753.003.253.503.754.00LossLearning curve \n",
            "\n",
            "Summary: we achieve 102 TFLOPs on average and each training step takes 32.5 seconds. we shard a model acros s 128 GPUs, and we have 4 copies of the model during training. activation checkpointing removes activations at the expense of additional computation during backward passes. we find the fol lowing setup to be the best performing in our training.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.42:\n",
            "40000 60000 80000 100000 120000 140000 Steps2.002.252.502.753.003.253.503.754.00LossLearning curve config lr 6e-5 + bs 1024 lr 6e-5 lr 4e-5 lr 2e-5 + dropout lr 1e-5 + dropout metric smooth train loss val loss 2.102.152.202.252.30 config lr 6e-5 + bs 1024 lr 6e-5 lr 4e-5 lr 2e-5 + dropout lr 1e-5 + dropout metric smooth train loss val loss Figure 2: (Smoothed) training and validation losses for BloombergGPT. Inner plot is a zoomed-in version of the area within dashed rectangle in the outer plot (with shared x-axis). Colors denote different hyperparameter configurations. Styles denote training vs validation loss. Mixed Precision Training. To reduce the memory requirements, forward and backward passes are done in BF16, while parameters are stored and updated in full pr ecision (FP32). The ALiBi matrices are computed in full precision and stored in BF16. We al so use FP32 to calculate fused softmax in the Attention block and store its resul ts in BF16. Finally, the softmax calculations in the loss function are computed in FP32. Fused Kernels. Another possibility for optimization is combining composition of several operations into \n",
            "\n",
            "Summary: training and validation losses for BloombergGPT are plotted. forward and backward passes are done in BF16, while parameters are stored and updated in full precision (FP32) to reduce the memory requirements, forward and backward passes are done in BF16. to reduce the memory requirements, forward and backward passes are done in BF16, while parameters are stored and updated in full precision (FP32)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.43:\n",
            "Finally, the softmax calculations in the loss function are computed in FP32. Fused Kernels. Another possibility for optimization is combining composition of several operations into a single GPU operation. This can both reduce peak memory u sage by avoiding storage of intermediate results in the computation graph, as wel l as help improve speed. Similar to Megatron-LM (Shoeybi et al., 2019), we use a masked-caus al-softmax fused kernel in SMP in the self-attention module. In practice, w e observe 4-5 TFLOPs improvement for speed, and avoid out-of-memory errors given the rest of the configuration. 4 Training Run The process of training BloombergGPT involved decisions along the way based on the progress of model training. We share some highlights of this process. A d etailed presentation appears in the Training Chronicles (Appendix C). Figure 2 shows the learning curves for both training and validation sets. The solid lines show (smoothed) tr aining loss and the dotted lines show loss on the held-out validation set. Changes in the col or of the lines 15indicate changes to the optimization hyperparameter configurations, eit her as scheduled, or in response to increasing or stagnating validation loss. This plot shows the path taken by the successful model training run. To present a clear plot, the F igure does not show other attempts with different model configurations, overwritten partial r uns after a rollback, or other training strategies \n",
            "\n",
            "Summary: combining composition of several operations into a single GPU operation can help improve speed. we use a masked-causal-softmax fused kernel in SMP in the self-attention module. the process of training BloombergGPT involved decisions along the way based on the progress of model training. a d etailed presentation appears in the Training Chronicles (Appendix C)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.44:\n",
            "plot, the F igure does not show other attempts with different model configurations, overwritten partial r uns after a rollback, or other training strategies not utilized in the final model. We measured training loss every five steps on the current batch. The raw values vary wildly, causing large jitter when plotted. The plot smoothes the tr aining loss by showing a running average yt=/summationtextt i=0xi(1)(ti) /summationtextt i=0(1)(ti)where= 0.001. Smoothing is not needed for the validation loss since it is measured on the entire validation set every 300 steps. We trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after completing /tildelow80% of one epoch through our training data (569B tokens out of the 709B tokens available). We ended training early because the loss on our held -out development set was no longer improving, although it’s possible that substantially l onger training may have yielded further improvements. We began the run with a warm-up batch size of 1,024 for 7,200 steps, after whic h we switched to the regular batch size of 2,048 (color changes from black to blue ). Change in batch size manifests as a visible curvature change in the validation los s at step 7,200. Most of the remainder of the training performed stably with decreasing \n",
            "\n",
            "Summary: raw values vary wildly, causing large jitter when plotted. we measured training loss every five steps on the current batch. we trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after completing /tildelow80% of one epoch through our training data (569B tokens out of 709B tokens available)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.45:\n",
            "size manifests as a visible curvature change in the validation los s at step 7,200. Most of the remainder of the training performed stably with decreasing tr aining and validation losses. Intervention was required at later stages, after step 115,500, when we observed flat or increasing validation loss. We then applied the following correc tive modifications in sequence: •Step 115,500 (blue to orange): Shrink learning rate to two-thirds •Step 129,900 (orange to green): Halve learning rate, and add dropout (with 0.1 prob - ability) •Step 137,100 (green to red): Halve learning rate again We ended the run at step 146,000 based on the lack of observable progress on the v alidation loss. We selected the checkpoint at step 139,200 as the final model based on v alidation loss and downstream evaluations. 5 Evaluation We evaluated the performance of BloombergGPT on two broad categories of tasks: finance-specific and general purpose. The finance-specific tasks help us test our hypoth- esis that training on high-quality finance-specific data will yield b etter results on financial tasks. The general purpose tasks investigate whether the performance of our model is directly comparable to previously published results. For financial tasks, we assembled pub- licly available financial datasets that include a range of NLP tasks. Then, t o directly test BloombergGPT ’s ability on Bloomberg tasks of interest, we also included tasks dra wn \n",
            "\n",
            "Summary: the performance of BloombergGPT was evaluated on two broad categories of tasks: finance-specific and general purpose. the finance-specific tasks help us test our hypoth- esis that training on high-quality finance-specific data will yield b etter results on financial tasks. the general purpose tasks investigate whether the performance of our model is directly comparable to previously published results.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.46:\n",
            "NLP tasks. Then, t o directly test BloombergGPT ’s ability on Bloomberg tasks of interest, we also included tasks dra wn from Bloomberg-internal high-quality evaluation sets for sentiment an alysis and named en- tity recognition. For general-purpose tasks, we draw from multiple ex isting benchmarks and group results into the following categories: BIG-bench Hard, Know ledge Assessments, Reading Comprehension, and Linguistic Tasks. The number of tasks per t ype and the definitions of the groups are presented in Table 5. 16Suite Tasks What does it measure? Public Financial Tasks 5 Public datasets in the financial domain Bloomberg Financial Tasks 12 NER and sentiment analysis tasks Big-bench Hard (Suzgun et al., 2022) 23 Reasoning and general NLP tasks Knowledge Assessments 5 Testing closed-book information recall Reading Comprehension 5 Testing open-book tasks Linguistic Tasks 9 Not directly user-facing NLP tasks Table 5: Evaluation Benchmarks. We evaluate BloombergGPT on a high-coverage set of standard benchmarks that assess downstream performance, taken from HE LM, SuperGLUE, MMLU, and the GPT-3 suite. Since these have significant ov erlap and/or include each other, we restructure them into the categories pr esented here. We only evaluate on one setup per dataset. We further assess BloombergGPT on a suite of internal and public financial tasks. Name # Tokens (B) # Params. ( \n",
            "\n",
            "Summary: t o directly test BloombergGPT’s ability on tasks of interest, we also included tasks dra wn from Bloomberg-internal high-quality evaluation sets. for general-purpose tasks, we draw from multiple ex isting benchmarks and group results into the following categories: BIG-bench Hard, Know ledge Assessments, Reading Comprehension, and Linguistic Tasks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.47:\n",
            "setup per dataset. We further assess BloombergGPT on a suite of internal and public financial tasks. Name # Tokens (B) # Params. (B) Compute BloombergGPT 569 50.6 1.00  GPT-NeoX 472 20 0.33  OPT 300 66 0.69  BLOOM 366 176 2.24  GPT-3 300 175 1.82  Table 6: Evaluation model cohort. OPT and BLOOM each have multiple sizes available and we report those we evaluated. We note that compute numbers are only parti ally comparable between models: For example, BLOOMs training data is only 1/3 English, and OPT repeated some of its training data. We report GPT-3 res ults whenever available but did not run it ourselves due to lack of availabi lity. We compare BloombergGPT to the three closest models described in 7 based on model size, type of training data, overall performance, and most import antly, access. An overview of the model sizes and compute is provided in Table 6. 1. GPT-NeoX (Black et al., 2022): According to Liang et al. (2022), this model is th e best performing available model under 50B parameters. 2. OPT 66B(Zhang et al., 2022a): We chose to compare to OPT 66Bsince our model size and structure roughly match, though our model is smaller. 3. BLOOM 176B(Scao et al., 2022 \n",
            "\n",
            "Summary: we further assess BloombergGPT on a suite of internal and public financial tasks. we compare BloombergGPT to the three closest models described in 7 based on model size, type of training data, overall performance, and most import antly, access. we report GPT-3 res ults whenever available but did not run it ourselves due to lack of availabi lity.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.48:\n",
            "Bsince our model size and structure roughly match, though our model is smaller. 3. BLOOM 176B(Scao et al., 2022): While this model is substantially larger than BloombergGPT, we use the same model architecture and software stack. We note that BLOO M176Bis multilingual, so while it is much larger, it also is trained on data from more languages. 17All three models use some of the same general-purpose datasets we use in our training cor- pus. We additionally report results from the original GPT-3 (Brown et al., 2020) whenever externally available.2 We prefer running models ourselves to ensure identical evaluati on setups, and we place any results that have been reported elsewhere and were not run by us into a separated group. To fairly compare the models, we avoid any tuning of prompts and oth er techniques that could lead to improved results for some, but not all, models. For t hat reason, every task is tested via “standard” prompting (shown in Table 7), i.e., wit hout any parameter changes to the underlying model, without task descriptions, and wi thout Chain-of-Thought prompting (Wei et al., 2022b). The number of few-shot examples present ed to the model depends on the task, and we include these details in the respectiv e sections. For each group of results, we further present a win rate similar to Liang et al. (2022) th at represents the \n",
            "\n",
            "Summary: BLOO 176B is substantially larger than BloombergGPT. BLOO M176B is trained on data from more languages. all three models use some of the same general-purpose datasets we use in our training cor- pus. the win rate for each group of results is similar to Liang et al. (2022) th at.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.49:\n",
            "respectiv e sections. For each group of results, we further present a win rate similar to Liang et al. (2022) th at represents the fraction of “wins” in side-by-side comparisons over individual tasks between all model pairs for which we have run the evaluation ourselves. 5.1 Few-shot Methodology For tasks where a set of candidates are given, we perform likelihood-b ased classification, following Brown et al. (2020). We consider three methods for classificat ion: regular, cali- bration, and normalization. Formally, •Regular: arg max p(|s) •Calibration: arg max p(|s)/p(|“Answer:”) •Normalization: arg max p(|s)/len() whereis a candidate, sis the context, and len measures the number of sub-word tokens. We report the performance of the best method for each model and task. For other tasks, we perform generation via greedy decoding. We use the official split and report performance on the test set wheneve r possible. If the test labels are not publicly available, we report performance on th e dev set instead. If an official split for a dataset does not exist, we create train and test spli ts by selecting 20% of examples to be the test and the rest as train. All few-shot context ex amples are sampled from the training set. To reduce the variance of few-shot \n",
            "\n",
            "Summary: a few-shot method is used to classify text using likelihood-based methods. for tasks where a set of candidates are given, we perform likelihood-b ased classification. we use the official split and report performance on the test set wheneve r possible. if an official split does not exist, we create train and test spli ts by selecting 20% of examples to be the test.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.50:\n",
            "examples to be the test and the rest as train. All few-shot context ex amples are sampled from the training set. To reduce the variance of few-shot evaluation, w e sample different shots for each test example, unless otherwise specified. For the sake of consistency, for each test example, all models have identical surface form as input in our e valuation. 5.2 Heldout Loss We begin by testing how well BloombergGPT models the language distribution of the in- distribution finance data. We evaluate the bits per byte of the differe nt models on a heldout dataset that contains examples from all sections of FinPile (described in 2). To limit data leakage and better simulate real-world usage of LLMs, we select a tempor ally heldout 2. Anotherrelatedgeneral-purposemodelatacomparablesize (L LaMA,Touvronetal.,2023), wasreleased during the preparation of this manuscript, but third-party evalua tion results were not available and we haven’t received access to the model weights. 18Overall Bloomberg Filings Newswires Press Web0.00.20.40.60.8bits per byteBloombergGPT GPT-Neo-X OPT 66B BLOOM 176B Figure 3: Bits per byte on a heldout test set of each data type in our FinPile (lower is better). The set of documents is held out in time and deduplicat ed with the training set, such that all of it is completely unseen by BloombergGPT \n",
            "\n",
            "Summary: w e sample different shots for each test example, unless otherwise specified. we evaluate the bits per byte of the differe nt models on a heldout dataset. to limit data leakage and better simulate real-world usage of LLMs, we select a temporally heldout 2. the set of documents is held out in time and deduplicat ed with the training set.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.51:\n",
            "set of documents is held out in time and deduplicat ed with the training set, such that all of it is completely unseen by BloombergGPT. Regardless, we observe a large gap between the models. The improveme nt is largest for specialized in-domain documents like Filings. dataset that is strictly further in the future than the training se t, and perform deduplication between the training and heldout set. During evaluation, for docume nts that are longer than 2,048 tokens, we use a sliding window approach with half window size as c ontext. That means that any token beyond the first 2,048 has at least 1,024 tokens as context dur ing prediction. We report the loss breakdown by the type of document in FinPile. Figure 3 shows that BloombergGPT consistently outperforms other models. While this is expected and mainly serves as a sanity check, it also provid es valuable insight into the generalization capabilities of the other models. For example, the gap to BloombergGPT is most significant in the Filings category, likely because these docu ments, while public, are typically in PDF format and thus not included in any existing datasets. 5.3 Financial Tasks The NLP tasks most often considered in finance are also common in the broader NLP liter- ature; but, these tasks take on different characteristics and challen ges when performed on financial data. Take the example of sentiment analysis, where a headline such as “COM- PANY to cut 10,000 jobs” portray \n",
            "\n",
            "Summary: a set of documents is held out in time and deduplicat ed with the training set. the improveme nt is largest for specialized in-domain documents like Filings. the gap to BloombergGPT is most significant in the Filings category. financial tasks take on different characteristics and challen ges when performed on financial data.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.52:\n",
            "challen ges when performed on financial data. Take the example of sentiment analysis, where a headline such as “COM- PANY to cut 10,000 jobs” portrays negative sentiment in the general sense b ut can at times be considered positive for financial sentiment towards COMPANY, as it mi ght result in the stock price or investor confidence increasing. We use a combination of p ublic and internal benchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, and OPT66B. All task types considered and their corresponding prompt templates are shown in Table 7. 19Task Template/Example Discriminative Sentiment Analysis sentence  Question: what is the sentiment? Answer: negative/neutral/positive  Aspect Sentiment Analysis sentence  Question: what is the sentiment on target? Answer: negative/neutral/positive  Binary Classification sentence  Question: question? Answer: Yes/No Generative NER Steve Jobs is the CEO of Apple Extract named entity: Steve Jobs (person), Apple (organization) NER+NED AAPL stopped using Intel Chips Extract ticker: AAPL, INTC QA context Question: question? Answer: answer Table 7: Template for the different tasks we evaluate in the financial domai n. 5.3.1 External Financial Tasks Our public financial benchmarks include four tasks from the \n",
            "\n",
            "Summary: com-pany to cut 10,000 jobs portrays negative sentiment in the general sense b ut can at times be considered positive for financial sentiment towards COMPANY. we use a combination of p ublic and internal benchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, and OPT66B.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.53:\n",
            "Table 7: Template for the different tasks we evaluate in the financial domai n. 5.3.1 External Financial Tasks Our public financial benchmarks include four tasks from the FLUE bench mark (Shah et al., 2022) and the ConvFinQA dataset (Chen et al., 2022). As LLM performance on most of these financial tasks have not been broadly reported, there is no stand ard testing frame- work. Thus, we adapt them to a few-shot setting (see Section 5.1). Our guiding principle in designing the experiments was to select the number of shots suc h that the average per- formance across all the models was best. While non-LLM numbers of custom m odels for these tasks are available, we omit reporting them here due to differe nces in the evaluation setup. As a result, our claims are restricted to comparisons of LLMs. We e valuate on the following tasks (more details provided in Appendix B): •FPB (Malo et al., 2014): The Financial Phrasebank Dataset includes a sentimen t classification task on sentences from financial news. Any news that could b enefit/hurt an investor is considered positive/negative and neutral otherwise. We create our own splits and report F1 score weighted by support in a 5-shot setup. •FiQA SA (Maia et al., 2018): The second sentiment analysis task is to predict the aspect-specific sentiment in English financial news and microblog he \n",
            "\n",
            "Summary: public benchmarks include four tasks from the FLUE bench mark and the convFinQA dataset. as LLM performance on most of these financial tasks have not been broadly reported, there is no stand ard testing frame- work. as a result, our claims are restricted to comparisons of LLMs on the external financial tasks. we e valuate on the following tasks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.54:\n",
            "SA (Maia et al., 2018): The second sentiment analysis task is to predict the aspect-specific sentiment in English financial news and microblog he adlines, which were published as a part of the 2018 challenge on financial question answerin g and opinion mining. While the original dataset is annotated on a continuous scale, we discretize the data into a classification setup with negative, neut ral, and positive classes. Like with FPB, we create our own splits including microbl ogs and news, and use a 5-shot setup, reporting weighted F1. 20BloombergGPT GPT-NeoX OPT 66B BLOOM 176B ConvFinQA 43.41 30.06 27.88 36.31 FiQA SA 75.07 50.59 51.60 53.12 FPB 51.07 44.64 48.67 50.25 Headline 82.20 73.22 79.41 76.51 NER 60.82 60.98 57.49 55.56 All Tasks (avg) 62.51 51.90 53.01 54.35 All Tasks (WR) 0.93 0.27 0.33 0.47 Table 8: Results on financial domain tasks. •Headline (Sinha and Khandait, 2020): This is a binary classification task of whether a news headline in the gold commodity domain includes certain informat ion. This human-annotated dataset consists of English news headlines about “gold”. Each news article carries \n",
            "\n",
            "Summary: sentiment analysis task is to predict aspect-specific sentiment in english financial news and microblog he adlines. original dataset is annotated on a continuous scale, but discretized into a classification setup with negative, neut ral, and positive classes. like with FPB, we create our own splits including microbl ogs and news, and use a 5-shot setup, reporting weighted F1.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.55:\n",
            "gold commodity domain includes certain informat ion. This human-annotated dataset consists of English news headlines about “gold”. Each news article carries a subset of the following tags: “price or not”, “price up”, “price down”, “price stable”, “past price”, “future price”, “past general”, “future gener al”, “asset comparison”. We verbalize each tag into a question using the official docu mentation, use 5 shots, and report the average weighted F1 score across all categories. •NER (Salinas Alvarado et al., 2015): This is a named entity recognition task on finan- cial data gathered for credit risk assessment from financial agreements fi led with the SEC. The annotated entity types follow the standard CoNLL format (Tjong Ki m Sang and De Meulder, 2003) and are annotated with PER, LOC, ORG, and MISC. As it is nontrivial to learn to predict empty outputs in few-shot setups, we drop sentences that do not contain any entity. We further drop MISC tags due to their amb iguous definition. All the models required more shots to perform well and we thus selected 20 shots and report the entity-level F1 score. •ConvFinQA (Chen et al., 2022): Given input from S&P 500 earnings reports that includes text and at least one table with financial data, the task is to answ \n",
            "\n",
            "Summary: each news article carries a subset of the following tags: “price or not”, “price up”, “price down”, “price stable”, “past price”, “future price”, “past general”, “asset comparison” we verbalize each tag into a question, use 5 shots, and report the average weighted F1 score across all categories.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.56:\n",
            "., 2022): Given input from S&P 500 earnings reports that includes text and at least one table with financial data, the task is to answ er conver- sational questions that require numerical reasoning over the input. This task requires numerical reasoning, an understanding of structured data and financial c oncepts, and a model needs to relate follow-up questions to the dialog turns. For ConvFinQA, we use an entire gold conversation and its context is used as input to the models. As each “turn” of the conversation concludes, the “turn” along with the answer for that turn is appended as context for future turns. We re port the exact match accuracy on the public development set. BloombergGPT performs best of all models for four of the five tasks (ConvFinQA, FiQA SA, FPB, and Headline) and comes in second in NER (Table 8). Consequen tly, BloombergGPT also has the highest win rate among all the models that we tested. The gap to equally-sized models is especially pronounced for ConvFinQA w hich is challenging due to the requirement to use conversational input to reason over tab les and generate an answer. 21Name Time Tokens Test Size % Pos % Neu % Neg Equity News 2018–2019 150-200 1,000 7 87 6 Equity Social Media 2015–2020 15-20 1,000 10 83 7 Equity Transcript 2008–2020 70-80 800 19 75 6 ES News 2016–2019 100-120 1,000 32 53 15 Country News 2009–2021 50-1,000 1,000 18 60 22 Table \n",
            "\n",
            "Summary: BloombergGPT performs best of all models for four of the five tasks. it also has the highest win rate among all the models that we tested. the gap to equally-sized models is especially pronounced for conver- sational questions that require numerical reasoning over the input. BloombergGPT also has the highest win rate among all the models that we tested.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.57:\n",
            "2020 70-80 800 19 75 6 ES News 2016–2019 100-120 1,000 32 53 15 Country News 2009–2021 50-1,000 1,000 18 60 22 Table 9: An overview of the Bloomberg-internal sentiment analysis task s. Input token and label distribution numbers are computed on the test set. 5.3.2 Internal Task: Sentiment Analysis For the Bloomberg-internal tasks, we consider aspect-specific sent iment analysis, which is prevalent in financial literature. All of the datasets we use are in Engli sh. Our annotation process consists of a discovery phase during which we establish the an- notation and sampling procedures, understand how many annotators are typ ically required per example, and determine the level of training that is needed for t he annotators (Tseng et al., 2020). Depending on the complexity of the task, our annotators are a ded icated team of financial experts at Bloomberg, consultant workers, or a combination of bot h. In each case, ties are resolved by adjudication from additional annotators and ambiguou s examples are excluded. All the datasets in this section were annotated by 2 annotat ors with a third annotator breaking any ties. We measure the performance of LLMs for the internal datasets using a five- shot evalu- ation, similar to the external datasets. As the datasets are large, we random ly sample at most 1k test examples. We report F1 weighted by the support of each label. Note that, \n",
            "\n",
            "Summary: Bloomberg-internal tasks consider aspect-specific sent iment analysis. our annotators are a ded icated team of financial experts at Bloomberg, consultant workers, or a combination of bot h. as the datasets are large, we random ly sample at most 1k test examples. we report F1 weighted by the support of each label.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.58:\n",
            "datasets are large, we random ly sample at most 1k test examples. We report F1 weighted by the support of each label. Note that, similar to the external datasets, it is likely that the unlabeled versions of the data used in our internal datasets occur in FinPile and are therefore seen by BloombergGPT during training. However, since some of FinPile is also available on the web, other LLMs we compare against may have also been trained on unlabeled versions of this d ata. Dataset statistics are provided in Table 9. •Equity News Sentiment : This task is to predict the aspect-specific sentiment ex- pressed in the news story toward a company. The dataset consists of Engl ish news stories from Bloomberg, premium, and web content. Annotations of “positiv e”, “neg- ative”, or “neutral” indicate that the news story is likely to increase, decrease, or not change the long-term investor confidence in the company. •Equity Social Media Sentiment : The task is similar to “Equity News Sentiment” but instead of news, we use financially-relevant English social medi a content. •Equity Transcript Sentiment : This task is also similar to “Equity News Senti- ment” but instead of news, we use transcripts from company press conf erences. The transcripts are made available through the use of speech recognition and at times, human edits. Long transcripts are processed in chunks, and each \n",
            "\n",
            "Summary: the datasets are large, we random ly sample at most 1k test examples. we report F1 weighted by the support of each label. some of the data used in our internal datasets occur in FinPile. other LLMs we compare against may have also been trained on unlabeled versions of this d ata - see table 9.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.59:\n",
            "ences. The transcripts are made available through the use of speech recognition and at times, human edits. Long transcripts are processed in chunks, and each chun k in our dataset typically contains between 70 and 80 tokens. 22BloombergGPT GPT-NeoX OPT 66B BLOOM 176B Equity News 79.63 14.17 20.98 19.96 Equity Social Media 72.40 66.48 71.36 68.04 Equity Transcript 65.06 25.08 37.58 34.82 ES News 46.12 26.99 31.44 28.07 Country News 49.14 13.45 17.41 16.06 All Tasks (avg) 62.47 29.23 35.76 33.39 All Tasks (WR) 1.00 0.00 0.67 0.33 Table 10: Results on internal aspect-specific sentiment analysis datas ets. BloombergGPT far outperforms all other models on sentiment analysis tasks. Name Tokens Test Size LOC ORG PER BFW /tildelow21 500 0.2 1.6 0.0 BN /tildelow30 500 0.7 1.0 0.6 Filings /tildelow32 500 0.1 1.3 0.4 Headlines /tildelow50 500 0.7 2.7 1.0 Premium /tildelow29 500 0.6 1.4 0.3 Transcripts /tildelow23 500 0.6 0.6 0.3 Social Media /tildelow12 500 0.4 1.4 0.2 Table 11: An overview of statistics of our internal \n",
            "\n",
            "Summary: BloombergGPT far outperforms all other models on sentiment analysis tasks. each chun k in our dataset typically contains between 70 and 80 tokens. BloombergGPT far outperforms all other models on sentiment analysis tasks. ences.com is one of the world's largest news and media companies. ences.com is the world's largest news and media company.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.60:\n",
            "low23 500 0.6 0.6 0.3 Social Media /tildelow12 500 0.4 1.4 0.2 Table 11: An overview of statistics of our internal NER test set. We report average number of LOCation, ORGanization, PERson per example. •ES News Sentiment : While this task is to predict the aspect-specific sentiment expressed in the news story towards a company (aspect), the goal is not t o indicate effect on investor confidence. The stories are annotated “positive”, “negati ve”, or “neutral” if the news story contains content that reflects good, bad, or n eutral news about the company’s environmental and social policies. •Country News Sentiment : This task is different from the other sentiment tasks in that the goal is to predict the sentiment expressed in the news s tory towards a country. The dataset consists of English news stories from Bloomberg, p remium, and web content. The stories are annotated “positive”, “negative”, or “neutral” i f the news story alludes to the growth, shrinkage, or status quo of that countr y’s economy. Table 10 shows that across the four internal aspect-specific sentimen t tasksBloombergGPT performs better than all the other tested models, by a wide margin. T he only task in which the models perform similarly is the social media sentiment task, whileBloombergGPT outperforms the other models by at least 25 and up to over 60 points in \n",
            "\n",
            "Summary: BloombergGPT performs better than all the other tested models across the four internal aspect-specific sentimen t tasks. the goal is not t o indicate effect on investor confidence. the only task in which the models perform similarly is the social media sentiment task, whileBloombergGPT outperforms the other models by at least 25 and up to over 100.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.61:\n",
            "the models perform similarly is the social media sentiment task, whileBloombergGPT outperforms the other models by at least 25 and up to over 60 points in the other three. 5.3.3 Exploratory Task: NER Even though NER is a well-established NLP task with state-of-the-art re sults using BERT (Wu and Dredze, 2019; Luoma and Pyysalo, 2020) and T5 (Liu et al., 2022) style models, 23NER is largely an unexplored task for generative LLMs. NER is not in HELM (Liang et al., 2022), there is a single (Polish) task in BIG-bench (Srivastava et al., 2022), and none of the LLM papers we study report NER performance. Hence, we consider NER as an exp loratory task and report preliminary NER results given its importance in the Fi nancial sector. There are a few reasons for why NER may be a difficult task for generative LLMs. NER is an information extraction task, and a better fit for encoder-decod er or encoder-only architectures. The generative nature of LLMs does not confer an advantage for NER. We find that extensive prompt engineering and a greater number of shots are required to obtain reasonable results for NER than for other tasks. Finance-specific NER has s ubtleties that make it especially \n",
            "\n",
            "Summary: NER is largely an unexplored task for generative LLMs. NER is an information extraction task, and a better fit for encoder-decod er or encoder-only architectures. the generative nature of LLMs does not confer an advantage for NER. NER has s ubtleties that make it difficult to obtain reasonable results for NER.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.62:\n",
            "number of shots are required to obtain reasonable results for NER than for other tasks. Finance-specific NER has s ubtleties that make it especially difficult for zero or few-shot learning. For example, consider the (fabricated) headline “Bloomberg: Mr. Musk adds new fea- tures to Twitter and comments on China”. Depending on our annotation guide lines and downstream task needs: (a) the reporting news organization “Bloomberg” c an be tagged or not, depending on whether we want only salient entities, (b) “Mr. Mu sk” or just “Musk” is the PER to be tagged, (c) “Twitter” can be tagged as an ORG or a PRD (produc t) as features are added to the Twitter product and not the organization, and ( d) “China” can be tagged ORG or LOC, though the right tag is likely ORG. Without adding e xtensive annotation guidelines in the prompt, the LLM does not know the intended tagging behavior. Based on preliminary testing, we determined the following settin g to obtain the best performance on the internal NER tasks from all models. First, we restr ict the entity types to be predicted to be ORG, PER, and LOC. In all, we filtered out less t han 1% of entities. We also remove all documents that contain no entities (i.e., all “O”’s ). Both of these modi- fications are intended \n",
            "\n",
            "Summary: finance-specific NER has s ubtleties that make it especially difficult for zero or few-shot learning. without adding e xtensive annotation guidelines in the prompt, the LLM does not know the intended tagging behavior. we restr ict the entity types to be predicted to be ORG, PER, and LOC.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.63:\n",
            ". We also remove all documents that contain no entities (i.e., all “O”’s ). Both of these modi- fications are intended to increase the usefulness of the examples se en in few-shot prompting. We expect that further work on prompt engineering for NER could produc e better results. We consider seven Bloomberg internal NER datasets from different domai ns. •BN NER : This is a named entity recognition task on entities occurring in En glish long-form Bloomberg news content (the “BN wire”) between 2017 to 2020. •BFW NER : Similar to “BN NER” but instead of using the long-form BN wire, we use short-form stories from the “Bloomberg First Word” wire between 2018 to 2020. •Filings NER : The goal of this task is to identify entities that occur in mandatory financial disclosures filed by companies. The dataset contains filings sam pled between 2016 and 2019. •Headlines NER : The goal of this task is to identify entities that occur in headlines of English Bloomberg news content. The dataset contains headlines samp led between 2016 and 2020. •Premium NER : The goal of this task is to identify entities that occur in a subset of the third-party English news content ingested by Bloomberg. The d ataset contains stories sampled between 2019 and 2021. •Transcripts NER : The goal of this task is to identify entities that occur in transcrip ts of company press conferences. The dataset contains \n",
            "\n",
            "Summary: we consider seven Bloomberg internal NER datasets from different domai ns. we remove all documents that contain no entities (i.e., all “O”’s) both of these modi- fications are intended to increase the usefulness of the examples se en in few-shot prompting. we expect that further work on prompt engineering for NER could produce better results.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.64:\n",
            ". •Transcripts NER : The goal of this task is to identify entities that occur in transcrip ts of company press conferences. The dataset contains transcripts from 2019. 24BloombergGPT GPT-NeoX OPT 66B BLOOM 176B NER BFW 72.04 71.66 72.53 76.87 BN 57.31 52.83 46.87 59.61 Filings 58.84 59.26 59.01 64.88 Headlines 53.61 47.70 46.21 52.17 Premium 60.49 59.39 57.56 61.61 Transcripts 75.50 70.62 72.53 77.80 Social Media 60.60 56.80 51.93 60.88 All Tasks (avg) 62.63 59.75 58.09 64.83 All Tasks (WR) 0.57 0.29 0.19 0.95 NER+NED BFW 55.29 34.92 36.73 39.36 BN 60.09 44.71 54.60 49.85 Filings 66.67 31.70 65.63 42.93 Headlines 67.17 36.46 56.46 42.93 Premium 64.11 40.84 57.06 42.11 Transcripts 73.15 23.65 70.44 34.87 Social Media 67.34 62.57 70.57 65.94 All Tasks (avg) 64.83 39.26 58.79 45.43 All Tasks (WR) 0. \n",
            "\n",
            "Summary: the dataset contains transcripts from 2019. NER+NED BFW 55.29 34.92 36.73 39.36 BN 60.09 44.71 54.60 49.85 Filings 66.67 31.70 65.63 42.93 Headlines 67.17 36.46 56.46 42.93 Premium 64.11 40.84 57.06 42.11 Transcripts 75.49 59.39 57.56 61.61 Transcripts 75.50 70.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.65:\n",
            "57 70.57 65.94 All Tasks (avg) 64.83 39.26 58.79 45.43 All Tasks (WR) 0.95 0.00 0.67 0.38 Table 12: Results on internal NER and NED datasets. On NER, while the much l arger BLOOM 176B model outperforms all other models, results from all models are relatively close, with BloombergGPT outperforming the other two models. On NER+NED, BloombergGPT outperforms all other models by a large margin. •Social Media NER : The goal of this task is to identify entities that occur in English financially-relevant social media content. The dataset contains soci al media content sampled between 2009 and 2020. As our datasets are substantive, we randomly sample 4,000 training and 500 test ing ex- amples from each filtered internal dataset. We utilize 20-shot prompts an d evaluate using F1. The results from the internal NER tasks are mixed (Table 12). The muc h larger BLOOM 176Bwins most of the NER tasks. Of the like-sized models, BloombergGPT per- forms the best placing first once (Headlines), second four times (BN, P remium, Transcripts, Social media), third once (BFW), and last once (Filings). Exploratory Task: NER+NED Named entity disambiguation (NED) links entity mentions to known entities in knowledge bases or other structured information sources. Within the financial world, we seek to link text mentions \n",
            "\n",
            "Summary: the much larger BLOOM 176B model outperforms all other models on NER. on NER+NED, BloombergGPT outperforms all other models by a large margin. the goal of this task is to identify entities that occur in financially-relevant social media content. the results from the internal NER tasks are mixed (Table 12)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.66:\n",
            "disambiguation (NED) links entity mentions to known entities in knowledge bases or other structured information sources. Within the financial world, we seek to link text mentions of companies to their ticker sym- bols, an abbreviation that uniquely identifies publicly traded share s of a particular stock on a particular stock market. We directly test the ability of an LLM to complete this task by evaluatin g a joint NER+NED task: identify the stock tickers of companies mentioned in a document. This 25requires the model to first identify company mentions and then gene rate the corresponding stock ticker. For example, given “AAPL announced that they will stop usi ng Intel chips in future products.” the correct NER output would be “AAPL, Intel” whi le the correct NER+NED output would be “AAPL, INTC”. One of the advantages of this task is that it is robust to variations in extrac ting the exact text span. While NER evaluation requires exact matches, ticker s may be successfully produced without first identifying spans. Furthermore, it evalu ates a model’s knowledge of companies, their various surface forms, and company to ticker mappings. We create evaluation data with linked tickers for this task by runnin g a state-of-the- art entity linking system for companies in financial data over the Blo omberg internal NER annotated documents from each domain. We remove documents with no li nked tickers. Fol- lowing our \n",
            "\n",
            "Summary: we seek to link text mentions of companies to their ticker sym- bols. this 25requires the model to first identify company mentions and then gene rate the corresponding stock ticker. while NER evaluation requires exact matches, ticker s may be successfully produced without first identifying spans. we remove documents with no linked tickers.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.67:\n",
            "omberg internal NER annotated documents from each domain. We remove documents with no li nked tickers. Fol- lowing our NER evaluations, we randomly sample 4,000 training and 500 testing examples from each filtered internal dataset. We utilize 20-shot prompts and evalu ate using F1. Table 12 shows that BloombergGPT outperforms all other models by a large margin, except on social media data where it comes in second behind BLOOM 176B. In our social media data, companies are often referenced by their tickers, remov ing the requirement of the model to link the mention and reverting the task to NER. These resu lts further underscore the advantage of BloombergGPT for financial tasks. 5.4 BIG-bench Hard We now turn to evaluate BloombergGPT on standard, general-purpose NLP tasks. While the focus of our model is on financial tasks, our inclusion of general-purp ose training data may help improve not only the financial tasks, but also allow our model t o perform well on more standard NLP datasets. We start with BIG-bench Hard (Suzgun et al., 2022), a subset of the most challenging tasks in BIG-bench (Srivastava et al., 2022). It only includes tasks in which the best available model at construction was unable to ac hieve a performance higher than the average human rater via standard prompting techniques. Results for each task are shown in Table 13. \n",
            "\n",
            "Summary: omberg internal NER annotated documents from each domain. we remove documents with no li nked tickers. we then evaluate BloombergGPT on standard, general-purpose NLP tasks. our model outperforms all other models except on social media data. a new version of the BIG-bench test is available.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.68:\n",
            "unable to ac hieve a performance higher than the average human rater via standard prompting techniques. Results for each task are shown in Table 13. Overall, while BloombergGPT falls behind the much larger PaLM 540B (10x parameters) and BLOOM 176B (3.5x parameters), it is the best-performing among similarly sized models. In fact, i ts performance is closer to BLOOM 176Bthan it is to either GPT-NeoX or OPT 66B. It further achieves the best perfor- mance of all models in date understanding, hyperbaton (ordering of adjec tives), and tracking shuffled objects. In sum, according to this benchmark, we find that de veloping finance- specificBloombergGPT did not come at the expense of its general-purpose abilities. 5.5 Knowledge Assessments We next assess knowledge, which we define as the ability to recall in formation seen during model training, via scenarios that have the model answer questions w ithout providing addi- tional context or resources (closed-book question answering). This in cludes multiple-choice questions, and we report accuracy. We follow the template of Brown et al. (2020). The list of scenarios is as follows: •ARC (Clark et al., 2018): Multiple-choice questions collected from 3rd to 9th grade science exams, includes easy and challenging splits. 26BIG-bench Hard Task BloombergGPT GPT-NeoX OPT 66 \n",
            "\n",
            "Summary: BloombergGPT falls behind the much larger paLM 540B (10x parameters) and BLOOM 176B (3.5x parameters) but it is the best-performing among similarly sized models. it achieves the best perfor- mance of all models in date understanding, hyperbaton, and tracking shuffled objects. we next assess knowledge, which we define as the ability to recall in formation seen during model training.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.69:\n",
            "9th grade science exams, includes easy and challenging splits. 26BIG-bench Hard Task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B PaLM 540B Boolean Expressions62.40 71.20 48.40 69.20 83.2 Causal Judgement 49.73 52.41 51.87 51.87 61.0 Date Understanding 54.80 45.60 49.60 50.00 53.6 Disambiguation QA 34.00 40.80 40.40 40.40 60.8 Dyck Languages15.60 26.00 14.80 42.00 28.4 Formal Fallacies 50.80 52.80 54.00 52.80 53.6 Geometric Shapes15.20 8.00 11.60 22.40 37.6 Hyperbaton 92.00 92.00 91.60 92.00 70.8 Logical Deduction(avg) 34.53 30.93 31.87 34.00 60.4 Movie Recommendation 90.40 86.40 91.20 91.20 87.2 Multi-Step Arithmetic[Two] 1.20 0.40 0.40 0.00 1.6 Navigate42.00 45.20 42.00 50.00 62.4 Object Counting33.20 21.20 26.00 36.80 51.2 Penguins in a Table 37.67 33.56 28.08 40.41 44.5 Reasoning about Colored Objects 34.80 26.00 31.20 36.80 38.0 Ruin Names 56.00 54.00 52.80 \n",
            "\n",
            "Summary: 9th grade science exams, includes easy and challenging splits. 26BIG-bench hard task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B PaLM 540B Boolean Expressions62.40 71.20 48.40 69.20 83.2 Causal Judgement 49.73 52.41 51.87 51.87 61.0 Date Understanding 54.80 45.60 49.60 50.00 53.6 Disambiguation QA 34.00 40.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.70:\n",
            "44.5 Reasoning about Colored Objects 34.80 26.00 31.20 36.80 38.0 Ruin Names 56.00 54.00 52.80 54.80 76.0 Salient Translation Error Detection 20.00 20.40 16.40 23.60 48.8 Snarks 69.66 62.36 69.66 72.47 78.1 Sports Understanding 62.80 53.20 54.40 53.20 80.4 Temporal Sequences29.20 21.20 23.60 36.80 39.6 Tracking Shuffled Objects(avg) 25.33 24.53 24.00 23.47 19.6 Web of Lies49.20 52.40 54.00 51.20 51.2 Word Sorting4.80 5.20 2.40 7.60 32.0 NLP Task (avg) 54.39 51.63 52.60 54.96 62.7 Algorithmic Task(avg) 28.42 27.84 25.37 33.95 40.9 All Tasks (avg) 41.97 40.25 39.58 44.91 52.3 All Tasks (WR) 0.57 0.45 0.39 0.75 - Table 13: BIG-bench hard results using standard 3-shot prompting. Follo wing the conven- tion from Suzgun et al. (2022), we denote algorithmic tasks with the supersc ript, and present averages for NLP and algorithmic categories. The baseline numbe rs from PaLM 540B(Chowd \n",
            "\n",
            "Summary: BIG-bench hard results using standard 3-shot prompting. the baseline numbe rs from paLM 540B(chowdhury et al., 2000) are shown in table 13. the algorithmic tasks are denoted with the supersc ript, and present averages for NLP and algorithmic categories.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.71:\n",
            "ript, and present averages for NLP and algorithmic categories. The baseline numbe rs from PaLM 540B(Chowdhery et al., 2022) are taken from the original BBH paper. •CommonsenseQA (Talmor et al., 2019): Multiple-choice QA dataset that requires different types of commonsense knowledge. •MMLU (Hendrycks et al., 2021): Manually collected multiple-choice knowled ge ques- tions in 57 subjects. •PhysicalQA (PiQA, Bisk et al., 2020): Questions about how the physical world works. BloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, and OPT66Bin one task, and comes second in the other three (Table 14). Similar to the previous section, it outperforms models of similar size while almost being on par with the much larger models. The Massive Multitask Language Understanding (MMLU, He ndrycks et al., 2021) covers 57 different subjects and thus has a much wider cove rage than the tasks described above. The aggregated results in Table 15 paint a more consiste nt picture and follow the insights seen in BIG-bench hard. BloombergGPT consistently outperforms OPT66B, which in turn outperforms GPT-NeoX, while GPT-3 performs best. In contrast 27Task BloombergGP \n",
            "\n",
            "Summary: BloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, and OPT66Bin one task, and comes second in the other three. BloombergGPT consistently outperforms OPT66B, which in turn outperforms GPT-NeoX, while GPT-3 performs best.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.72:\n",
            "66B, which in turn outperforms GPT-NeoX, while GPT-3 performs best. In contrast 27Task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3 ARC (easy) 73.99 70.79 71.25 75.93 71.2 ARC (challenging) 48.63 45.39 44.54 50.85 53.2 CommonsenseQA 65.52 60.36 66.42 64.21 - PiQA 77.86 75.84 77.58 77.04 80.5 All Tasks (avg) 66.50 63.10 64.95 67.01 - All Tasks (WR) 0.75 0.08 0.33 0.67 - Table 14: Knowledge tasks 1-shot results. The baseline numbers from GP T-3 are taken from Brown et al. (2020). Among all models, BloombergGPT achieves the highest win rate among the models we ran ourselves, and performs second best on average. Model BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3 Humanities 36.26 32.75 33.28 34.05 40.8 STEM 35.12 33.43 30.72 36.75 36.7 Social Sciences 40.04 36.63 38.32 41.50 50.4 Other 46.36 42.29 42.63 46.48 48.8 Average 39.18 35.95 35.99 39.13 43.9 Table 15: Results (5-shot) on the MM \n",
            "\n",
            "Summary: BloombergGPT achieves the highest win rate among the models we ran ourselves, and performs second best on average. 66B outperforms GPT-NeoX, while GPT-3 performs best. the baseline numbers from GP T-3 are taken from brown et al. (2020) the baseline numbers from GP T-3 are taken from brown et al. (2020)\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.73:\n",
            "42.63 46.48 48.8 Average 39.18 35.95 35.99 39.13 43.9 Table 15: Results (5-shot) on the MMLU (Hendrycks et al., 2021) benchmark. Th e base- line numbers from GPT-3 are taken from Hendrycks et al. (2021). While BloombergGPT lacks behind BLOOM 176Bon three of the categories, its aver- age is the highest among all models we evaluated ourselves. The gap to GPT- 3 is largest on social sciences while the performance in other categories is close. to the previous sections, BloombergGPT also outperforms BLOOM 176Bin this category, although by a slim margin. It falls behind the reported performance of GP T-3, especially in the social science category. The gap to GPT-3 is closest in the STEM and “Other” domains which include finance and accounting-related questions. 5.6 Reading Comprehension We define reading comprehension benchmarks as tasks in which the mod el can generate the correct response based on information contained in the presented inpu t text. Our grouping includes open-book QA tasks, as opposed to Brown et al. (2020), who separate them into a different categories. We follow the template of Brown et al. (2020), and re port accuracy. We include the following tasks: •BoolQ (Clark et al., 2019): Yes/No questions about a passage from Wikipedia. •OpenBookQA (Mihaylov \n",
            "\n",
            "Summary: BloombergGPT lacks behind BLOOM 176Bon three of the categories, but its aver- age is the highest among all models we evaluated ourselves. the gap to GPT-3 is largest on social sciences while the performance in other categories is close. the gap to GPT-3 is closest in the STEM and “Other” domains which include finance and accounting-related questions.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.74:\n",
            "lQ (Clark et al., 2019): Yes/No questions about a passage from Wikipedia. •OpenBookQA (Mihaylov et al., 2018): Multiple-choice elementary-level scienc e questions, given a book of science facts, applied to new situations. •RACE (Lai et al., 2017): A multiple choice dataset of middle and high school English examinations. 28RC Scenario BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3 BoolQ 74.59 46.36 57.46 52.94 76.7 OpenBookQA 51.60 44.20 58.00 47.20 58.8 RACE (middle) 54.32 41.23 47.42 52.30 57.4 RACE (high) 41.74 34.33 37.02 39.14 45.9 MultiRC 62.29 22.86 18.80 26.65 72.9 ReCoRD 82.79 67.86 82.53 78.01 90.2 All Tasks (avg) 61.22 42.81 50.21 49.37 67.0 All Tasks (WR) 0.94 0.06 0.50 0.50 - Table 16: Reading Comprehension Results (1-shot). The baseline numbe rs from GPT-3 are taken from Brown et al. (2020). BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. •Multi-Sentence \n",
            "\n",
            "Summary: BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. •OpenBookQA: multiple-choice elementary-level science questions, given a book of science facts, applied to new situations. •RACE: a multiple choice dataset of middle and high school english examinations. BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.75:\n",
            "et al. (2020). BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. •Multi-Sentence Reading Comprehension (MultiRC, Khashabi et al., 2018): Short paragraphs and multi-sentence questions. •Reading Comprehension with Commonsense Reasoning (ReCoRD, Zhang et al., 2018): Automatically generated questions about CNN and Daily Mail news articles. Table 16 reflects a similar ranking as in the above evaluations: While GP T-3 has the highest performance, BloombergGPT is a close second. Except for OpenBookQA, The performance of BloombergGPT is the highest among BLOOM 176B, GPT-NeoX, and OPT66B. Surprisingly, BLOOM 176B falls behind significantly in this category. 5.7 Linguistic Tasks We define as linguistic tasks those scenarios that are not directly conne cted to user-facing applications. These include tasks that evaluate disambiguation, grammar, or entailment. These tasks are designed to directly assess a model’s ability to un derstand language. We follow the template of Brown et al. (2020), and report accuracy. The list of t asks is as follows: •Recognizing Textual Entailment (RTE, Dagan et al., 2007; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009): \n",
            "\n",
            "Summary: BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. except for openbookQA, the performance of BloombergGPT is the highest among BLOOM 176B, GPT-NeoX, and OPT66B. we follow the template of brown et al. (2020), and report accuracy.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.76:\n",
            "Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009): Given two text fragments, i dentify whether the meaning of one text is entailed. •Adversarial NLI (ANLI, Nie et al., 2020): Adversarially constructed entailment detection. •CommitmentBank (CB, De Marneffe et al., 2019): Naturally occurring discourses whose final sentence contains a clause-embedding predicate. •Choice of Plausible Alternatives (COPA, Gordon et al., 2011): Premise and two alternatives, where the task is to select the alternative that more p lausibly has a causal relation with the premise. •Words in Context (WIC Pilehvar and Camacho-Collados, 2019): Determine if a word is being used with the same meaning in two sentences. 29Linguistic Scenario BloombergGPT GPT-NeoX OPT 66B BLOOM 176B GPT-3 RTE 69.31 53.79 54.87 57.40 70.4 ANLI Round 1 32.90 32.60 33.10 33.60 32.0 ANLI Round 2 34.40 33.80 34.20 33.80 33.9 ANLI Round 3 37.33 36.17 34.92 35.17 35.1 CB 53.57 48.21 44.64 \n",
            "\n",
            "Summary: given two text fragments, i dentify whether the meaning of one text is entailed. •Adversarial NLI (ANLI, Nie et al., 2020): Adversarially constructed entailment detection. •CommitmentBank (CB): Naturally occurring discourses whose final sentence contains a clause-embedding predicate.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.77:\n",
            "33.80 33.9 ANLI Round 3 37.33 36.17 34.92 35.17 35.1 CB 53.57 48.21 44.64 48.21 64.3 COPA 86.00 88.00 86.00 84.00 87.0 WIC 52.51 50.00 52.51 50.16 48.6 WinoGrad 80.95 79.12 82.78 78.02 89.7 WinoGrande 64.09 60.62 66.14 67.01 73.2 HellaSWAG 73.92 68.37 73.47 73.21 78.1 StoryCloze 80.87 78.30 81.83 80.28 84.7 All Tasks (avg) 60.63 57.18 58.59 58.26 63.4 All Tasks (WR) 0.85 0.27 0.58 0.42 - Table 17: Results on the Linguistic Scenarios (1-shot). The baseline num bers from GPT- 3 are taken from Brown et al. (2020). Win rates and averages are computed only based on accuracy numbers. BloombergGPT consistently scores highest among the models we evaluate, achieving an 85% win rate. •Winograd (Levesque et al., 2011): Determine which word a pronoun refers to when it is semantically unambiguous. •Winogrande (Sakaguchi et al., 2019): Adversarially mined challenging Winograd examples. •HellaSWAG ( \n",
            "\n",
            "Summary: ANLI Round 3 37.33 36.17 34.92 35.17 35.1 CB 53.57 48.21 44.64 48.21 64.3 COPA 86.00 88.00 86.00 84.00 87.0 WIC 52.51 50.00 52.51 50.16 48.6 WinoGrad 80.95 79.12 82.78 78.02 89.7 WinoGrande 64.09 60.62 66.14 67.01 73.2 Hella\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.78:\n",
            "ogrande (Sakaguchi et al., 2019): Adversarially mined challenging Winograd examples. •HellaSWAG (Zellers et al., 2019): Pick the best ending to a story or set of instruc - tions. •StoryCloze (Mostafazadeh et al., 2016): Select the correct ending sentence for five- sentence long stories. The results (Table 17) for linguistic tasks follow a similar trend to t he knowledge category. BloombergGPT falls slightly behind GPT-3 and outperforms the other models. Simi lar to the reading comprehension category, BLOOM 176B falls behind BloombergGPT. 5.8 Summary Across dozens of tasks in many benchmarks a clear picture emerges. Among t he models with tens of billions of parameters that we compare to, BloombergGPT performs the best. Furthermore, in some cases, it is competitive or exceeds the performance of much larger models (hundreds of billions of parameters). While our goal for BloombergGPT was to be a best-in-class model for financial tasks, and we included gen eral-purpose training data to support domain-specific training, the model has still attained abilities on general- purpose data that exceed similarly sized models, and in some cases mat ch or outperform much larger models. 30Input : Get me the last price and market cap for Apple Output :get(px_last,cur_mkt \n",
            "\n",
            "Summary: linguistic tasks follow a similar trend to t he knowledge category. BloombergGPT falls slightly behind GPT-3 and outperforms the other models. in some cases, BloombergGPT is competitive or exceeds the performance of much larger models. the goal for BloombergGPT was to be a best-class model for financial tasks, but it has still attained abilities on general-purpose data that exceed similarly sized models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.79:\n",
            "much larger models. 30Input : Get me the last price and market cap for Apple Output :get(px_last,cur_mkt_cap) for([’AAPL US Equity’]) Input : Tesla price Output :get(px_last) for([’TSLA US Equity’]) Input : Get the yield and spread for EC527035 Corp and AL580550 Corp Output :get(yield,spread) for([’EC527035 Corp’,’AL580550 Corp’]) Input : apple and ibm market cap and eps Output :get(cur_mkt_cap,is_eps) for([’AAPL US Equity’,’IBM US Equity’]) Input : industry subgroup of ibm apple microsoft google Output :get(industry_subgroup()) for([’AAPL US Equity’,’IBM US Equity’, ’MSFT US Equity’,’GOOGL US Equity’]) Figure 4: Using BloombergGPT to generate valid Bloomberg Query Language. Using only a few examples in a few-shot setting, the model can utilize its knowledge about stock tickers and financial terms to compose valid queries to ret rieve the data, given a request in natural language. In each case, the model is given 3 examples (not shown) followed by the ‘Input” and a prompt of “Output: ”. \n",
            "\n",
            "Summary: the model can compose valid queries to ret rieve the data, given a request in natural language. Using only a few examples in a few-shot setting, the model can utilize its knowledge about stock tickers and financial terms. the model is given 3 examples (not shown) followed by the “Input” and a prompt of “Output”\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.80:\n",
            "natural language. In each case, the model is given 3 examples (not shown) followed by the ‘Input” and a prompt of “Output: ”. 6 Qualitative Samples We now share qualitative examples from our model that highlight the ben efits of our domain specialization. Generation of Bloomberg Query Language. One use case for BloombergGPT is to make interactions with financial data more natural. An existing way to re trieve data is via the Bloomberg Query Language (BQL). BQL can be used to interact with diff erent classes of securities, each with its own fields, functions, and parameters. BQL is an incredibly powerful but complex tool. As we show in Figure 4, BloombergGPT can be utilized to make BQL more accessible by transforming natural language queries into v alid BQL. Suggestion of News Headlines. Other use cases that are well supported are in the news space. Since it is trained on many news articles, it can be used for many news applications and assist journalists in their day-to-day work. For example, when con structing newsletters, journalists may have to write short headlines for each new section. W hile a dedicated model to help with this task may be too expensive to maintain, BloombergGPT performs well out of the box (Figure 5). Financial Question Answering. Due to the financial domain training data, we are able to query BloombergGPT for knowledge relevant to the financial world. For example, it performs well at identifying the CEO of a company. Figure 6 shows sev eral examples in \n",
            "\n",
            "Summary: BloombergGPT can be used to make interactions with financial data more natural. Bloomberg Query Language (BQL) can be used to interact with diff erent classes of securities. BloombergGPT can be used for many news applications and assist journalists in their day-to-day work. it performs well out of the box at identifying the CEO of a company.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.81:\n",
            "for knowledge relevant to the financial world. For example, it performs well at identifying the CEO of a company. Figure 6 shows sev eral examples in- cluding output from other models. While BloombergGPT correctly identifies the CEOs, GPT-NeoX does not, and FLAN-T5-XXL completely fails, consistently ignoring the com- 31Input : The US housing market shrank in value by $2.3 trillion, or 4.9%, in the second half of 2022, according to Redfin. That’s the largest drop in percentage terms since the 2008 housing crisis, when values slumped 5.8% during the same period. Output : Home Prices See Biggest Drop in 15 Years Input : The global economy is in a better place today than many predicted month s ago, Janet Yellen said at the G20. At home, she pointed to a resilient US econom y, where headline inflation has moderated and the labor market is strong. She also cal led for the IMF to move swiftly toward a fully-financed program for Ukraine. Output : Yellen Sees Global Economy More Resilient Than Expected Input : Google was sued by the US and eight states seeking the breakup of its ad-tech business for allegedly monopolizing the digital advertising m arket. The lawsuit is the Biden administration’s first big challenge to a tech titan and on e of the rare times since 1982 that the DOJ has sought to cleave up a major company \n",
            "\n",
            "Summary: the housing market shrank in value by $2.3 trillion in the second half of 2022. that's the largest drop in percentage terms since the 2008 housing crisis. the global economy is in a better place today than many predicted months ago. google is sued by the US and eight states seeking the breakup of its ad-tech business. a new report says the u.s. economy is in a better place today than many predicted.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.82:\n",
            "s first big challenge to a tech titan and on e of the rare times since 1982 that the DOJ has sought to cleave up a major company. Output : Google Sued for Monopoly in Online Ad Market Figure 5: Using BloombergGPT to generate short headline suggestions in a three-shot setting. Bloomberg News sends many newsletters a day that require s these head- lines.BloombergGPT could help with the editing process by suggesting initial headlines from the text. pany and instead predicting the CEO at Cirrus Logic who was included in the prompt. WhileBloombergGPT does not perfectly solve this task and makes mistakes, we were not able to find any example where the other models solved the task whil eBloombergGPT did not. 7 Related Work Language Models. Language modeling has a long history in the NLP community. The idea of training a probabilistic language model for scoring word sequenc es was likely first introduced by Jelinek (1976). N-gram models were popular for decades (Br own et al., 1992), and were trained on corpora up to 2 trillion tokens (Brants et al., 2007). R esearch on training language models accelerated over the last decade due to inno vations in machine learning, data availability, and compute. Early work in autoregressive l anguage modeling (e.g., Mikolov et al., 2010; Sutskever et al., 2011) used recurrent neural \n",
            "\n",
            "Summary: s first big challenge to a tech titan and on e of the rare times since 1982 that the DOJ has sought to cleave up a major company. s first big challenge to a tech titan and on e of the rare times since 1982 that the DOJ has sought to cleave up a major company. s first big challenge to a tech titan and on e of the rare times since 1982 that the DOJ has sought to cleave up a major company\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.83:\n",
            "(e.g., Mikolov et al., 2010; Sutskever et al., 2011) used recurrent neural n etworks, but these were small models trained on small datasets. The introduction of the transformer architecture (Vaswani et al., 2017) facilitated the scaling of these mode ls in terms of data, compute, and the number of parameters. The process of developing models that could better approximate the distribution of language over large corpora led to the discovery that the representation s these models produce are useful starting points for many downstream tasks. This w as demonstrated by Radford et al. (2018) and Howard and Ruder (2018) who showed that generative pretr aining 32Input : CEO of Assicurazioni Generali? BloombergGPT : Philippe Donnet GPT-NeoX : Antonio De Lorenzo, Simone Gambarini, Enrico Zanetti FLAN-T5-XXL : John M Forsyth, Christopher K Peters, empty string  Input : CEO of Silicon Valley Bank? BloombergGPT : Greg Becker GPT-NeoX : John Hodor, Mike Fridgen, Peter R Briger FLAN-T5-XXL : Christian Klein, John M Forsyth, John M Forsyth Input : CEO of IHS Markit Ltd? BloombergGPT : Lance Uggla GPT-NeoX : John F Lech \n",
            "\n",
            "Summary: the process of developing models that could better approximate the distribution of language over large corpora led to the discovery that the representations these models produce are useful starting points for many downstream tasks. generative representations are useful starting points for many downstream tasks. the process of developing models that could better approximate the distribution of language over large corpora led to the discovery that the representations these models produce are useful starting points.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.84:\n",
            "th Input : CEO of IHS Markit Ltd? BloombergGPT : Lance Uggla GPT-NeoX : John F Lechleiter, David Steward, Niall O Broin FLAN-T5-XXL : William M Hawkins, John M Forsyth, John M Forsyth Input : CEO of Walgreens Boots Alliance? BloombergGPT : Rosalind Brewer GPT-NeoX : Mark V Dei, Stefano Pessina, Stefano Pessina FLAN-T5-XXL : Christian Klein, John M Forsyth, John M Forsyth Input : CEO of Citigroup Inc? BloombergGPT : Jane Fraser GPT-NeoX : Michael L Corbat, Michael L Corbat, Michael L Corbat* FLAN-T5-XXL : Christian Sewing, John M Forsyth, John M Forsyth Figure 6: Testing the ability of BloombergGPT, GPT-NeoX, and FLAN-T5-XXL to re- call the names of CEOs of companies. Each model is run in a 10-shot setting. We sample up to three answers and present all of them if they are incorr ect. *Michael Corbat was CEO of Citigroup until 2021, highlighting the importance of an up-to-date model. with an autoregressive language modeling objective achieves strong per formance in transfer learning. Rad \n",
            "\n",
            "Summary: each model is run in a 10-shot setting. we sample up to three answers and present all of them if they are incorr ect. each model is run in a 10-shot setting. we sample up to three answers and present all of them if they are incorr ect. if a model does not produce a correct answer, it is discarded.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.85:\n",
            "21, highlighting the importance of an up-to-date model. with an autoregressive language modeling objective achieves strong per formance in transfer learning. Radford et al. (2019) further showed scaling the model size and training data led to autoregressive language models that perform well in different downs tream tasks without any additional supervised fine-tuning. Brown et al. (2020) showed that further scaling the models led to the e mergence of new model capabilities and increased model robustness. Since the rel ease of GPT-3 by Brown et al. (2020), many other researchers built large language models to study dat a quantity, data quality, network architecture, parameter scaling, data scaling, t okenization, and open- sourcing strategies (Raffel et al., 2020; Zhang et al., 2022a; Black et al., 2022; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Lieber et al., 2021; Zeng et al., 2022; Tafjord and Clark, 2021; Smith et al., 2022; Scao et al., 2022; Taylor et al., 2022; Lin et al., 332022; Soltan et al., 2022; Thoppilan et al., 2022; Bao et al., \n",
            "\n",
            "Summary: autoregressive language models achieve strong per formance in transfer learning. many other researchers built large language models to study dat a quantity, data quality, network architecture, parameter scaling, data scaling, t okenization, and open- sourcing strategies. a large language model can be used to study data quality, network architecture, parameter scaling, data scaling, t okenization, and open-sourcing strategies.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.86:\n",
            "22; Soltan et al., 2022; Thoppilan et al., 2022; Bao et al., 2022; Sanh et al., 2022; Roller et al., 2021; Glaese et al., 2022; Wang et al., 2021; Peng et al., 2022, among many others). Domain-Specific Large Language Models. The value of domain-specific training for masked (encoder only) language models is well established. Commonly ac cepted approaches are to train BERT models (Devlin et al., 2019) from scratch on domain-speci fic data or to continue pretraining an existing model on new domain-specific data ( Gururangan et al., 2020). Following these strategies, BioBERT (Lee et al., 2020) adapts BERT to th e biomed- ical domain and SciBERT is trained on scientific publications (Beltagy e t al., 2019). The results of these papers showed that in-domain training allows model s to outperform previ- ous state-of-the-art models in a variety of biomedical text mining tas ks. Further examples of this paradigm are ClinicalBERT for the clinical domain (Huang et al., 2019), B ioMed- RoBERTa for scientific biomedical papers (Gururangan et al \n",
            "\n",
            "Summary: in-domain training allows model s to outperform previ- ous state-of-the-art models in a variety of biomedical text mining tasks. the value of domain-specific training for masked (encoder only) language models is well established. in-domain training can also be used to train existing models on new domain-specific data.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.87:\n",
            "ang et al., 2019), B ioMed- RoBERTa for scientific biomedical papers (Gururangan et al., 2020), and BERT weet and Bernice for Twitter data (Nguyen et al., 2020; DeLucia et al., 2022). Since the training of auto-regressive—decoder-only—language models of m ore than 10B parameters is significantly more costly than training masked LMs under 1B parameters, there have been much fewer examples of domain-specific autoregressi ve models. However, existing approaches follow the same two strategies. Adapting an existi ng model, medPaLM (Singhal et al., 2022) adapted PaLM to the biomedical domain and Minerva (Lewkow ycz et al., 2022) to mathematical reasoning tasks. Recently, several examples of from-scratch trained decoder-only m odels for domain- specific data have emerged. One popular domain is protein sequences s ince they can be represented using language-like sequences but are not covered by n atural language mod- els (e.g., Lin et al., 2022; Xiao et al., 2021; Nijkamp et al., 2022). However, there can b e benefits even for models in natural language domains. Galactica is trained exclusively on a large \n",
            "\n",
            "Summary: decoder-only language models of more than 10B parameters are more costly to train. several examples of from-scratch trained decoder-only m odels for domain-specific data have emerged. one popular domain is protein sequences s ince they can be represented using language-like sequences. however, there can b e benefits even for models in natural language domains.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.88:\n",
            "t al., 2022). However, there can b e benefits even for models in natural language domains. Galactica is trained exclusively on a large collection of scientific datasets, and includes special proces sing to handle scientific notations (Taylor et al., 2022). While performing very well on scientific tasks, Galactica also surprisingly also performs well on more standard NLP tasks. BioGPT (Lu o et al., 2022) and BioMedLM (Bolton et al., 2023) are both smaller GPT-style models traine d on biomedical data. Lehman et al. (2023) compares encoder/decoder models train ed exclusively on domain-specific data, versus those adapted from general-purpose traini ng. Researchers working on large generative language dialog models have reached similar conc lusions about the benefits of using domain-specific training data (Zhang et al., 2020; Rol ler et al., 2021; Thoppilan et al., 2022). These findings highlight the advantages of in-domain pretraining, especi ally if sufficient data is available, as it is in our case. Inspired by the general capabilit ies of Galactica, we augment our private data with public data with the goal of investigating wh ether a model can gain in-domain capabilities without sacrificing general-domain perfor mance. Training Data. Large corpora of raw text data are critical for training LLM \n",
            "\n",
            "Summary: taylor et al. trained galactica exclusively on a large collection of scientific datasets. while performing very well on scientific tasks, galactica also performs well on more standard NLP tasks. we augment our private data with public data with the goal of investigating wh ether a model can gain in-domain capabilities without sacrificing general-domain performance.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.89:\n",
            "can gain in-domain capabilities without sacrificing general-domain perfor mance. Training Data. Large corpora of raw text data are critical for training LLMs. As a result, there are now several corpora available that cover a wide range of sources. The Colossal Clean Crawled Corpus (C4, Raffel et al., 2020) draws from Common Cr awl to create a processed training corpus. The Pile is a carefully cur ated corpus that contains a wide range of data sources (Gao et al., 2021). These datasets are built on or inc lude web crawls (OpenWebText2) augmented with an array of data from high-qu ality sources (Pubmed, Arxiv). Various efforts aim to clean datasets, especially web data, by removing 34unwanted or harmful text (Touvron et al., 2023; Rae et al., 2020). BLOOM (Scao et al., 2022) carefully selected data sources and included various filtering me chanisms (Jernite et al., 2022). While web data is an effective strategy for obtaining large amounts of diver se data, robust cleaning efforts still result in data artifacts, duplicates ( Carlini et al., 2020), various types of toxic language (Welbl et al., 2021), and it can lead to unintended mar ginalization of minority voices (Xu \n",
            "\n",
            "Summary: large corpora of raw text data are critical for training LLMs. there are several corpora available that cover a wide range of sources. these datasets are built on or include web crawls augmented with an array of data sources. clean datasets still result in data artifacts, duplicates, and toxic language -.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.90:\n",
            "toxic language (Welbl et al., 2021), and it can lead to unintended mar ginalization of minority voices (Xu et al., 2021). Dodge et al. (2021) studied C4 to better un derstand the metadata, and the included and excluded data. Their findings suggest that C4 contains machine-generated text, is biased due to exclusion filters and might contain examples drawn from evaluation datasets for NLP tasks. A similar effort was undertaken by Zen g et al. (2022) to document the pre-processing they undertook to train thei r Chinese large language model. Lee et al. (2022a) investigated the impact of deduplication on model performan ce for several datasets and found that deduplication reduces the emission of m emorized training data, allows better estimation of the generalization error, and improves training time and cost without impacting performance. These insights highlight the im portance and challenges of constructing high-quality training corpora. As discussed in 2, Bloomberg’s core business curates and provides access to datasets, which we use to construct a high-quality dataset FinPile to trainBloombergGPT, resulting in best-in-class financial performance. Evaluation. The tasks addressed by language models have vastly increased and requi re a very different evaluation process from traditional task-specific sys tems. There have been two paradigms for LLM evaluation: The first is to evaluate a model in many differe nt scenarios via \n",
            "\n",
            "Summary: the tasks addressed by language models have vastly increased and requi re a very different evaluation process from traditional task-specific sys tems. the second paradigm is to evaluate a model in many differe nt scenarios via a sys tem called a sys tem test. a sys tem called a sys tem test aims to evaluate a model in many differe nt scenarios via a sy\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.91:\n",
            "sys tems. There have been two paradigms for LLM evaluation: The first is to evaluate a model in many differe nt scenarios via automatic evaluation (Liang et al., 2022; Srivastava et al., 2022) and the second is to perform extrinsic and task-specific evaluations by integrating them into user workflows (e.g., Lee et al., 2022b; Goyal et al., 2022). While the second strategy is necessary for assessing deployments of models in products, it is infeasible to run these human evaluations at a scale of the first st rategy and it is thus standard to follow the first strategy when introducing new models. In our case, we combine multiple general-purpose evaluations from multiple existing benc hmarks that have different goals. Srivastava et al. (2022) aim for maximum coverage by soliciting tasks fr om the entire research community, while HELM (Liang et al., 2022) suggests evaluati on in various “scenarios” that are represented through specific datasets. Earlier lan guage model papers developed their own evaluation schemata (Brown et al., 2020). While the se benchmarks allow for a side-by-side comparison between models, it is challengi ng to ensure that all experimental parameters (prompts, decoding strategies, few-shot examples, etc \n",
            "\n",
            "Summary: in our case, we combine multiple general-purpose evaluations from multiple existing benc hmarks that have different goals. while the se benchmarks allow for a side-by-side comparison between models, it is challengi ng to ensure that all experimental parameters (prompts, decoding strategies, few-step decoding, etc.) are controlled for.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.92:\n",
            "comparison between models, it is challengi ng to ensure that all experimental parameters (prompts, decoding strategies, few-shot examples, etc.) are the same. For that reason, we differentiate between reported and verified n umbers in our evaluation ( 5). Beyond the general-purpose evaluation, we also require a targeted domain evaluation. Prior domain-specific models like Galactica (Taylor et al., 2022) chose a s et of tasks that the model is likely to perform well on. In their case, these were v arious scientific tasks. However, there exists no standard benchmark for the financial NLP domain. While the recent work on FLUE (Shah et al., 2022) aims to provide such a benchmark, it h as limited coverage of relevant tasks, no suggested evaluation strategy for few-shot l earning, and the quality of some annotations is low. To provide externally comparable res ults, we developed 35a few-shot strategy for FLUE, but also decided to augment the publicly a vailable evaluation tasks with company-internal benchmarks. Model Size. Large language model training remains expensive in terms of the compu- tational cost and human effort to assemble data and train the model. Determ ining the optimal amount of training data and model shape and size for the best utili zation of re- sources becomes important. Kaplan et al. (2020) first studied the dependence of language model performan ce on architecture, parameter size, compute power \n",
            "\n",
            "Summary: there exists no standard benchmark for the financial NLP domain. to provide externally comparable res ults, we developed 35a few-shot strategy for FLUE. large language model training remains expensive in terms of the compu- tational cost and human effort to assemble data and train the model. the optimal amount of training data and model shape and size for the best utili zation of re- sources becomes important.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.93:\n",
            "re- sources becomes important. Kaplan et al. (2020) first studied the dependence of language model performan ce on architecture, parameter size, compute power, and dataset size. The y reported that the number of model parameters, the dataset size, and the amount of compute i mproves perfor- mance on the autoregressive language modeling objective smoothly accordi ng to the power law. A similar investigation by Hernandez et al. (2021) into data transfer f or differing dis- tributions found that this also follows a power law. Moving beyond s tudying the effect on loss, Rae et al. (2021) analyzed the effect of scale on undesirable properties such as bias and toxicity by training a wide range of model sizes. Comparing model architectures, Levine et al. (2020) studied the scalin g of models that use self-attention and derived guidelines for depth-to-width allo cation. Tay et al. (2021) reported that model shape (depth-width ratio) impacted performance on downstream tasks even if it had minimal impact on the pretraining objective. Tay et al. (2022a) further studied the effect of scaling for different model architectures and showed that architecture choice is pertinent when scaling and that the vanilla transformer arc hitecture scales best. Of particular importance to this work is the study of Hoffmann et al. (2022), wh o inves- tigated the effect of model size and the number of \n",
            "\n",
            "Summary: kaplan et al. (2020) reported that number of model parameters, the dataset size, and the amount of compute improves perfor- mance on the autoregressive language modeling objective. a similar investigation by Hernandez et al. (2021) into data transfer f or differing dis- tributions found that this also follows a power law.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.94:\n",
            "this work is the study of Hoffmann et al. (2022), wh o inves- tigated the effect of model size and the number of training tokens on the performance of a model given a fixed compute budget. They posited that existing large l anguage models were undertrained and that model size and the number of training tokens sh ould be scaled equally. They demonstrated this hypothesis through Chinchilla, a model sign ificantly smaller, yet higher performing, than most of the largest LLMs. These findings opened the door for “Chinchilla optimal” training of smaller models that achieve strong p erformance, and for which inference can be run much more efficiently than for their larger counterparts. These findings led us to consider a nearly Chinchilla-optimal model using a standard architecture. Tokenization. Tokenization and vocabulary choice play a critical role in model perfor - mance as they can help the model learn meaningful representations and ge neralize to unseen words. Byte-Pair encoding (BPE) (Sennrich et al., 2016) learns a greed y bottom-up vo- cabulary by repeatedly merging the most frequent sequence pairs in the training set till a predetermined vocabulary size is reached. Radford et al. (2018) adapted B PE by limiting the base vocabulary to be all possible bytes as opposed to all Unicode char acters. Wordpiece tokenization (Schuster and Nakajima \n",
            "\n",
            "Summary: model size and the number of training tokens should be scaled equally. tokenization and vocabulary choice play a critical role in model perfor - mance. wordpiece tokenization can help the model learn meaningful representations. a nearly Chinchilla-optimal model using a standard architecture can be trained based on the findings of Hoffmann et al.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.95:\n",
            "limiting the base vocabulary to be all possible bytes as opposed to all Unicode char acters. Wordpiece tokenization (Schuster and Nakajima, 2012) also learns a greedy bottom-up voc abulary by repeatedly merging the sequence-pair that maximizes the likelih ood of the training data, which is a slight deviation from the method in Sennrich et al. (2016). In contrast to BPE and Wordpiece, the Unigram tokenizer (Kudo, 2018) learns a top- down vocabulary by first initializing a large vocabulary and repeatedl y discarding those vocabulary items that increase loss (e.g., log-likelihood of the train ing data) the least. By construction, the Unigram model can tokenize an input text in several d ifferent ways. That is, the Unigram model saves probabilities allowing for smarter tokeni zation at inference time. 36Finally, SentencePiece (Kudo and Richardson, 2018) adapts the schemes mentioned above to handle languages that are not space separated. Beltagy et al. (2019) constr ucted a vocabulary specific to scientific text and observed that their domain -specific trained vocab- ulary only had a 42% overlap with the non-domain-specific BERT vocabulary trained on general domain text. Similarly, Lewis et al. (2020) showed that a dedicated biomedical vo- cabulary improved \n",
            "\n",
            "Summary: limiting the base vocabulary to be all possible bytes. wordpiece tokenization learns a greedy bottom-up voc abulary. Unigram tokenizer learns a top-down vocabulary by first initializing a large vocabulary and repeatedl y discarding those vocabulary items that increase loss the least. the Unigram model can tokenize an input text in several different ways.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.96:\n",
            "ERT vocabulary trained on general domain text. Similarly, Lewis et al. (2020) showed that a dedicated biomedical vo- cabulary improved performance on sequence labeling tasks consisten tly. Lieber et al. (2021) constructed a larger vocabulary to ensure token efficiency, which th e authors claim resulted in reduced training time and better semantic representation. The se findings demonstrate the importance of selecting a tokenizer and accompanying vocabulary th at best reflects that training domain. For those reasons, we decided to train our own unigram tok enizer instead of relying on existing public ones. Positional Embeddings. Transformer-based models rely on positional embeddings to encode position and location information of words in a text. Encoding th e sequence posi- tion and the effect of this choice on model performance have been studi ed extensively. These include sinusoidal embeddings (Vaswani et al., 2017), rotary position embe ddings (Su et al., 2021a), adding relative position bias (Raffel et al., 2020), and adding linear biase s to atten- tion heads (Press et al., 2022). A side-effect of the strategy in Press et al. (2022) is that one can train on shorter sequences without loss in performance on longer sequ ences. This has two benefits: first, models learn to generalize (extrapolate) to longe r sequences \n",
            "\n",
            "Summary: a dedicated biomedical vocabulary improved performance on sequence labeling tasks consisten tly. for those reasons, we decided to train our own unigram tok enizer. positional embeddings encode position and location information of words in a text. a side-effect of this strategy is that one can train on shorter sequences without loss in performance on longer sequences.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.97:\n",
            "loss in performance on longer sequ ences. This has two benefits: first, models learn to generalize (extrapolate) to longe r sequences and second, models can be trained on shorter sequences reducing training time. 8 Ethics, Limitations, and Implications The rapid development and adoption of large language models have been accompan ied by a rigorous conversation about the ethics, uses, and limitations of these m odels. For a more complete treatment of these topics, we direct the reader to Bommasani et al. (2021); Bender et al. (2021); Birhane et al. (2022); Weidinger et al. (2021, 2022). We discuss issues that are directly relevant to the development of BloombergGPT. 8.1 Ethical Use Finance is a sensitive area for technology, and ensuring accurate, fact ual information is crucial for our products, our clients, and the firm’s reputation in th e marketplace. On the other hand, our clients are also eager to adopt state-of-the-art techn ology to support their workflows. To provide natural language applications to the financial community, we have developed a rigorous risk and testing assessment process. Thi s process includes careful annotation guidelines (Tseng et al., 2020), pre-launch review at multipl e levels by the central risk and compliance organizations, and by the product leaders (e.g., the newsroom) as applicable, and post-launch monitoring. Moreover, we conduct our resear \n",
            "\n",
            "Summary: the rapid development and adoption of large language models have been accompanied by a rigorous conversation about the ethics, uses, and limitations of these models. to provide natural language applications to the financial community, we have developed a rigorous risk and testing assessment process. to ensure accurate, factual information is crucial for our products, our clients, and the firm’s reputation in the marketplace.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.98:\n",
            "and by the product leaders (e.g., the newsroom) as applicable, and post-launch monitoring. Moreover, we conduct our resear ch, development, and deployment of NLP and AI systems in accordance with all applicable regul ations. Similarly, toxicity and bias are areas where, as a company, we take extraor dinary care with any content we produce, whether from humans or machines. Since the measurement of toxicity and bias in our model depends on its application areas, quantif ying the potential for the generation of harmful language remains an open question. We are particular ly interested in studying whether FinPile, which is cleaner and contains fewer examples of overtly biased 37or toxic language (e.g., Press Releases), reduces the proclivity of t he model to generate inappropriate content. As we move to develop products built on this t echnology, we will apply existing testing procedures, as well as risk and compliance c ontrols, to ensure safe use. 8.2 Openness An ongoing debate in the community concerns how LLMs should be released, i f at all. While models that are not publicly available cannot be fully evaluated by the community, distribut- ing models can lead to nefarious purposes. Especially for a model lik eBloombergGPT, which is trained on a significant amount of press releases, news article s, and filings, a release carries a high risk for abuse through imitation. We have witnessed many different strategies to mitigate risks assoc \n",
            "\n",
            "Summary: eBloomberg is interested in studying whether FinPile, which is cleaner, reduces proclivity of model to generate inappropriate content. 8.3 Openness An ongoing debate in the community concerns how LLMs should be released, i f at all. eBloombergGPT is trained on a significant amount of press releases, news article s, and filings.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.99:\n",
            "s, and filings, a release carries a high risk for abuse through imitation. We have witnessed many different strategies to mitigate risks assoc iated with the release of LLMs. One strategy is to freely and openly share trained models (Scao e t al., 2022), and rely on a license that dictates how the model should and should not be u sed. Another requires individuals to apply for access to the trained model parame ters (Zhang et al., 2022a; Touvron et al., 2023). A more restrictive approach is to provide API access to models, but no access to the underlying model parameters or detail ed information on the data the model was trained on (Brown et al., 2020). Finally, some have prov ided no access to the model (Chowdhery et al., 2022; Hoffmann et al., 2022). Each decision refle cts a combination of factors, including model use, potential harms, and busi ness decisions. One of Bloomberg’s core business propositions is around providing acce ss to data that has been collected over the course of decades. As is well known, LLMs are susceptible to data leakage attacks and it is possible to extract significant segments of te xt given model weights (Carlini et al., 2020, 2022). Moreover, even giving selective acces s to researchers isn’t a guarantee that the model cannot \n",
            "\n",
            "Summary: a release carries a high risk for abuse through imitation. one of Bloomberg’s core business propositions is around providing acce ss to data. even giving selective acces s to researchers isn’t a guarantee that the data will be used properly. a release carries a high risk for abuse through imitation. a release carries a high risk for misuse through imitation.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.100:\n",
            "Carlini et al., 2020, 2022). Moreover, even giving selective acces s to researchers isn’t a guarantee that the model cannot be leaked. Without strong privac y guarantees, we must be concerned that providing access to model weights entails gi ving access to FinPile. For this reason, we err on the side of caution and follow the practice of othe r LLM developers in not releasing our model. Nevertheless, our insights and experiences in training and evaluati ngBloombergGPT contribute to the developing understanding of these models. In p articular, our experience may be useful to those building domain-specific models. During t he process of developing BloombergGPT, we found the OPT chronicles, experiences of the BLOOM team, as well as work of non-open models like GPT-3, PaLM, Chinchilla, and Gopher, to be crucial enablers of our work. In support of this tradition, we include our Traini ng Chronicles (Appendix C). 9 Conclusion We have presented BloombergGPT, a best-in-class LLM for financial NLP. Our model contributes to the ongoing dialog on effective ways to train d omain-specific models. Our training strategy of mixing domain-specific and general-p urpose data results in a model that balances performance in both domains. Additionally, our work off ers another data point on selecting Chinchilla optimal-sized models. Finally, we hope \n",
            "\n",
            "Summary: our model contributes to the ongoing dialog on effective ways to train d omain-specific models. our training strategy of mixing domain-specific and general-p urpose data results in a model that balances performance in both domains. our model contributes to the ongoing dialog on effective ways to train d omain-specific models, we hope.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.101:\n",
            "that balances performance in both domains. Additionally, our work off ers another data point on selecting Chinchilla optimal-sized models. Finally, we hope that our model training logs will provide a guide for those training their own LLMs. We have several interesting directions to pursue. First, task fi ne-tuning has yielded significant improvements in LLMs, and we plan to consider what unique op portunities exist 38 \n",
            "\n",
            "Summary: our work off ers another data point on selecting optimal-sized models. our model training logs will provide a guide for those training their own LLMs. our work off ers another data point on selecting chinchilla optimal-sized models. our work off ers another data point on selecting chinchilla optimal-sized models -.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Recursive\n",
            "T5 recurs. level: 2\n",
            "\n",
            "Number of chunks: 28\n",
            "Chunk no.1:\n",
            "BloombergGPT is a large language model trained on a wide range of financial data. it outperforms existing models on financial tasks by significan t margins. the model can also be used to answer financial questions. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set based on a wide range of financial data. outperforms existing models on financial tasks by significan t margins. outperforms performance on general LLM benchmarks without sacrificing performance. outperforms existing models on financial tasks by significan t margins. outperforms existing models on financial tasks by significan t margins. a new version of BloombergGPT is available. tokens have a value of 38B tokens – 5.31% of training. public datasets has a value of 345B tokens – 48.73% of training. wikipedia has a value of 24B tokens – 19.48% of training. twitter has a value of 2B tokens – 1.21% of training. co-first authors: gmann16@bloomberg.net 1arXiv:2303.17564v2 [cs.LG] 9 2.2.3 Wikipedia (24B tokens – 3.35% of training)........................................ 9 2.3 Tokenization................................................................................................................................................ 9 3.3 Training Configuration................................................................................ training configuration...................................................................................................... 13 3.4 large-scale optimization.............................. 14 4 training run 15 5 evaluation 16 \n",
            "\n",
            "Summary: BloombergGPT is a large language model trained on a wide range of financial data. it outperforms existing models on financial tasks by significan t margins. the model can also be used to answer financial questions. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set based on a wide range of financial data.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.2:\n",
            "Configuration................................................................................ training configuration...................................................................................................... 13 3.4 large-scale optimization.............................. 14 4 training run 15 5 evaluation 16 5.1 few-shot methodology.............................................................. 18 5.2 heldout loss................................................................... 18 5.3 financial tasks......................................................................................................................................................................................................................................................................................................................................................................... 5.3 Financial Tasks......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... 26 5.6 Reading Comprehension.............................................. 28 5.7 Linguistic Tasks................................................ 29 5.8 Summary...................................... 30 6 Qualitative Samples 31 7 Related Work 32 8 Ethics, Limitations, and Implications...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture \n",
            "\n",
            "Summary: architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.3:\n",
            "architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 62 a.3 layer norm (ln) 63 a.4 feedforwardnetwork (ffn) 63 a.5 list of all trainable parameters................................................ 64 b details on external financial tasks 65 c training Chronicles 67 c.0 still.................................................................................................................. 67 c.0 still.................................................. the release of GPT-3 in 2020 demonstrated the powerful benefits of training very large auto-regressive language models (LLMs) GPT-3 had 175 b illion parameters, a hundredfold increase over the previous GPT-2 model. a large number of parameters can be used to train very large auto-regressive language models (LLMs) GPT-3 had 175 b illion parameters, a hundredfold increase over the previous model. evidence suggests that large models exhibit emergent behaviors. few-shot prompting dramatically expands the range of tasks supported by models. models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 billion (PaLM, 2022), and 1 trillion parameters. financial technology (FinTech) \n",
            "\n",
            "Summary: release of GPT-3 in 2020 demonstrated the powerful benefits of training very large auto-regressive language models (LLMs) GPT-3 had 175 b illion parameters, a hundredfold increase over the previous model. models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 billion (PaLM, 2022), and 1 trillion parameters.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.4:\n",
            ", Rae et al., 2021), 540 billion (PaLM, 2022), and 1 trillion parameters. financial technology (FinTech) is a large and growing area with NLP technologies having an increasingly important role. these efforts have almost exclusively focused on general LLMs, trained on datasets that cover a broad range of topics and domains. recent efforts training models using only domain-specific data have yielded models that, while much smaller, beat general purpose LLMs on tasks within those domains. financial technology (FinTech) is a large and growing area with NLP technologies having an increasingly important role. the complexit y and terminology of the financial domain warrant a domain-specific system. no LLM has been tuned for or evaluated on tasks for the financial domain. 1.1 BloombergGPT We train BloombergGPT, a 50 billion parameter language model that supports a wide range of tasks within the financial industry. at Bloomberg, we support a large and diverse set of tasks, well served by a general model. but the vast majority of our applications are within the financial domain, better served by a specific model. we achieve this goal by constructing the largest domain-specific dataset yet. we train a BLOOM-style, 50 bill ion parameter model based on guidelines from Hoffmann et al. a LOOM-style, 50 bill ion parameter model is designed based on guidelines from Hoffmann et al. (2022) and le Scao et al. (2022) the model vastly outperforms existing models on in-domain \n",
            "\n",
            "Summary: financial technology (FinTech) is a large and growing area with NLP technologies having an increasingly important role. recent efforts training models using only domain-specific data have yielded models that beat general purpose LLMs on tasks within those domains. a LOOM-style, 50 bill ion parameter model is designed based on guidelines from Hoffmann et al.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.5:\n",
            "et al. (2022) and le Scao et al. (2022) the model vastly outperforms existing models on in-domain financial tasks. our goal is to contribute to t he broader research community. the resu lting model does very well on domain-specific tasks, but also maintains strong performance on ge neral-purpose benchmarks. training data includes a significant amount of curated and prep ared data from reliable sources. evaluations are built on available datasets, but for domain-specific tasks, there remains a mismatch between evaluation and actual use cases. evaluations are built on available datasets and not necessarily on how the model will be used in practice. we train a 50 billion parameter model on 569 billion tokens from our corpus of o ver 700 billion tokens. the critical step of tokenization trans forms the text into a format suitable for the language model. our model produces a model that is competitive with larger models. we use a Unigram model instead of greedy merge-based s ub-word tokenizers. we train BloombergGPT on a range of financial documents drawn from the Bloomberg archiv es. the data is a comprehensive dataset consisting of news, filings, web-scraped fi- nancial documents, and social media. Bloomberg archiv es news, filings, press releases, web-scraped financial documents. we augment FinPile with public data widely used to train LLMs. the result is a training corpus that is roughly half domain-specific text and \n",
            "\n",
            "Summary: the model vastly outperforms existing models on in-domain financial tasks. the resu lting model does very well on domain-specific tasks. it also maintains strong performance on ge neral-purpose benchmarks. the model produces a model that is competitive with larger models, authors say. a Bloomberg archiv es dataset is used to train the model.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.6:\n",
            "documents. we augment FinPile with public data widely used to train LLMs. the result is a training corpus that is roughly half domain-specific text and half general-purpose text. we de-duplicate each dataset according to Lee et al. (2022a) GitHub 1,428 5,364 766 3.38 227 3.20% books3 19 552,398 1,064 4.97 214 3.02% arxiv 124 47,819 591 3.56 166 2.35% openwebtext2 1,684 3,850 648 5.07 128 1.80% freelaw 349 15,381 537 4.99 108 1.52% StackExchange 1,538 2,201 339 4.17 81 1.15% DM mathematics 100 8,193 82 1.92 the full training set used to train BloombergGPT. the statistics provided are the average number of characters per document (“C/D”), th e average number of characters per token (“C/T”) and the percentage of the overall t okens (“T%”) Bloomberg analysts have curated a set of financial documents to create the FinPile. Bloomberg collects web content by identifying sites that contain fi nancially relevant infor- mation. some documents, such as company filings, are available to the general public. the rest of the documents are private and available, amon g other sources, through the Bloomberg Terminal. each document in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31. identifying sites that contain fi n \n",
            "\n",
            "Summary: Bloomberg analysts have curated a set of financial documents to create the FinPile. each document in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31. Bloomberg collects web content by identifying sites that contain fi nancially relevant infor- mation. some documents, such as company filings, are available to the general public.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.7:\n",
            "in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31. identifying sites that contain fi nancially relevant infor- mation. Bloomb erg’s web crawl is focused on high-quality websites. the News category includes all news sources excluding news articles written by Bloomberg journalists. the content in this category comes from reputable sources of news that are relevant to the financial community so as to maintain factuality and reduce bias. in some countries, like the us, public com panies are mandated. the number of tokens (in millions) contained within documen ts inFinPile, or the number of tokens in a document. the number of tokens in a document. the number of tokens in a document. the number of tokens in a document. the number of tokens in a document. in our dataset, a majority of the filings come from EDGAR, which is the SEC’s online database. the largest sources are “bloomberg news” (0.44% of total) and “bloomberg first word” (0.13% of total), the Bloomberg-authored wire of real-time news. the largest sources are “press releases” (1.21% of total) and “bloomberg first word” (0.13% of total) Bloomberg News (0.44% of total) and “Bloomberg First Word” (0.13% of total) are Bloomberg-authored wires of real-time new s. we use three widely known and available public datasets in our traini ng corpus. \n",
            "\n",
            "Summary: in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31. the largest sources are “bloomberg news” (0.44% of total) and “bloomberg first word” (0.13% of total), the Bloomberg-authored wire of real-time news. the content in this category comes from reputable sources of news that are relevant to the financial community.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.8:\n",
            "total) are Bloomberg-authored wires of real-time new s. we use three widely known and available public datasets in our traini ng corpus. we believe such diverse data will aid generalization to new domains and may even support training on financial data. the creators of The Pile have deliberately chosen to include duplicate content. however, as we deduplicate each of our datasets, the size of The Pile is significantly reduced. we also include a dump of english Wikipedia pages in addition to out-of-date versions of the corpus. this is to ensure that the factuality of the model is not compromised. d be beneficial for the factuality of the model to have up-to-date Wikipedia pages inclu ed. we use the Unigram tokenizer instead of a greedy merge-based s ub-word tokenizer. we include spaces in the alphabetic order of the 256 bytes in the input byte sequen ce. we include spaces in the alphabetic chunks, which allows multi-word tokens to be learned, increasing information density and reducing context lengt hs. we train our tokenizer on The Pile as it draws from diverse domains, in cluding code and academic papers. our tokenizer is based on BLOOM, NeoX, OPT (GPT 2), and BloombergGPT. split and merge approach is used to train a Unigram tokenizer on a large dataset. the result is a tokenizer with 7 million tokens. to reduce the size of the vocabulary to 217 tokens, we drop the token \n",
            "\n",
            "Summary: we use three widely known and available public datasets in our traini ng corpus. we use the Unigram tokenizer instead of a greedy merge-based s ub-word tokenizer. our tokenizer is based on BLOOM, NeoX, OPT (GPT 2), and BloombergGPT. we believe such diverse data will aid generalization to new domains.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.9:\n",
            "a large dataset. the result is a tokenizer with 7 million tokens. to reduce the size of the vocabulary to 217 tokens, we drop the tokens with the smallest prob- abilities and renormalize. to ensure we do not need an out-of-vocabular y token, we add as tokens the 36 (of 256 possible) bytes that do not occur in the data. we select our vocabulary size of 217tokens based on experiments with 25,000 to 550,000 words. there are various considerations in choosing the vocabulary size. one advantage of a large vocabulary for LLMs is that more information can fit into the context window. on the other hand, there is overhead with a larger vocabulary: a larger prop ortion of model parameters are required for token embedding. BloombergGPT is a decoder-only causal language model based on BLOOM. the model contains 70 layers of transformer decoder blocks defined as follows: hl=hl1+ SA(LN( hl1)) hl= hl+ FFN(LN( hl1)) where SA is multi-head self-attention, LN is layer-normalization, an d FFN is a feed-forward network with 1-hidden layer. token embeddings are tied to the linear mapping before the final softmax. h1= lnem(h0) + SA(ln(h0)); whereh0is the initial token embedding. 111e22 3.2e22 1e23 3.2e23 \n",
            "\n",
            "Summary: BloombergGPT is a decoder-only causal language model based on bloom. token embeddings are tied to the linear mapping before the final softmax. the model contains 70 layers of transformer decoder blocks defined as follows: hl=hl1+ SA(ln(hl1)) hl= hl+ FFN(ln(hl1))\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.10:\n",
            "SA(ln(h0)); whereh0is the initial token embedding. 111e22 3.2e22 1e23 3.2e23 1e24 3.2e24 FLOPs102050100200500100020005000Tokens (b) NeoX LaMDA GPT-3/Jurassic/OPT OPT the size of our model is based on Chinchilla scaling laws. approach 1 Parameters = exp10(log10(FLOPs)0.4981.004) = 52.993B Tokens = exp10(log10(FLOPs)0.4981.004) = 52.993B Approach 2 Parameters = exp10(log10(FLOPs)0.4981.004) = 52.993B Tokens = exp10(log10(FLOPs)0.4981.004) = finPile is already among the largest domain-specific training sets at our disposal. the scaling law derived by Chinchilla is tokenizer-specific. our tokenizer can encode the same document more compactly due to the support of multi-word expressions and t he larger vocabulary size. finPile will be used in a future version of the finpile project. BloombergGPT has 50B parameters, which is roughly the Chinchilla optimal size for our compute budget. we want the dimensions t o be multiples of 8 to achieve higher performance in Tensor Core operations. we also want to follow the tradition th at the hidden dimension is evenly divisible by the number of attention heads, wi th the quotient \n",
            "\n",
            "Summary: finPile is already among the largest domain-specific training sets at our disposal. the scaling law derived by Chinchilla is tokenizer-specific. our tokenizer can encode the same document more compactly due to the support of multi-word expressions and a larger vocabulary size. finPile will be used in a future version of the finpile project.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.11:\n",
            "or Core operations. we also want to follow the tradition th at the hidden dimension is evenly divisible by the number of attention heads, wi th the quotient giving the attention head dimension. BloombergGPT is a PyTorch model trained with a standard left-right causal language modeling objective. we want all our train- ing sequences to be exactly the same length, in our case 2,048 tokens, to maximize GPU utilization. for optimization efficiency, training sequences ar e grouped together into batches, as described in more detail below. we set the maximum learning rate to 6e-5 and use the cosine decay learning rate sc heduler with linear warmup. we also employ batch size warmup: in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens) and switch to a batch size of 2,048 (4.2M tokens) we use 13the technique of querykeylayerscaling, which was proposed to improve numerical stability for FP16 mixed-precision training bu t may also help in BF16. training Instability. LLMs optimization requires running convex optimization algo- rithms over incredibly complex non-convex loss surfaces. despite the fact that gradient clipping was enabled, Chowdhery et al. (2022) found that the loss spiked roughly 20 times while training pa we use the amazon SageMaker service provided by aws to train and evaluate BloombergGPT. each p4d.24xlarge instance has 8 NVIDIA 40GB A \n",
            "\n",
            "Summary: BloombergGPT is a PyTorch model trained with a standard left-right causal language modeling objective. we want all our train- ing sequences to be exactly the same length, in our case 2,048 tokens, to maximize GPU utilization. we use the technique of querykeylayerscaling, which was proposed to improve numerical stability for FP16 mixed-precision training.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.12:\n",
            "Maker service provided by aws to train and evaluate BloombergGPT. each p4d.24xlarge instance has 8 NVIDIA 40GB A100 GPUs. for quick data access, we use amazon FSX f or Lustre, which supports up to 1000 MB/s read and write throughput per TiB s torage unit. we achieve 102 TFLOPs on average and each training step takes 32.5 seconds. we shard a model acros s 128 GPUs, and we have 4 copies of the model during training. activation checkpointing removes activations at the expense of additional computation during backward passes. we find the fol lowing setup to be the best performing in our training. training and validation losses for BloombergGPT are plotted. forward and backward passes are done in BF16, while parameters are stored and updated in full precision (FP32) to reduce the memory requirements, forward and backward passes are done in BF16. to reduce the memory requirements, forward and backward passes are done in BF16, while parameters are stored and updated in full precision (FP32) combining composition of several operations into a single GPU operation can help improve speed. we use a masked-causal-softmax fused kernel in SMP in the self-attention module. the process of training BloombergGPT involved decisions along the way based on the progress of model training. a d etailed presentation appears in the Training Chronicles (Appendix C) raw values vary wildly, causing large jitter when plotted \n",
            "\n",
            "Summary: each p4d.24xlarge instance has 8 NVIDIA 40GB a100 GPUs. each training step takes 32.5 seconds. forward and backward passes are done in BF16, while parameters are stored and updated in full precision (FP32) raw values vary wildly, causing large jitter when the model is run in the background.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.13:\n",
            "a d etailed presentation appears in the Training Chronicles (Appendix C) raw values vary wildly, causing large jitter when plotted. we measured training loss every five steps on the current batch. we trained for a total of 139,200 steps ( /tildelow53 days) and ended model training after completing /tildelow80% of one epoch through our training data (569B tokens out of 709B tokens available) the performance of BloombergGPT was evaluated on two broad categories of tasks: finance-specific and general purpose. the finance-specific tasks help us test our hypoth- esis that training on high-quality finance-specific data will yield b etter results on financial tasks. the general purpose tasks investigate whether the performance of our model is directly comparable to previously published results. t o directly test BloombergGPT’s ability on tasks of interest, we also included tasks dra wn from Bloomberg-internal high-quality evaluation sets. for general-purpose tasks, we draw from multiple ex isting benchmarks and group results into the following categories: BIG-bench Hard, Know ledge Assessments, Reading Comprehension, and Linguistic Tasks. we further assess BloombergGPT on a suite of internal and public financial tasks. we compare BloombergGPT to the three closest models described in 7 based on model size, type of training data, overall performance, and most import antly, access. we report GPT-3 res ults whenever available but did not run it ourselves due to lack of availabi \n",
            "\n",
            "Summary: the performance of BloombergGPT was evaluated on two broad categories of tasks: finance-specific and general purpose. finance-specific tasks help us test our hypoth- esis that training on high-quality finance-specific data will yield b etter results on financial tasks. the general purpose tasks investigate whether the performance of our model is directly comparable to previously published results.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.14:\n",
            "and most import antly, access. we report GPT-3 res ults whenever available but did not run it ourselves due to lack of availabi lity. BLOO 176B is substantially larger than BloombergGPT. BLOO M176B is trained on data from more languages. all three models use some of the same general-purpose datasets we use in our training cor- pus. the win rate for each group of results is similar to Liang et al. (2022) th at. a few-shot method is used to classify text using likelihood-based methods. for tasks where a set of candidates are given, we perform likelihood-b ased classification. we use the official split and report performance on the test set wheneve r possible. if an official split does not exist, we create train and test spli ts by selecting 20% of examples to be the test. w e sample different shots for each test example, unless otherwise specified. we evaluate the bits per byte of the differe nt models on a heldout dataset. to limit data leakage and better simulate real-world usage of LLMs, we select a temporally heldout 2. the set of documents is held out in time and deduplicat ed with the training set. a set of documents is held out in time and deduplicat ed with the training set. the improveme nt is largest for specialized in-domain documents like Filings. the gap to BloombergGPT is most significant in the Filings category. financial \n",
            "\n",
            "Summary: BLOO 176B is substantially larger than BloombergGPT. BLOO M176B is trained on data from more languages. the improveme nt is largest for specialized in-domain documents like Filings. the win rate for each group of results is similar to liang et al. (2022) th at.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.15:\n",
            "me nt is largest for specialized in-domain documents like Filings. the gap to BloombergGPT is most significant in the Filings category. financial tasks take on different characteristics and challen ges when performed on financial data. com-pany to cut 10,000 jobs portrays negative sentiment in the general sense b ut can at times be considered positive for financial sentiment towards COMPANY. we use a combination of p ublic and internal benchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, and OPT66B. public benchmarks include four tasks from the FLUE bench mark and the convFinQA dataset. as LLM performance on most of these financial tasks have not been broadly reported, there is no stand ard testing frame- work. as a result, our claims are restricted to comparisons of LLMs on the external financial tasks. we e valuate on the following tasks. sentiment analysis task is to predict aspect-specific sentiment in english financial news and microblog he adlines. original dataset is annotated on a continuous scale, but discretized into a classification setup with negative, neut ral, and positive classes. like with FPB, we create our own splits including microbl ogs and news, and use a 5-shot setup, reporting weighted F1. each news article carries a subset of the following tags: “price or not”, “price up”, “price down”, “price stable”, “past price”, “fu \n",
            "\n",
            "Summary: the gap to BloombergGPT is most significant in the Filings category. financial tasks take on different characteristics and challen ges when performed on financial data. we use a combination of public and internal benchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, and OPT66B.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.16:\n",
            "set of the following tags: “price or not”, “price up”, “price down”, “price stable”, “past price”, “future price”, “past general”, “asset comparison” we verbalize each tag into a question, use 5 shots, and report the average weighted F1 score across all categories. BloombergGPT performs best of all models for four of the five tasks. it also has the highest win rate among all the models that we tested. the gap to equally-sized models is especially pronounced for conver- sational questions that require numerical reasoning over the input. BloombergGPT also has the highest win rate among all the models that we tested. Bloomberg-internal tasks consider aspect-specific sent iment analysis. our annotators are a ded icated team of financial experts at Bloomberg, consultant workers, or a combination of bot h. as the datasets are large, we random ly sample at most 1k test examples. we report F1 weighted by the support of each label. the datasets are large, we random ly sample at most 1k test examples. we report F1 weighted by the support of each label. some of the data used in our internal datasets occur in FinPile. other LLMs we compare against may have also been trained on unlabeled versions of this d ata - see table 9. BloombergGPT far outperforms all other models on sentiment analysis tasks. each chun k in our dataset typically contains between 70 and 80 tokens. BloombergGPT far outper \n",
            "\n",
            "Summary: BloombergGPT performs best of all models for four of the five tasks. it also has the highest win rate among all the models that we tested. the gap to equally-sized models is especially pronounced for conver- sational questions that require numerical reasoning over the input. Bloomberg-internal tasks consider aspect-specific sent iment analysis.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.17:\n",
            "performs all other models on sentiment analysis tasks. each chun k in our dataset typically contains between 70 and 80 tokens. BloombergGPT far outperforms all other models on sentiment analysis tasks. ences.com is one of the world's largest news and media companies. ences.com is the world's largest news and media company. BloombergGPT performs better than all the other tested models across the four internal aspect-specific sentimen t tasks. the goal is not t o indicate effect on investor confidence. the only task in which the models perform similarly is the social media sentiment task, whileBloombergGPT outperforms the other models by at least 25 and up to over 100. NER is largely an unexplored task for generative LLMs. NER is an information extraction task, and a better fit for encoder-decod er or encoder-only architectures. the generative nature of LLMs does not confer an advantage for NER. NER has s ubtleties that make it difficult to obtain reasonable results for NER. finance-specific NER has s ubtleties that make it especially difficult for zero or few-shot learning. without adding e xtensive annotation guidelines in the prompt, the LLM does not know the intended tagging behavior. we restr ict the entity types to be predicted to be ORG, PER, and LOC. we consider seven Bloomberg internal NER datasets from different domai ns. we remove all documents that contain no entities ( \n",
            "\n",
            "Summary: BloombergGPT far outperforms all other models on sentiment analysis tasks. ences.com is one of the world's largest news and media companies. NER is largely an unexplored task for generative LLMs. NER has s ubtleties that make it especially difficult for zero or few-shot learning.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.18:\n",
            "G, PER, and LOC. we consider seven Bloomberg internal NER datasets from different domai ns. we remove all documents that contain no entities (i.e., all “O”’s) both of these modi- fications are intended to increase the usefulness of the examples se en in few-shot prompting. we expect that further work on prompt engineering for NER could produce better results. the dataset contains transcripts from 2019. NER+NED BFW 55.29 34.92 36.73 39.36 BN 60.09 44.71 54.60 49.85 Filings 66.67 31.70 65.63 42.93 Headlines 67.17 36.46 56.46 42.93 Premium 64.11 40.84 57.06 42.11 Transcripts 75.49 59.39 57.56 61.61 Transcripts 75.50 70. the much larger BLOOM 176B model outperforms all other models on NER. on NER+NED, BloombergGPT outperforms all other models by a large margin. the goal of this task is to identify entities that occur in financially-relevant social media content. the results from the internal NER tasks are mixed (Table 12) we seek to link text mentions of companies to their ticker sym- bols. this 25requires the model to first identify company mentions and then gene rate the corresponding stock ticker. while NER evaluation requires exact matches, ticker s may be successfully produced without first identifying spans. \n",
            "\n",
            "Summary: we consider seven Bloomberg internal NER datasets from different domai ns. we remove all documents that contain no entities (i.e., all “O”’s) both of these modi- fications are intended to increase the usefulness of the examples se en in few-shot prompting. we expect that further work on prompt engineering for NER could produce better results.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.19:\n",
            "s and then gene rate the corresponding stock ticker. while NER evaluation requires exact matches, ticker s may be successfully produced without first identifying spans. we remove documents with no linked tickers. omberg internal NER annotated documents from each domain. we remove documents with no li nked tickers. we then evaluate BloombergGPT on standard, general-purpose NLP tasks. our model outperforms all other models except on social media data. a new version of the BIG-bench test is available. BloombergGPT falls behind the much larger paLM 540B (10x parameters) and BLOOM 176B (3.5x parameters) but it is the best-performing among similarly sized models. it achieves the best perfor- mance of all models in date understanding, hyperbaton, and tracking shuffled objects. we next assess knowledge, which we define as the ability to recall in formation seen during model training. 9th grade science exams, includes easy and challenging splits. 26BIG-bench hard task BloombergGPT GPT-NeoX OPT 66B BLOOM 176B PaLM 540B Boolean Expressions62.40 71.20 48.40 69.20 83.2 Causal Judgement 49.73 52.41 51.87 51.87 61.0 Date Understanding 54.80 45.60 49.60 50.00 53.6 Disambiguation QA 34.00 40. BIG-bench hard results using standard 3-shot prompting. the baseline numbe rs from paLM 540B( \n",
            "\n",
            "Summary: our model outperforms all other models except on social media data. BloombergGPT falls behind the much larger paLM 540B (10x parameters) and BLOOM 176B (3.5x parameters) but it achieves the best perfor- mance of all models in date understanding, hyperbaton, and tracking shuffled objects. a new version of the BIG-bench test is available.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.20:\n",
            "ation QA 34.00 40. BIG-bench hard results using standard 3-shot prompting. the baseline numbe rs from paLM 540B(chowdhury et al., 2000) are shown in table 13. the algorithmic tasks are denoted with the supersc ript, and present averages for NLP and algorithmic categories. BloombergGPT achieves the highest performance among BLOOM 176B, GPT-NeoX, and OPT66Bin one task, and comes second in the other three. BloombergGPT consistently outperforms OPT66B, which in turn outperforms GPT-NeoX, while GPT-3 performs best. BloombergGPT achieves the highest win rate among the models we ran ourselves, and performs second best on average. 66B outperforms GPT-NeoX, while GPT-3 performs best. the baseline numbers from GP T-3 are taken from brown et al. (2020) the baseline numbers from GP T-3 are taken from brown et al. (2020) BloombergGPT lacks behind BLOOM 176Bon three of the categories, but its aver- age is the highest among all models we evaluated ourselves. the gap to GPT-3 is largest on social sciences while the performance in other categories is close. the gap to GPT-3 is closest in the STEM and “Other” domains which include finance and accounting-related questions. BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3 \n",
            "\n",
            "Summary: BloombergGPT achieves the highest performance among the models we ran ourselves. 66B outperforms GPT-NeoX, while GPT-3 performs best. the gap to GPT-3 is largest on social sciences while the performance in other categories is close. 66B outperforms GPT-NeoX, while GPT-3 performs best.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.21:\n",
            "ther” domains which include finance and accounting-related questions. BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. •OpenBookQA: multiple-choice elementary-level science questions, given a book of science facts, applied to new situations. •RACE: a multiple choice dataset of middle and high school english examinations. BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. except for openbookQA, the performance of BloombergGPT is the highest among BLOOM 176B, GPT-NeoX, and OPT66B. we follow the template of brown et al. (2020), and report accuracy. given two text fragments, i dentify whether the meaning of one text is entailed. •Adversarial NLI (ANLI, Nie et al., 2020): Adversarially constructed entailment detection. •CommitmentBank (CB): Naturally occurring discourses whose final sentence contains a clause-embedding predicate. ANLI Round 3 37.33 36.17 34.92 35.17 35.1 CB 53.57 48.21 44.64 48.21 64.3 COPA 86.00 88.00 86.00 84.00 87.0 WIC 52.51 50.00 52.51 50.16 48.6 WinoGrad 80.95 79.12 82.78 78 \n",
            "\n",
            "Summary: BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. except for openbookQA, the performance of BloombergGPT is the highest among BLOOM 176B, GPT-NeoX, and OPT66B. BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.22:\n",
            "87.0 WIC 52.51 50.00 52.51 50.16 48.6 WinoGrad 80.95 79.12 82.78 78.02 89.7 WinoGrande 64.09 60.62 66.14 67.01 73.2 Hella linguistic tasks follow a similar trend to t he knowledge category. BloombergGPT falls slightly behind GPT-3 and outperforms the other models. in some cases, BloombergGPT is competitive or exceeds the performance of much larger models. the goal for BloombergGPT was to be a best-class model for financial tasks, but it has still attained abilities on general-purpose data that exceed similarly sized models. the model can compose valid queries to ret rieve the data, given a request in natural language. Using only a few examples in a few-shot setting, the model can utilize its knowledge about stock tickers and financial terms. the model is given 3 examples (not shown) followed by the “Input” and a prompt of “Output” BloombergGPT can be used to make interactions with financial data more natural. Bloomberg Query Language (BQL) can be used to interact with diff erent classes of securities. BloombergGPT can be used for many news applications and assist journalists in their day-to-day work. it performs well out of the box at identifying the CEO of a company. the housing market shrank in value by $2.3 trillion in the second half of 2022. that's the largest drop in percentage terms since the 2008 housing crisis. the global economy is \n",
            "\n",
            "Summary: the goal for BloombergGPT was to be a best-class model for financial tasks. but it has still attained abilities on general-purpose data that exceed similarly sized models. the housing market shrank in value by $2.3 trillion in the second half of 2022. the goal for the model was to be a best-class model for financial tasks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.23:\n",
            "shrank in value by $2.3 trillion in the second half of 2022. that's the largest drop in percentage terms since the 2008 housing crisis. the global economy is in a better place today than many predicted months ago. google is sued by the US and eight states seeking the breakup of its ad-tech business. a new report says the u.s. economy is in a better place today than many predicted. s first big challenge to a tech titan and on e of the rare times since 1982 that the DOJ has sought to cleave up a major company. s first big challenge to a tech titan and on e of the rare times since 1982 that the DOJ has sought to cleave up a major company. s first big challenge to a tech titan and on e of the rare times since 1982 that the DOJ has sought to cleave up a major company the process of developing models that could better approximate the distribution of language over large corpora led to the discovery that the representations these models produce are useful starting points for many downstream tasks. generative representations are useful starting points for many downstream tasks. the process of developing models that could better approximate the distribution of language over large corpora led to the discovery that the representations these models produce are useful starting points. each model is run in a 10-shot setting. we sample up to three answers and present all of them if they are incorr ect. each model is run in a 10-shot setting. we sample up to three answers and present all of them if they are incorr \n",
            "\n",
            "Summary: a new report says the u.s. economy is in a better place today than many predicted. google is sued by the us and eight states seeking the breakup of its ad-tech business. the process of developing models that could better approximate the distribution of language over large corpora led to the discovery that the representations these models produce are useful starting points.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.24:\n",
            "r ect. each model is run in a 10-shot setting. we sample up to three answers and present all of them if they are incorr ect. if a model does not produce a correct answer, it is discarded. autoregressive language models achieve strong per formance in transfer learning. many other researchers built large language models to study dat a quantity, data quality, network architecture, parameter scaling, data scaling, t okenization, and open- sourcing strategies. a large language model can be used to study data quality, network architecture, parameter scaling, data scaling, t okenization, and open-sourcing strategies. in-domain training allows model s to outperform previ- ous state-of-the-art models in a variety of biomedical text mining tasks. the value of domain-specific training for masked (encoder only) language models is well established. in-domain training can also be used to train existing models on new domain-specific data. decoder-only language models of more than 10B parameters are more costly to train. several examples of from-scratch trained decoder-only m odels for domain-specific data have emerged. one popular domain is protein sequences s ince they can be represented using language-like sequences. however, there can b e benefits even for models in natural language domains. taylor et al. trained galactica exclusively on a large collection of scientific datasets. while performing very well on scientific tasks, galactic \n",
            "\n",
            "Summary: autoregressive language models achieve strong per formance in transfer learning. a large language model can be used to study data quality, network architecture, parameter scaling, data scaling, t okenization. in-domain training allows model s to outperform previ- ous state-of-the-art models in a variety of biomedical text mining tasks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.25:\n",
            "taylor et al. trained galactica exclusively on a large collection of scientific datasets. while performing very well on scientific tasks, galactica also performs well on more standard NLP tasks. we augment our private data with public data with the goal of investigating wh ether a model can gain in-domain capabilities without sacrificing general-domain performance. large corpora of raw text data are critical for training LLMs. there are several corpora available that cover a wide range of sources. these datasets are built on or include web crawls augmented with an array of data sources. clean datasets still result in data artifacts, duplicates, and toxic language -. the tasks addressed by language models have vastly increased and requi re a very different evaluation process from traditional task-specific sys tems. the second paradigm is to evaluate a model in many differe nt scenarios via a sys tem called a sys tem test. a sys tem called a sys tem test aims to evaluate a model in many differe nt scenarios via a sy in our case, we combine multiple general-purpose evaluations from multiple existing benc hmarks that have different goals. while the se benchmarks allow for a side-by-side comparison between models, it is challengi ng to ensure that all experimental parameters (prompts, decoding strategies, few-step decoding, etc.) are controlled for. there exists no \n",
            "\n",
            "Summary: taylor et al. trained galactica exclusively on a large collection of scientific datasets. we augment our private data with public data with the goal of investigating wh ether a model can gain in-domain capabilities without sacrificing general-domain performance. large corpora of raw text data are critical for training LLMs. we combine multiple general-purpose evaluations from multiple existing benchmarks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.26:\n",
            "i ng to ensure that all experimental parameters (prompts, decoding strategies, few-step decoding, etc.) are controlled for. there exists no standard benchmark for the financial NLP domain. to provide externally comparable res ults, we developed 35a few-shot strategy for FLUE. large language model training remains expensive in terms of the compu- tational cost and human effort to assemble data and train the model. the optimal amount of training data and model shape and size for the best utili zation of re- sources becomes important. kaplan et al. (2020) reported that number of model parameters, the dataset size, and the amount of compute improves perfor- mance on the autoregressive language modeling objective. a similar investigation by Hernandez et al. (2021) into data transfer f or differing dis- tributions found that this also follows a power law. model size and the number of training tokens should be scaled equally. tokenization and vocabulary choice play a critical role in model perfor - mance. wordpiece tokenization can help the model learn meaningful representations. a nearly Chinchilla-optimal model using a standard architecture can be trained based on the findings of Hoffmann et al. limiting the base vocabulary to be all possible bytes. wordpiece tokenization learns a greedy bottom-up voc abulary. Unigram tokenizer learns a top-down vocabulary by first initializing a large vocabulary and repeatedl y discarding those vocabulary items that increase \n",
            "\n",
            "Summary: there exists no standard benchmark for the financial NLP domain. to provide externally comparable res ults, we developed 35a few-shot strategy for FLUE. tokenization and vocabulary choice play a critical role in model perfor - mance. the optimal amount of training data and model shape and size for the best utili zation of re- sources becomes important.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.27:\n",
            "ulary. Unigram tokenizer learns a top-down vocabulary by first initializing a large vocabulary and repeatedl y discarding those vocabulary items that increase loss the least. the Unigram model can tokenize an input text in several different ways. a dedicated biomedical vocabulary improved performance on sequence labeling tasks consisten tly. for those reasons, we decided to train our own unigram tok enizer. positional embeddings encode position and location information of words in a text. a side-effect of this strategy is that one can train on shorter sequences without loss in performance on longer sequences. the rapid development and adoption of large language models have been accompanied by a rigorous conversation about the ethics, uses, and limitations of these models. to provide natural language applications to the financial community, we have developed a rigorous risk and testing assessment process. to ensure accurate, factual information is crucial for our products, our clients, and the firm’s reputation in the marketplace. eBloomberg is interested in studying whether FinPile, which is cleaner, reduces proclivity of model to generate inappropriate content. 8.3 Openness An ongoing debate in the community concerns how LLMs should be released, i f at all. eBloombergGPT is trained on a significant amount of press releases, news article s, and filings. a release carries a high risk for abuse through imitation. one of Bloomberg’s core business propositions is around providing acce ss to data. even giving selective acces s to researchers isn’t a guarantee that \n",
            "\n",
            "Summary: eBloomberg is interested in studying whether FinPile, which is cleaner, reduces proclivity of model to generate inappropriate content. eBloombergGPT is trained on a significant amount of press releases, news article s, and filings. a release carries a high risk for abuse through imitation. eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.28:\n",
            "Bloomberg’s core business propositions is around providing acce ss to data. even giving selective acces s to researchers isn’t a guarantee that the data will be used properly. a release carries a high risk for abuse through imitation. a release carries a high risk for misuse through imitation. our model contributes to the ongoing dialog on effective ways to train d omain-specific models. our training strategy of mixing domain-specific and general-p urpose data results in a model that balances performance in both domains. our model contributes to the ongoing dialog on effective ways to train d omain-specific models, we hope. our work off ers another data point on selecting optimal-sized models. our model training logs will provide a guide for those training their own LLMs. our work off ers another data point on selecting chinchilla optimal-sized models. our work off ers another data point on selecting chinchilla optimal-sized models -. \n",
            "\n",
            "Summary: our model contributes to the ongoing dialog on effective ways to train d omain-specific models. our training strategy of mixing domain-specific and general-p urpose data results in a model that balances performance in both domains. our model training logs will provide a guide for those training their own LLMs -. a release carries a high risk for misuse through imitation.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Recursive\n",
            "T5 recurs. level: 3\n",
            "\n",
            "Number of chunks: 8\n",
            "Chunk no.1:\n",
            "BloombergGPT is a large language model trained on a wide range of financial data. it outperforms existing models on financial tasks by significan t margins. the model can also be used to answer financial questions. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set based on a wide range of financial data. architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture 61 architecture release of GPT-3 in 2020 demonstrated the powerful benefits of training very large auto-regressive language models (LLMs) GPT-3 had 175 b illion parameters, a hundredfold increase over the previous model. models grew in size to 280 billion (Gopher, Rae et al., 2021), 540 billion (PaLM, 2022), and 1 trillion parameters. financial technology (FinTech) is a large and growing area with NLP technologies having an increasingly important role. recent efforts training models using only domain-specific data have yielded models that beat general purpose LLMs on tasks within those domains. a LOOM-style, 50 bill \n",
            "\n",
            "Summary: BloombergGPT is a large language model trained on a wide range of financial data. the model outperforms existing models on financial tasks by significan t margins. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set. the model can also be used to answer financial questions.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.2:\n",
            "only domain-specific data have yielded models that beat general purpose LLMs on tasks within those domains. a LOOM-style, 50 bill ion parameter model is designed based on guidelines from Hoffmann et al. the model vastly outperforms existing models on in-domain financial tasks. the resu lting model does very well on domain-specific tasks. it also maintains strong performance on ge neral-purpose benchmarks. the model produces a model that is competitive with larger models, authors say. a Bloomberg archiv es dataset is used to train the model. Bloomberg analysts have curated a set of financial documents to create the FinPile. each document in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31. Bloomberg collects web content by identifying sites that contain fi nancially relevant infor- mation. some documents, such as company filings, are available to the general public. in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31. the largest sources are “bloomberg news” (0.44% of total) and “bloomberg first word” (0.13% of total), the Bloomberg-authored wire of real-time news. the content in this category comes from reputable sources of news that are relevant to the financial community. we use three widely known and available public datasets in our traini ng corpus. we use the Unigram tokenizer instead of a greedy merge-based s u \n",
            "\n",
            "Summary: a LOOM-style, 50 bill ion parameter model is designed based on guidelines from Hoffmann et al. the model does very well on domain-specific tasks and maintains strong performance on general-purpose benchmarks. a Bloomberg archiv es dataset is used to train the model. each document in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.3:\n",
            "known and available public datasets in our traini ng corpus. we use the Unigram tokenizer instead of a greedy merge-based s ub-word tokenizer. our tokenizer is based on BLOOM, NeoX, OPT (GPT 2), and BloombergGPT. we believe such diverse data will aid generalization to new domains. BloombergGPT is a decoder-only causal language model based on bloom. token embeddings are tied to the linear mapping before the final softmax. the model contains 70 layers of transformer decoder blocks defined as follows: hl=hl1+ SA(ln(hl1)) hl= hl+ FFN(ln(hl1)) finPile is already among the largest domain-specific training sets at our disposal. the scaling law derived by Chinchilla is tokenizer-specific. our tokenizer can encode the same document more compactly due to the support of multi-word expressions and a larger vocabulary size. finPile will be used in a future version of the finpile project. BloombergGPT is a PyTorch model trained with a standard left-right causal language modeling objective. we want all our train- ing sequences to be exactly the same length, in our case 2,048 tokens, to maximize GPU utilization. we use the technique of querykeylayerscaling, which was proposed to improve numerical stability for FP16 mixed-precision training. each p4d.24xlarge instance has 8 NVIDIA 40GB a100 \n",
            "\n",
            "Summary: our tokenizer is based on BLOOM, NeoX, OPT (GPT 2), and BloombergGPT. finPile is already among the largest domain-specific training sets at our disposal. our tokenizer can encode the same document more compactly due to the support of multi-word expressions and a larger vocabulary size. finPile will be used in a future version of the finpile project.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.4:\n",
            "proposed to improve numerical stability for FP16 mixed-precision training. each p4d.24xlarge instance has 8 NVIDIA 40GB a100 GPUs. each training step takes 32.5 seconds. forward and backward passes are done in BF16, while parameters are stored and updated in full precision (FP32) raw values vary wildly, causing large jitter when the model is run in the background. the performance of BloombergGPT was evaluated on two broad categories of tasks: finance-specific and general purpose. finance-specific tasks help us test our hypoth- esis that training on high-quality finance-specific data will yield b etter results on financial tasks. the general purpose tasks investigate whether the performance of our model is directly comparable to previously published results. BLOO 176B is substantially larger than BloombergGPT. BLOO M176B is trained on data from more languages. the improveme nt is largest for specialized in-domain documents like Filings. the win rate for each group of results is similar to liang et al. (2022) th at. the gap to BloombergGPT is most significant in the Filings category. financial tasks take on different characteristics and challen ges when performed on financial data. we use a combination of public and internal benchmarks to assess the performance of BloombergGPT, BLOOM 176B, GPT-NeoX, and OPT66B. BloombergGPT performs best of all models for four of the five tasks. it also has the highest win rate among all the models that we tested. \n",
            "\n",
            "Summary: the performance of BloombergGPT was evaluated on two broad categories of tasks. finance-specific tasks help us test our hypoth- esis that training on high-quality finance-specific data will yield b etter results on financial tasks. the win rate for each group of results is similar to liang et al. (2022) th at.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.5:\n",
            "PT66B. BloombergGPT performs best of all models for four of the five tasks. it also has the highest win rate among all the models that we tested. the gap to equally-sized models is especially pronounced for conver- sational questions that require numerical reasoning over the input. Bloomberg-internal tasks consider aspect-specific sent iment analysis. BloombergGPT far outperforms all other models on sentiment analysis tasks. ences.com is one of the world's largest news and media companies. NER is largely an unexplored task for generative LLMs. NER has s ubtleties that make it especially difficult for zero or few-shot learning. we consider seven Bloomberg internal NER datasets from different domai ns. we remove all documents that contain no entities (i.e., all “O”’s) both of these modi- fications are intended to increase the usefulness of the examples se en in few-shot prompting. we expect that further work on prompt engineering for NER could produce better results. our model outperforms all other models except on social media data. BloombergGPT falls behind the much larger paLM 540B (10x parameters) and BLOOM 176B (3.5x parameters) but it achieves the best perfor- mance of all models in date understanding, hyperbaton, and tracking shuffled objects. a new version of the BIG-bench test is available. BloombergGPT achieves the highest performance among the models we ran ourselves. 66B outperforms GPT \n",
            "\n",
            "Summary: ences.com is one of the world's largest news and media companies. NER is largely an unexplored task for generative LLMs. BloombergGPT performs best of all models for four of the five tasks. it also has the highest win rate among all the models that we tested. the gap to equally-sized models is especially pronounced for numerical reasoning over the input.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.6:\n",
            "version of the BIG-bench test is available. BloombergGPT achieves the highest performance among the models we ran ourselves. 66B outperforms GPT-NeoX, while GPT-3 performs best. the gap to GPT-3 is largest on social sciences while the performance in other categories is close. 66B outperforms GPT-NeoX, while GPT-3 performs best. BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. except for openbookQA, the performance of BloombergGPT is the highest among BLOOM 176B, GPT-NeoX, and OPT66B. BloombergGPT far outclasses the models we evaluated ourselves, and is slightly behind GPT-3. the goal for BloombergGPT was to be a best-class model for financial tasks. but it has still attained abilities on general-purpose data that exceed similarly sized models. the housing market shrank in value by $2.3 trillion in the second half of 2022. the goal for the model was to be a best-class model for financial tasks. a new report says the u.s. economy is in a better place today than many predicted. google is sued by the us and eight states seeking the breakup of its ad-tech business. the process of developing models that could better approximate the distribution of language over large corpora led to the discovery that the representations these models produce are useful starting points. autoregressive language models achieve strong per formance in transfer learning. a large language model can be used \n",
            "\n",
            "Summary: 66B outperforms GPT-NeoX, while GPT-3 performs best. gap to GPT-3 is largest on social sciences while performance in other categories is close. goal for BloombergGPT was to be a best-class model for financial tasks. but it has still attained abilities on general-purpose data that exceed similarly sized models.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.7:\n",
            "that the representations these models produce are useful starting points. autoregressive language models achieve strong per formance in transfer learning. a large language model can be used to study data quality, network architecture, parameter scaling, data scaling, t okenization. in-domain training allows model s to outperform previ- ous state-of-the-art models in a variety of biomedical text mining tasks. taylor et al. trained galactica exclusively on a large collection of scientific datasets. we augment our private data with public data with the goal of investigating wh ether a model can gain in-domain capabilities without sacrificing general-domain performance. large corpora of raw text data are critical for training LLMs. we combine multiple general-purpose evaluations from multiple existing benchmarks. there exists no standard benchmark for the financial NLP domain. to provide externally comparable res ults, we developed 35a few-shot strategy for FLUE. tokenization and vocabulary choice play a critical role in model perfor - mance. the optimal amount of training data and model shape and size for the best utili zation of re- sources becomes important. eBloomberg is interested in studying whether FinPile, which is cleaner, reduces proclivity of model to generate inappropriate content. eBloombergGPT is trained on a significant amount of press releases, news article s, and filings. a release carries a high risk for abuse through imitation. eBloomberg is interested in studying whether FinPile \n",
            "\n",
            "Summary: autoregressive language models achieve strong per formance in transfer learning. in-domain training allows model s to outperform state-of-the-art models in a variety of biomedical text mining tasks. eBloomberg is interested in studying whether FinPile, which is cleaner, reduces proclivity of model to generate inappropriate content.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.8:\n",
            "article s, and filings. a release carries a high risk for abuse through imitation. eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content. our model contributes to the ongoing dialog on effective ways to train d omain-specific models. our training strategy of mixing domain-specific and general-p urpose data results in a model that balances performance in both domains. our model training logs will provide a guide for those training their own LLMs -. a release carries a high risk for misuse through imitation. \n",
            "\n",
            "Summary: eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content. our model contributes to the ongoing dialog on effective ways to train d omain-specific models. our training strategy of mixing domain-specific and general-p urpose data results in a model that balances performance in both domains.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Recursive\n",
            "T5 recurs. level: 4\n",
            "\n",
            "Number of chunks: 3\n",
            "Chunk no.1:\n",
            "BloombergGPT is a large language model trained on a wide range of financial data. the model outperforms existing models on financial tasks by significan t margins. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set. the model can also be used to answer financial questions. a LOOM-style, 50 bill ion parameter model is designed based on guidelines from Hoffmann et al. the model does very well on domain-specific tasks and maintains strong performance on general-purpose benchmarks. a Bloomberg archiv es dataset is used to train the model. each document in FinPile is time-stamped, with dates ranging from 2007-03-01 to 2022-07-31. our tokenizer is based on BLOOM, NeoX, OPT (GPT 2), and BloombergGPT. finPile is already among the largest domain-specific training sets at our disposal. our tokenizer can encode the same document more compactly due to the support of multi-word expressions and a larger vocabulary size. finPile will be used in a future version of the finpile project. the performance of BloombergGPT was evaluated on two broad categories of tasks. finance-specific tasks help us test our hypoth- esis that training on high-quality finance-specific data will yield b etter results on financial tasks. the win rate for each group of results is similar to liang et al. (2022) th at. ences.com is one of the world's largest \n",
            "\n",
            "Summary: the performance of BloombergGPT was evaluated on two broad categories of tasks. the model outperforms existing models on financial tasks by significan t margins. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set. the model does very well on domain-specific tasks and maintains strong performance on general-purpose benchmarks.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.2:\n",
            "each group of results is similar to liang et al. (2022) th at. ences.com is one of the world's largest news and media companies. NER is largely an unexplored task for generative LLMs. BloombergGPT performs best of all models for four of the five tasks. it also has the highest win rate among all the models that we tested. the gap to equally-sized models is especially pronounced for numerical reasoning over the input. 66B outperforms GPT-NeoX, while GPT-3 performs best. gap to GPT-3 is largest on social sciences while performance in other categories is close. goal for BloombergGPT was to be a best-class model for financial tasks. but it has still attained abilities on general-purpose data that exceed similarly sized models. autoregressive language models achieve strong per formance in transfer learning. in-domain training allows model s to outperform state-of-the-art models in a variety of biomedical text mining tasks. eBloomberg is interested in studying whether FinPile, which is cleaner, reduces proclivity of model to generate inappropriate content. eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content. our model contributes to the ongoing dialog on effective ways to train d omain-specific models. our training strategy of mixing domain-specific and general-p urpose data results in a model that balances performance in both domains. \n",
            "\n",
            "Summary: ences.com is one of the world's largest news and media companies. 66B outperforms GPT-NeoX, while GPT-3 performs best. goal for BloombergGPT was to be a best-class model for financial tasks. eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Chunk no.3:\n",
            "domain-specific and general-p urpose data results in a model that balances performance in both domains. \n",
            "\n",
            "Summary: domain-specific and general-purpose data results in a model that balances performance in both domains. a model that balances performance in both domains. a model that balances performance in both domains. a model that balances domain-specific and general-p urpose data results in a model that balances performance in both domains.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
            "Recursive\n",
            "T5 recurs. level: 5\n",
            "\n",
            "Number of chunks: 1\n",
            "Chunk no.1:\n",
            "the performance of BloombergGPT was evaluated on two broad categories of tasks. the model outperforms existing models on financial tasks by significan t margins. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set. the model does very well on domain-specific tasks and maintains strong performance on general-purpose benchmarks. ences.com is one of the world's largest news and media companies. 66B outperforms GPT-NeoX, while GPT-3 performs best. goal for BloombergGPT was to be a best-class model for financial tasks. eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content. domain-specific and general-purpose data results in a model that balances performance in both domains. a model that balances performance in both domains. a model that balances performance in both domains. a model that balances domain-specific and general-p urpose data results in a model that balances performance in both domains. \n",
            "\n",
            "Summary: the model outperforms existing models on financial tasks by significan t margins. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set. goal for BloombergGPT was to be a best-class model for financial tasks. eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content.\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n"
          ]
        }
      ],
      "source": [
        "#Extracting a PDF to text string.\n",
        "text = pdf_extractor('/content/Article 6 BloombergGPT_ A Large Language Model for Finance.pdf')\n",
        "\n",
        "#Bart recursive summarization.\n",
        "bart_final_summary = b_recursive_summarize(text)\n",
        "\n",
        "#T5 recursive summarization\n",
        "t5_final_summary = t5_recursive_summarize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W94voJid43o",
        "outputId": "ccfbe83b-892d-4f9e-d9f9-9ce8513c4230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bart's summary: BloombergGPT is a 50 billion parameter language model that is trained on a wide range of data. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain.\n",
            "\n",
            "\n",
            "T5's summary: the model outperforms existing models on financial tasks by significan t margins. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set. goal for BloombergGPT was to be a best-class model for financial tasks. eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content.\n"
          ]
        }
      ],
      "source": [
        "#Printining out the final models' summaries.\n",
        "print(\"Bart's summary:\", bart_final_summary)\n",
        "print(\"\\n\")\n",
        "print(\"T5's summary:\", t5_final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jgyGdZx1Qcn",
        "outputId": "cd361946-0509-4f5e-cfc9-f66f1be20d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined summary: BloombergGPT is a 50 billion parameter language model that is trained on a wide range of data. Financial Technology (FinTech) is a large and growing area with NLP technol ogies having an increasingly important role. Financial NLP tasks include sentiment analysis, news classiﬁcation, and question answering. No LLM has been tuned for or evaluated on tasks for the ﬁnancial domain. the model outperforms existing models on financial tasks by significan t margins. it is based on a 50 billion-parameter language model with a 50 billion-parameter training set. goal for BloombergGPT was to be a best-class model for financial tasks. eBloomberg is interested in studying whether FinPile reduces proclivity of model to generate inappropriate content.\n"
          ]
        }
      ],
      "source": [
        "#Printing out the combined models' summary.\n",
        "print(\"Combined summary:\", bart_final_summary + \" \" + t5_final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "a2d8f99d418846b6870ba6a5112ede44",
            "a0ed8d0185444aa4af3f7be0847818fb",
            "63d1c8de5818486080bd8e22217ecb83",
            "7545407d737d4d86bee00b1020ce23ea",
            "39228cbfd70b49aaa841be8ea3d687f5",
            "80abd4e3fe5b48dcb1a5740c0e3b21fa",
            "97a5c7b99b1942fbb782f0c298defa90",
            "f5cbc046cd3e487e9ff67d54ceea7641",
            "00369445829b4b3c95a1b9d716063935",
            "4ddc4ca6841b451db31f138947d645ec",
            "3dc9f1b5bcf44bf3b633c0a003bfe0c5",
            "1379295da83e4320a4dccf1b84f76080",
            "41eddb7dc0f24cfb865e48e84fb52366",
            "0c826789a85b4a46bf9e6b52c570167f",
            "f6e692e7d3f940119d58a23a731ed926",
            "5489d36f59614ae7ba3c7e1aa75ea4a8",
            "1bc6c40433b146beb23f392155b3ed65",
            "45e0a414a2f941a38a86766a92450b34",
            "dd572ba98d644620aecc9c8e5fee6b65",
            "b89a0833b5514ed5ac7ebf19adc11033",
            "0cfaf6c7399d466d8fd9432055d3598c",
            "88267f6e8a3a4b3a84003bad3fd3a029"
          ]
        },
        "collapsed": true,
        "id": "eHLgJ5RB872A",
        "outputId": "69622549-fa39-4db5-c76c-1ace1a0ea417"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calculating scores...\n",
            "computing bert embedding.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2d8f99d418846b6870ba6a5112ede44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "computing greedy matching.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1379295da83e4320a4dccf1b84f76080",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done in 1.22 seconds, 2.46 sentences/sec\n"
          ]
        }
      ],
      "source": [
        "#Evaluate the models' performance.\n",
        "#Using the Bart score which is a function of the scalar vector and the vectors' Euclidean norm.\n",
        "##AB/(||A||*||B||) or ((a1b1)+(a2b2)...+(anbn)) / (sqrt(a1**2+a2**2...+an**2) * sqrt(b1**2+b2**2...+bn**2)).\n",
        "combined_summary = bart_final_summary + \" \" + t5_final_summary\n",
        "\n",
        "reference_texts = [text,text,text]\n",
        "candidate_summaries = [bart_final_summary, t5_final_summary, combined_summary]\n",
        "\n",
        "P, R, F1 = score(candidate_summaries, reference_texts, lang=\"en\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_i8u0Ru-1tr",
        "outputId": "97bc4d94-6db0-4712-c801-12709b95add3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary 1 -- P: 0.8799, R: 0.7593, F1: 0.8152\n",
            "Summary 2 -- P: 0.8537, R: 0.7339, F1: 0.7893\n",
            "Summary 3 -- P: 0.8701, R: 0.7710, F1: 0.8175\n"
          ]
        }
      ],
      "source": [
        "#Models' performance.\n",
        "for i, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
        "    print(f\"Summary {i+1} -- P: {p:.4f}, R: {r:.4f}, F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "gZQ9TM1k9Iug",
        "outputId": "a7a667e4-33d0-44c8-85ad-668d647c3876"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 900x400 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Visualizing models' performance.\n",
        "P = P.tolist()\n",
        "R = R.tolist()\n",
        "F1 = F1.tolist()\n",
        "\n",
        "summaries = ['BART', 'T5', 'Combined']\n",
        "\n",
        "x = range(len(summaries))\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(x, P, edgecolor='black')\n",
        "plt.xticks(x, summaries)\n",
        "plt.title('Precision')\n",
        "plt.ylim(0, 1)\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(x, R, edgecolor='black')\n",
        "plt.xticks(x, summaries)\n",
        "plt.title('Recall')\n",
        "plt.ylim(0, 1)\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.bar(x, F1, edgecolor='black')\n",
        "plt.xticks(x, summaries)\n",
        "plt.title('F1 Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00369445829b4b3c95a1b9d716063935": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c826789a85b4a46bf9e6b52c570167f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd572ba98d644620aecc9c8e5fee6b65",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b89a0833b5514ed5ac7ebf19adc11033",
            "value": 1
          }
        },
        "0cfaf6c7399d466d8fd9432055d3598c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1379295da83e4320a4dccf1b84f76080": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41eddb7dc0f24cfb865e48e84fb52366",
              "IPY_MODEL_0c826789a85b4a46bf9e6b52c570167f",
              "IPY_MODEL_f6e692e7d3f940119d58a23a731ed926"
            ],
            "layout": "IPY_MODEL_5489d36f59614ae7ba3c7e1aa75ea4a8"
          }
        },
        "1bc6c40433b146beb23f392155b3ed65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39228cbfd70b49aaa841be8ea3d687f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dc9f1b5bcf44bf3b633c0a003bfe0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41eddb7dc0f24cfb865e48e84fb52366": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc6c40433b146beb23f392155b3ed65",
            "placeholder": "​",
            "style": "IPY_MODEL_45e0a414a2f941a38a86766a92450b34",
            "value": "100%"
          }
        },
        "45e0a414a2f941a38a86766a92450b34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ddc4ca6841b451db31f138947d645ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5489d36f59614ae7ba3c7e1aa75ea4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63d1c8de5818486080bd8e22217ecb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5cbc046cd3e487e9ff67d54ceea7641",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00369445829b4b3c95a1b9d716063935",
            "value": 1
          }
        },
        "7545407d737d4d86bee00b1020ce23ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ddc4ca6841b451db31f138947d645ec",
            "placeholder": "​",
            "style": "IPY_MODEL_3dc9f1b5bcf44bf3b633c0a003bfe0c5",
            "value": " 1/1 [00:01&lt;00:00,  1.16s/it]"
          }
        },
        "80abd4e3fe5b48dcb1a5740c0e3b21fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88267f6e8a3a4b3a84003bad3fd3a029": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97a5c7b99b1942fbb782f0c298defa90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0ed8d0185444aa4af3f7be0847818fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80abd4e3fe5b48dcb1a5740c0e3b21fa",
            "placeholder": "​",
            "style": "IPY_MODEL_97a5c7b99b1942fbb782f0c298defa90",
            "value": "100%"
          }
        },
        "a2d8f99d418846b6870ba6a5112ede44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0ed8d0185444aa4af3f7be0847818fb",
              "IPY_MODEL_63d1c8de5818486080bd8e22217ecb83",
              "IPY_MODEL_7545407d737d4d86bee00b1020ce23ea"
            ],
            "layout": "IPY_MODEL_39228cbfd70b49aaa841be8ea3d687f5"
          }
        },
        "b89a0833b5514ed5ac7ebf19adc11033": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd572ba98d644620aecc9c8e5fee6b65": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5cbc046cd3e487e9ff67d54ceea7641": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e692e7d3f940119d58a23a731ed926": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cfaf6c7399d466d8fd9432055d3598c",
            "placeholder": "​",
            "style": "IPY_MODEL_88267f6e8a3a4b3a84003bad3fd3a029",
            "value": " 1/1 [00:00&lt;00:00, 31.18it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
